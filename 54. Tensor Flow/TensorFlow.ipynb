{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to store data in TF? There are various ways, first we learn how to create constants.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(2)\n",
    "# lets see what a has now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "# so a is tensor object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.constant(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n",
    "# so it does not represent the value it has now. So now if we are pronting a, it is not printing its value, its just printing\n",
    "# the tensor obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what if we sum it\n",
    "a  + b\n",
    "\n",
    "# so its not doing the addition, its just saying you want to do the addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ad38c3615cca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# if we want to do the addition we need to create session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "# if we want to do the addition we need to create session.\n",
    "sess = tf.Session()\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TF 2.0 supports eager execution which means you don't have to explicitly create a session and run the code in it. So the simplest solution would be:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More operations\n",
    "a1 = tf.constant([[3, 3]]) # array 1x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = tf.constant([[3],[3]]) # array 2x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "ans = tf.matmul(a1, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[18]], shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(ans)\n",
    "# now as we can see, it gabe us the result along with the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now as we saw, these are constants, once assigned you cannot change it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Session\n",
    "url : https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session  \n",
    "### If you have TensorFlow v2 then you dont have to use Session.  However if you want to use it,  method is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A session may own resources, such as tf.Variable, tf.queue.QueueBase, and tf.compat.v1.ReaderBase. It is important to release these resources when they are no longer required. To do this, either invoke the tf.Session.close method on the session, or use the session as a context manager. The following two examples are equivalent:\n",
    "\n",
    "\n",
    "#### Using the `close()` method.  \n",
    "sess = tf.compat.v1.Session()  \n",
    "sess.run(...)  \n",
    "sess.close()  \n",
    "  \n",
    "#### Using the context manager.\n",
    "with tf.compat.v1.Session() as sess:  \n",
    "  sess.run(...)    \n",
    "  \n",
    "### To run the session for placeholder, we need to disable the eager execution from v1 which we will do it later(not from v2 in our case, although we can do it but to understand both v1 and v2, we are doing it on v1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    a = tf.constant(5.0)\n",
    "    b = tf.constant(6.0)\n",
    "    c = a * b\n",
    "    sess.run(c)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# or we can use it liek this \n",
    "with tf.compat.v1.Session() as sess:\n",
    "    print(c.eval())\n",
    "    \n",
    "# also we cannot use the eval outside the session. \n",
    "# if we are using session with 'with' block, it will use the default session of its own block, and that session will\n",
    "# not work outside the 'with' block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable maintains shared, persistent state manipulated by a program.  \n",
    "  \n",
    "The Variable() constructor requires an initial value for the variable, which can be a Tensor of any type and shape. This initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.  \n",
    "  \n",
    "  \n",
    "v = tf.Variable(1.)  \n",
    "v.assign(2.)  \n",
    "url = https://www.tensorflow.org/api_docs/python/tf/Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=100>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1 = tf.Variable(100) # one way to create variable\n",
    "var1\n",
    "# remember this is global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "var2 = tf.Variable(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(103, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sum1= tf.add(var1, var2)\n",
    "print(sum1)\n",
    "# as we can see, we got the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Tensor.graph is meaningless when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0c06f8e9ac08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m     fetch_handler = _FetchHandler(\n\u001b[0m\u001b[0;32m   1176\u001b[0m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \"\"\"\n\u001b[0;32m    486\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0m\u001b[0;32m    307\u001b[0m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0;32m    308\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3758\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3759\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3761\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3835\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3836\u001b[0m       \u001b[1;31m# Actually obj is just the object it's referring to.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3837\u001b[1;33m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3838\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3839\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[1;32m--> 401\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mgraph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1248\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m     raise AttributeError(\n\u001b[0m\u001b[0;32m   1251\u001b[0m         \"Tensor.graph is meaningless when eager execution is enabled.\")\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Tensor.graph is meaningless when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "# but what if we do it with session?\n",
    "# it will work fine if the variable have 103, \n",
    "# The issues with variable is with tensorflow, we need to explicitly call initialize on variables only then these\n",
    "# variables will get 100 and 3. This all stuff works in Tensorflow v1. We dont have to initialize all this even  with compat \n",
    "# version. else it will give error.\n",
    "# Line which is used to call explicitly is tf.global_variables_initializer().  \n",
    "\n",
    "\n",
    "# Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. \n",
    "# Instructions for updating: Prefer Variable.assign which has equivalent behavior in 2.X.\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=1232>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to change the value of variable1, we can do is we pass it in assign()\n",
    "# in older version, we have to pass it in session to assign it, now in tf v2, we dont need session to assign it.\n",
    "var1.assign(1232)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.get_static_value**  \n",
    "Returns the constant value of the given tensor, if efficiently calculable.  \n",
    "REMEMBER IT RETURNS THE OPERATIONAL RESULT NOT THE CONSTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235\n"
     ]
    }
   ],
   "source": [
    "var3 = var1 + var2  # ADDITION OPERATION\n",
    "print(tf.get_static_value(var3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to print the variable, it will return none as there is no operation done on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_static_value(var2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1232>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default, variables are going to be global.\n",
    "var1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Placeholder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not supported in TF v2, Instead,  \n",
    "### tf.compat.v1.placeholder  \n",
    "As discussed before, we need to disable eager execution.  \n",
    "Inserts a placeholder for a tensor that will be always fed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.compat.v1.placeholder(  \n",
    "    dtype, shape=None, name=None  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0345de65e527>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3fdd3efc7c14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   3280\u001b[0m   \"\"\"\n\u001b[0;32m   3281\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3282\u001b[1;33m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[0;32m   3283\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   3284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "x = tf.compat.v1.placeholder(tf.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, both versions are not working, so we need to use it with v1 and disable the eager execution mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabling the eager execution for v1 not v2 \n",
    "tf1.compat.v1.disable_eager_execution()\n",
    "\n",
    "# initializing the placeholder.\n",
    "x = tf1.placeholder(tf1.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype int16\n\t [[node Placeholder (defined at <ipython-input-15-297178b1766d>:5) ]]\n\nOriginal stack trace for 'Placeholder':\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2877, in run_cell\n    result = self._run_cell(\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-297178b1766d>\", line 5, in <module>\n    x = tf1.placeholder(tf1.int16)\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 3285, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6727, in placeholder\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 748, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3561, in _create_op_internal\n    ret = Operation(\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2045, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1360\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1450\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1451\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype int16\n\t [[{{node Placeholder}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c401d7242970>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# as we have disabled eager execution, we can use session as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# it will not work until you feed the placeholder value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1369\u001b[0m                            run_metadata)\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1392\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1393\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1394\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1396\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype int16\n\t [[node Placeholder (defined at <ipython-input-15-297178b1766d>:5) ]]\n\nOriginal stack trace for 'Placeholder':\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2877, in run_cell\n    result = self._run_cell(\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-297178b1766d>\", line 5, in <module>\n    x = tf1.placeholder(tf1.int16)\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 3285, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6727, in placeholder\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 748, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3561, in _create_op_internal\n    ret = Operation(\n  File \"C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2045, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n"
     ]
    }
   ],
   "source": [
    "# as we have disabled eager execution, we can use session as well.\n",
    "with tf1.compat.v1.Session() as sess:\n",
    "    sess.run(x)\n",
    "    \n",
    "# it will not work until you feed the placeholder value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# to pass the value of placeholder, we need to pass it in feed_dict\n",
    "y = tf1.placeholder(tf1.int32)\n",
    "with tf1.compat.v1.Session() as sess:\n",
    "    print(sess.run(y, feed_dict = {y: 10}))    # we need to pass the value of x in key-value pair\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can run same code with different values in it using placeholder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert a: 1102\n",
      "35264\n"
     ]
    }
   ],
   "source": [
    "a = tf1.placeholder(tf1.int32)\n",
    "y = tf1.constant(32)\n",
    "z = a * y\n",
    "with tf1.compat.v1.Session() as sess:\n",
    "    print(sess.run(z, feed_dict = {a: int(input('Insert a: '))}))\n",
    "    \n",
    "# we can run the above code multiple times with different input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 24 36]\n",
      " [48 60 72]]\n"
     ]
    }
   ],
   "source": [
    "# one option we have is we can pass the shape of the placeholder\n",
    "# by default the shape is none\n",
    "a = tf1.placeholder(tf1.int32, shape = (2,3))\n",
    "z = a * tf.constant(12)\n",
    "with tf1.compat.v1.Session() as sess:\n",
    "    print(sess.run(z, feed_dict = {a: [[1,2, 3], [4,5,6]] }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### loading dataset from sklearn\n",
    "\n",
    "# from sklearn.datasets import load_digits\n",
    "# mnist = load_digits()\n",
    "# mnist.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-2dca132f04b4>:6: read_data_sets (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
      "WARNING:tensorflow:From C:\\Users\\hashi\\Data Science and Machine Learning\\Machine Learning\\54. Tensor Flow\\input_data.py:296: _maybe_download (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\hashi\\Data Science and Machine Learning\\Machine Learning\\54. Tensor Flow\\input_data.py:299: _extract_images (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\hashi\\Data Science and Machine Learning\\Machine Learning\\54. Tensor Flow\\input_data.py:304: _extract_labels (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\hashi\\Data Science and Machine Learning\\Machine Learning\\54. Tensor Flow\\input_data.py:112: _dense_to_one_hot (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\hashi\\Data Science and Machine Learning\\Machine Learning\\54. Tensor Flow\\input_data.py:328: _DataSet.__init__ (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# mnist = tf.keras.datasets.mnist\n",
    "# for running the below code, you need to have inputdata.py, dataset.py and MNIST_data folder in the same folder which has this\n",
    "# notebook.\n",
    "\n",
    "import input_data as ids\n",
    "mnist = ids.read_data_sets('MNIST_data/', one_hot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_Datasets(train=<input_data._DataSet object at 0x000002C215A693A0>, validation=<input_data._DataSet object at 0x000002C216E50790>, test=<input_data._DataSet object at 0x000002C216E50850>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so the above dataset has 70000 images.\n",
    "# lets see how these images are split\n",
    "mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at shape\n",
    "mnist.train.images.shape\n",
    "\n",
    "# so as we cans see, 55000 images are in training data and they have a shape of 28 x 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.validation.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]]),\n",
       " (55000, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check what labels are\n",
    "mnist.train.labels, mnist.train.labels.shape\n",
    "\n",
    "# labels are basically y data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By looking at the labels, we can define that this is multiclass classification. So the structure of our neural network in the output layer will have 10 units, and each units either predict 0 or 1. For that we need to one hot encode to the label and that we did.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see the label\n",
    "mnist.train.labels[0]\n",
    "\n",
    "# indicates the value of label is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting an image\n",
    "# for that we need to first convert the image to array then reshape it to m x n matrix.\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "first_image = mnist.train.images[0]\n",
    "first_image = np.array(first_image, dtype='float')\n",
    "first_image = first_image.reshape((28, 28))\n",
    "\n",
    "# plotting the image\n",
    "plt.imshow(first_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Initialising Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the Neural Network we built around it?**   \n",
    "  \n",
    "As the Image is of size 784, our input layer will have 784 entries.  \n",
    "Lets say we have 2 hidden layers, h1 and h2 and output layer.  \n",
    "And we have 10 output units in output layer, because we have 10 possible classes.  \n",
    "And lets say for now, we will have 256 entries in both hidden layers(obviously we can change it.).  \n",
    "And there will be one biases in input layer, h1 and h2.   \n",
    "  \n",
    "**So how many weights will be required?**    \n",
    "So between input layer and h1, weights required are 784x256 weights and 256 biases.  \n",
    "Between h1 and h2, 256x256 weights and 256 biases,  \n",
    "and between h2 and output layer, it will require 256x10 weights and 10 biases.  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating units in layers. \n",
    "\n",
    "# but first we need to import these\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "n_input = 784\n",
    "h1 = 256\n",
    "h2 = 256\n",
    "n_classes = 10  # output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.random_normal()**  \n",
    "Changed to **tf.random.normal()**   \n",
    "In v2, we dont need session to execute it.  \n",
    "https://www.tensorflow.org/api_docs/python/tf/random/normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_normal:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal([1])  # shape must be passed in array, its a 1d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_normal_1:0' shape=(1, 2) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal([1, 2]) # 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_normal_2:0' shape=(1, 2, 3) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal([1, 2, 3])  # 3d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating weights and biases.  \n",
    "# for weights we will create dictionary where keys will be layer name and its values will be the keys.\n",
    "\n",
    "weights = {\n",
    "    'h1' : tf.Variable(tf.random.normal([n_input, h1 ])),  # variable will be of size n_input x h1\n",
    "    'h2' : tf.Variable(tf.random.normal([h1, h2 ])),\n",
    "    'out' : tf.Variable(tf.random.normal([h2, n_classes]))\n",
    "}\n",
    "\n",
    "# creating biases\n",
    "\n",
    "biases = {\n",
    "    'h1' : tf.Variable(tf.random.normal([h1])),  # variable will be of size of the layers\n",
    "    'h2' : tf.Variable(tf.random.normal([h2])),\n",
    "    'out' : tf.Variable(tf.random.normal([n_classes]))   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Forward Propagation   \n",
    "  \n",
    "So we will create a function for Forward Propogation. it will take X, weights, biases.    \n",
    "So we will do matrix multiplication of X and weights.    \n",
    "SO dimension of X is 10000 x 784, weights are 784x256 at first layer, weight is actually a dictionary, for h1 it is of weight 784x256.    \n",
    "  \n",
    "So we will do this multiplication and result will be 10000x256 and we will add biases into it.  \n",
    "So for 10000 images, we will have output as 256 for layer 1.  \n",
    "Then we will use this output for layer2 then we will make that output of layer 2 pass through output layer to get our result.  \n",
    "At hidden layer 1, we will use activation function as 'relu'. 'relu' means *max(a, 0)* which means if it isnegative, it will push it to 0.  \n",
    "  \n",
    "And h2 layer activation function will also be relu.  \n",
    "And we will be using identity at the output layer means we will not be using any activation function on the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_propagation(x, weights, biases):\n",
    "#     # this will give you net input going in layer1.\n",
    "#     input_layer1 = tf.add(tf.matmul(x, weights['h1']), biases['h1'])  # multiplying layer1 weights and inputs and adding biases.\n",
    "#     output_layer1 = tf.nn.relu(input_layer1)  # output of layer1 by applying relu function.  \n",
    "    \n",
    "#     # this will give you net input going in layer2.\n",
    "#     input_layer2 = tf.add(tf.matmul(output_layer1, weights['h2']), biases['h2'])\n",
    "#     output_layer2 = tf.nn.relu(input_layer2)\n",
    "    \n",
    "#     # this will give you output.\n",
    "#     output = tf.add(tf.matmul(output_layer2, weights['out']), biases['out'])\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, weights, biases):\n",
    "    # this will give you net input going in layer1.\n",
    "    input_layer1 = tf.matmul(x, weights['h1'])+ biases['h1']  # multiplying layer1 weights and inputs and adding biases.\n",
    "    output_layer1 = tf.nn.relu(input_layer1)  # output of layer1 by applying relu function.  \n",
    "    \n",
    "    # this will give you net input going in layer2.\n",
    "    input_layer2 = tf.matmul(output_layer1, weights['h2']) + biases['h2']\n",
    "    output_layer2 = tf.nn.relu(input_layer2)\n",
    "    \n",
    "    # this will give you output.\n",
    "    output = tf.matmul(output_layer2, weights['out'])+ biases['out']\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Finding Predictions and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the predictions without any optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look here, passing weights and biases is fine as we have it in dictionary, but how do we pass this x?  \n",
    "So we need some tensor object which we have to pass as x.  \n",
    "So this x, sometime will be training data and sometime will be testing data, So we need to add this x in placeholder.  \n",
    "**Now as we know placeholder don't work in v2, we need to find out a way for it or try to use the older version.  \n",
    "Reason behind importing those packages is because of unstability of the versions. Might be possible that it will be fixed soon.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now converting x into tensor object.\n",
    "x = tf1.placeholder('float', [None, n_input])\n",
    "y = tf1.placeholder(tf1.int32, [None, n_classes])\n",
    "\n",
    "# Now in case of x, we dont know the size of input(which means we dont know how many inputs are there), \n",
    "# but we know what is the size of an image, So for giving size, we gave first arg as None which means we dont know how \n",
    "# many images are there and we know whats the size of each image which is 784 so we gave it. Same is the case for y. \n",
    "# y will be integer as it contains the classes which we one hot encoded it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 0, ..., 0, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # reason behind running the above code is it will give the error : \n",
    "# # RuntimeError: Attempting to capture an EagerTensor without building a function.\n",
    "\n",
    "\n",
    "pred = forward_propagation(x, weights , biases)\n",
    "\n",
    "# for finding the answers, we need to find the index of max.\n",
    "predictions = tf.argmax(pred, 1)  # argmax will give the index of maximum\n",
    "\n",
    "# to find the correct labels, we need to find that in y.\n",
    "correct_labels = tf.argmax(y, 1) \n",
    "\n",
    "# now to run this, we need to create session. \n",
    "sess = tf1.Session()\n",
    "\n",
    "# initializing weights\n",
    "sess.run(tf1.global_variables_initializer())\n",
    "predictions_eval = sess.run(predictions, feed_dict = {x:mnist.test.images} )\n",
    "predictions_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the above code will give you different predictions each time. To get the same predictions, \n",
    "# we need to refactor the code\n",
    "sess = tf1.Session()\n",
    "sess.run(tf1.global_variables_initializer())\n",
    "x = tf1.placeholder('float', [None, n_input])\n",
    "y = tf1.placeholder(tf1.int32, [None, n_classes])\n",
    "\n",
    "# getting predictions\n",
    "pred = forward_propagation(x, weights , biases)\n",
    "predictions = tf.argmax(pred, 1)\n",
    "correct_labels = tf.argmax(y, 1) \n",
    "\n",
    "# running through session.run()\n",
    "predictions_eval = sess.run(predictions, feed_dict = {x:mnist.test.images} )\n",
    "correct_pred = tf.equal(predictions, correct_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 2, 3, ..., 1, 1, 1], dtype=int64),\n",
       " array([7, 2, 1, ..., 4, 5, 6], dtype=int64),\n",
       " array([False,  True, False, ..., False, False, False]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the correct labels, in this, we need to run the predictions as well as the correct_labels as well.\n",
    "predictions_eval, labels, correct_pred = sess.run([predictions, correct_labels, correct_pred], feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "predictions_eval , labels, correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred.sum()\n",
    "# so we got 901 correct predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 1, 1, ..., 2, 1, 1], dtype=int64),\n",
       " array([7, 3, 4, ..., 5, 6, 8], dtype=int64),\n",
       " array([False, False, False, ..., False, False, False]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the correct labels, in this, we need to run the predictions as well as the correct_labels as well.\n",
    "correct_pred = tf.equal(predictions, correct_labels)\n",
    "predictions_eval, labels, correct_pred = sess.run([predictions, correct_labels, correct_pred], feed_dict={x:mnist.train.images, y:mnist.train.labels})\n",
    "predictions_eval , labels, correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5781"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to optimize the cost. \n",
    "So the cost we will use is **Cross Entropy** cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hashi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'softmax_cross_entropy_with_logits_sg/Reshape_2:0' shape=(None,) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate the cross entropy cost, we need predictions \n",
    "\n",
    "sess = tf1.Session()\n",
    "sess.run(tf1.global_variables_initializer())\n",
    "x = tf1.placeholder('float', [None, n_input])\n",
    "y = tf1.placeholder(tf1.int32, [None, n_classes])\n",
    "\n",
    "# getting predictions\n",
    "pred = forward_propagation(x, weights , biases)\n",
    "predictions = tf.argmax(pred, 1)\n",
    "correct_labels = tf.argmax(y, 1) \n",
    "\n",
    "# As we got predictions, we can implement cross entropy\n",
    "tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so what we will do is we will take the mean of the values \n",
    "# which means if we have 10000  images we dont want a 10000 sized array(plus 10 different labels), \n",
    "# we just want one value so that it will be easier to understand\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have the cost, what do we need to do now? now we need to write and figure out how to optimize this cost by changing weights, the best part is we dont have to write the backpropogation ourself, we will use the inbuilt optimizer which will take this cost function, figure out what all variables this cost function is dependent upon and then change those weights to optimize on cost.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Equal_2:0' shape=(None,) dtype=bool>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running through session.run()\n",
    "predictions_eval = sess.run(predictions, feed_dict = {x:mnist.test.images} )\n",
    "correct_pred = tf.equal(predictions, correct_labels)\n",
    "correct_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Running the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf1.Session()\n",
    "sess.run(tf1.global_variables_initializer())\n",
    "x = tf1.placeholder('float', [None, n_input])\n",
    "y = tf1.placeholder(tf1.int32, [None, n_classes])\n",
    "\n",
    "# getting predictions\n",
    "pred = forward_propagation(x, weights , biases)\n",
    "predictions = tf.argmax(pred, 1)\n",
    "correct_labels = tf.argmax(y, 1) \n",
    "\n",
    "# As we got predictions, we can implement cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now implementing optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.01)\n",
    "\n",
    "# so if we want our optimizer to work on cost, \n",
    "optimize = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1944.1223"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now running the cost\n",
    "c = sess.run(cost, feed_dict={x:mnist.train.images, y:mnist.train.labels})\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to run the optimizer as well to reduce the cost\n",
    "# remember we need to pass the optimizer with cost in new session else it will give error\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1934.9459"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c, o = sess.run([cost, optimize], feed_dict={x:mnist.train.images, y:mnist.train.labels})\n",
    "c\n",
    "# run it again and the cost will be less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. How does the Optimizer work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way optimizer works is by default all the variables that you create has a property **trainable = True**  \n",
    "So what this optimizer does for us is it finds out what all variables exists which has trainable = True, and its going to find gradient wrt all of them and change the values for all of them to try and optimize the function that you have. \n",
    "So we call minimize on cost, cost is dependent on weights and biases and these are the only variables which we created which have trainable = True.If we want we can even say that we want you to train on weights, if we dont want, we can pass trainable = False.  \n",
    "   \n",
    "So lets check which all are trainables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(784, 256) dtype=float32>,\n",
       " <tf.Variable 'Variable_1:0' shape=(256, 256) dtype=float32>,\n",
       " <tf.Variable 'Variable_2:0' shape=(256, 10) dtype=float32>,\n",
       " <tf.Variable 'Variable_3:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'Variable_4:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'Variable_5:0' shape=(10,) dtype=float32>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do one of the trainables to be false, we will see that that particular variable will not be changed when we optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Running Multiple Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858.4905\n",
      "607.53357\n",
      "447.29898\n",
      "324.68643\n",
      "232.62843\n",
      "192.55856\n",
      "176.73094\n",
      "162.894\n",
      "146.01962\n",
      "130.09297\n",
      "118.18825\n",
      "109.39218\n",
      "102.50127\n",
      "96.6726\n",
      "91.29068\n",
      "86.439384\n",
      "82.19738\n",
      "78.4587\n",
      "75.03629\n",
      "72.00887\n",
      "69.31768\n",
      "66.84227\n",
      "64.48732\n",
      "62.202427\n",
      "60.053295\n"
     ]
    }
   ],
   "source": [
    "# all we need to do is we need to run this code multiple times and at the same time we will find the predictions. \n",
    "for i in range(25):\n",
    "    c, o = sess.run([cost, optimize], feed_dict={x:mnist.train.images, y:mnist.train.labels})\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8829"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tf.argmax(pred, 1)\n",
    "correct_labels = tf.argmax(y, 1)\n",
    "correct_pred = tf.equal(predictions, correct_labels)\n",
    "predictions_eval, correct = sess.run([predictions, correct_pred], feed_dict = {x:mnist.test.images, y:mnist.test.labels} )\n",
    "correct.sum()\n",
    "\n",
    "# we got 88% accuracy.\n",
    "# lets try to run the data in batches.(batch gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did above is we gave complete data in feed_dict.  \n",
    "So lets have batch size of 100, we will run the train data loop to 550 times as it have 55000 images and this loop will call the c and _ every time.  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144.59566029595987\n",
      "127.2348647205685\n",
      "111.59988022088237\n",
      "130.10961578199317\n",
      "100.48243910983763\n",
      "92.17223857966962\n",
      "85.91728730646173\n",
      "115.6466229147045\n",
      "82.18699420422126\n",
      "80.48632407905825\n",
      "74.78467465910944\n",
      "62.38809691751521\n",
      "51.797957158023564\n",
      "80.77875794693136\n",
      "51.08150137557459\n",
      "61.0849328309705\n",
      "63.44990200066832\n",
      "65.63887774693285\n",
      "54.436397831188515\n",
      "58.9679408597367\n",
      "49.01499483716043\n",
      "49.635187422695026\n",
      "58.581354380796256\n",
      "47.927827825049576\n",
      "36.1667811576732\n"
     ]
    }
   ],
   "source": [
    "# we need to run chapter 10 again before running this.\n",
    "batch_size = 100\n",
    "for i in range(25):\n",
    "    num_batches = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    # to print c only 25 times, we need to add c into total cost\n",
    "    total_cost = 0\n",
    "    for j in range(num_batches):\n",
    "        # mnist gives us the way to find the next batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        c, o = sess.run([cost, optimize], feed_dict={x:batch_x, y:batch_y})\n",
    "        total_cost += c\n",
    "    print(total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost here seems to be higher(i have run this twice) because we added different batches to the cost as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9677"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tf.argmax(pred, 1)\n",
    "correct_labels = tf.argmax(y, 1)\n",
    "correct_pred = tf.equal(predictions, correct_labels)\n",
    "predictions_eval, correct = sess.run([predictions, correct_pred], feed_dict = {x:mnist.test.images, y:mnist.test.labels} )\n",
    "correct.sum()\n",
    "\n",
    "# accuracy increased to 95% in first run and and 96.7% in second run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORFLOW 2 : MNIST: Load and Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*we can also load mnist dataset from keras.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# lets look at the shape\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Lets see how the input image looks like.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, it looks like number 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before feeding this data into Neural Network, we need to do some processing.    \n",
    "We need to flatten these images(which means we have 60000 images of shape 28 x 28, which is 2d array, we need to flatten it to 1d array which is 784).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reshape, the first dimension should be no of samples, to flatten out rest of the dimension, we add -1 in the last argument.  \n",
    "And we need to do one more thing, currently, the pixels values are between 0 to 255, so we need to normalize it between 0 to 1 also, to do that we need to multiply it with 255.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], -1)/255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our Neural Network, we need to one hot encode our labels also. After one hot encoding, for one training samples, there will be 10 different values in the output. So it will be one hot encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# lets look at the shape\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORFLOW 2 : Model Architecture and Building Model using Sequential API\n",
    "\n",
    "we will have 2 hidden layers(dense layers) with 256 units and one output layer with 10 units(as it is no of class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the model object\n",
    "model = Sequential()\n",
    "# we also need to define input shape which is 784 of one dimension in our case.\n",
    "model.add(Dense(256, activation = 'relu', input_shape = (784, )))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "# output layer\n",
    "model.add(Dense(10, activation = 'softmax'))  # for binary classification, we use sigmoid, for more class we use softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as model is defined, we need to define what is the optimizer, loss function and metric\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy']) # for bin class, we use binary_crossentropy, for multiclass, we use this.\n",
    "# metric is on which  we want to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/59 [==============================] - 3s 15ms/step - loss: 0.5937 - accuracy: 0.8411\n",
      "Epoch 2/10\n",
      "59/59 [==============================] - 1s 15ms/step - loss: 0.2063 - accuracy: 0.9403\n",
      "Epoch 3/10\n",
      "59/59 [==============================] - 1s 11ms/step - loss: 0.1461 - accuracy: 0.9575\n",
      "Epoch 4/10\n",
      "59/59 [==============================] - 1s 11ms/step - loss: 0.1125 - accuracy: 0.9673\n",
      "Epoch 5/10\n",
      "59/59 [==============================] - 1s 11ms/step - loss: 0.0900 - accuracy: 0.9743\n",
      "Epoch 6/10\n",
      "59/59 [==============================] - 1s 11ms/step - loss: 0.0725 - accuracy: 0.9791\n",
      "Epoch 7/10\n",
      "59/59 [==============================] - 1s 11ms/step - loss: 0.0598 - accuracy: 0.9831\n",
      "Epoch 8/10\n",
      "59/59 [==============================] - 1s 12ms/step - loss: 0.0507 - accuracy: 0.9857\n",
      "Epoch 9/10\n",
      "59/59 [==============================] - 1s 11ms/step - loss: 0.0430 - accuracy: 0.9880\n",
      "Epoch 10/10\n",
      "59/59 [==============================] - 1s 11ms/step - loss: 0.0349 - accuracy: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b82099220>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "model.fit(x_train, y_train, epochs = 10, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0708 - accuracy: 0.9763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07083430886268616, 0.9763000011444092]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see how our model does on testing data\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORFLOW 2 : Building Model using Functional API"
   ]
  },
  {
   "attachments": {
    "Functional%20API%20model.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we learned uptil now is Sequential. Uptil now our model is stack of layers and a graph where we have different paths for the input to the output. As we can see, one path is from input, h1, h2 to the concat. Second path is input, h3 to the concat. \n",
    "![Functional%20API%20model.jpg](attachment:Functional%20API%20model.jpg)   \n",
    "For this, we will be using Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing functional api\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Concatenate, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*so we need to think in terms of how data flows in the model using this api\n",
    "first we need to define the input object* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape = [784,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*now building the first path*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to pass input 1 through the dense layer.\n",
    "hidden_1 = Dense(256, activation = 'relu')(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now building the hidden2*  \n",
    "In this, we will pass hidden_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_2 = Dense(256, activation = 'relu')(hidden_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we will pass the input from hidden_3 as shown in diagram*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_3 = Dense(256, activation = 'relu')(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we need to concatenate hidden_2 and hidden_3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = Concatenate()([hidden_2, hidden_3]) #this will concatenate hidden2 and hidden3 outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we will pass this concat through the output layer(Ofcourse it will be a dense layer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Dense(10, activation = 'softmax')(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Defining the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          200960      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          200960      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           5130        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 472,842\n",
      "Trainable params: 472,842\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = [input], outputs = [output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "118/118 [==============================] - 2s 12ms/step - loss: 0.3847 - accuracy: 0.8949\n",
      "Epoch 2/5\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.1434 - accuracy: 0.9589\n",
      "Epoch 3/5\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.0951 - accuracy: 0.9726\n",
      "Epoch 4/5\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.0676 - accuracy: 0.9804\n",
      "Epoch 5/5\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.0515 - accuracy: 0.9847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b832de610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compiling and fitting the model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "\n",
    "model.fit(x_train, y_train, epochs = 5, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07314954698085785, 0.9778000116348267]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the model\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORFLOW 2 : Creating Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the custom layers from this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating custom layers, we need to define class\n",
    "class MyLayer(layers.Layer):   # MyLayer is subclass of layers.Layer\n",
    "    def __init__(self, units, input_dim, activation):\n",
    "        # first we need to call the constructor function of the super class which os layer class which is layers.Layer\n",
    "        # in that we need to pass the name of the layer which we want to create.\n",
    "        super(MyLayer, self).__init__()\n",
    "        # after this, we need to set the layer attributes that means to set the weights, to set the biases and to set the \n",
    "        # activation function.\n",
    "        \n",
    "        # creating weights\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value = w_init(shape = (input_dim, units), dtype = 'float32'), trainable = True) # for layer1\n",
    "        \n",
    "        # creating biases\n",
    "        b_init = tf.random_normal_initializer()\n",
    "        self.b = tf.Variable(initial_value = b_init(shape = (units,), dtype = 'float32'), trainable = True)\n",
    "        \n",
    "        # setting the activation\n",
    "        self.activation = activation\n",
    "        \n",
    "    # in every layer, there is forward pass(propogation) and there is a function call() in which all the computations related \n",
    "    # to the forward pass are written. \n",
    "    # so we override this function\n",
    "    def call(self, inputs): \n",
    "        # first we need to define linear operation, which will be input*weights+biases\n",
    "        linear_op = tf.add(tf.matmul(inputs, self.w), self.b)\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            return tf.nn.relu(linear_op)\n",
    "        elif self.activation == 'softmax':\n",
    "            return tf.nn.softmax(linear_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the custom models, we use this class  \n",
    "we have created the layer class, but we havent created any layer. To create layer, we need to create an object of the layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    # so the model should be concerned with how many units are there in each layer.\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_classes):\n",
    "        # creating superclass of the model class.\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # we need to set the weights biases and the activation like we did in layers, similarly we can set it for this model\n",
    "        # so the attributes of this models should be hiddenlayer1, hiddenlayer2 and there is an output layer.\n",
    "        self.layer1 = MyLayer(n_hidden1, n_input, 'relu')    # inputs are no of units, and output of prev layer, and activation.\n",
    "        self.layer2 = MyLayer(n_hidden2, n_hidden1, 'relu')\n",
    "        self.out_layer = MyLayer(n_classes, n_hidden2, 'softmax')\n",
    "    \n",
    "    # this call function should define the main computation for the model\n",
    "    # for models, it should be how data is flowing in the model\n",
    "    def call(self, inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        x = self.layer2(x)\n",
    "        return  self.out_layer(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORFLOW 2 : Initialising and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our model by creating the object of the class\n",
    "model = MyModel(784, 256, 256, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "118/118 [==============================] - 2s 9ms/step - loss: 0.4940 - accuracy: 0.8652\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.1738 - accuracy: 0.9498\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.1221 - accuracy: 0.9640\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0886 - accuracy: 0.9735\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0686 - accuracy: 0.9800\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0556 - accuracy: 0.9839\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0454 - accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0360 - accuracy: 0.9900\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0287 - accuracy: 0.9919\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0249 - accuracy: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22bb25e6370>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 10, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07301206886768341, 0.9775999784469604]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the model\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
