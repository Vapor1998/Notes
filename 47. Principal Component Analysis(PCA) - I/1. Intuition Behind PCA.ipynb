{
 "cells": [
  {
   "attachments": {
    "PCA.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition Behind PCA\n",
    "IN PCA, we want to reduce our dimensionality.  \n",
    "If we have lets say 1000 different features, theres a high chance all of them are corelated to each other.  \n",
    "They are actually representing similar kind of data.  \n",
    "Can we reduce the no of features to 100? but still keep majority of the info that we had in the original  data.  \n",
    "We want to redduce the amount of data but we want to keep similar kind of Information with us.  \n",
    "LETS SEE AN EG. \n",
    "Lets say we have a 2d data with different features x1 and x2.  \n",
    "![PCA.png](attachment:PCA.png)\n",
    "\n",
    "But if we draw a line u1, we will see that this line pretty much explains our datapoints.  \n",
    "We see that they are away from line a little bit, but basically if they were a little closer, we wouldve argue that your data is kind of find by this line.  \n",
    "this means if we had our axis as $x_{1}'$ and $x_{2}'$, all our data could be represented by using $x_{1}'$ and $x_{2}'$ is not giving us that much of information.  \n",
    "If we look at $x_{2}'$ values of all these points, its actualy very small..  \n",
    "The $x_{1}'$ values are the one which has the maximum difference among these datapoints.  \n",
    "Or we can say $x_{1}'$ is the direction which explains our data value or we can say $x_{1}'$ has the max variance.  \n",
    "Thats the direction our data is changing the most.  \n",
    "In the other direction, data change is almost 0.  \n",
    "What we want to do is we want to find another direction, so when we say we reduce the feature we are not saying we have $x_{1}$ to $x_{1000}$ and we will pic $x_{100}$ out of these features, what we are really saying is we will actually create new features lets say $x_{1}'$, $x_{2}'$, .... $x_{100}'$ these are new features that we want to create that basically explains your data really well.  \n",
    "So there is no reason to pick 100 we might end up picking 900 or 60 as well.  \n",
    "So that will be a number that depends upon data to data.  \n",
    "SO here what we are saying is $x_{2}'$ is not the direction which seeing a lot variance in our data the max variation that we see is $x_{1}'$, So the idea of PCA is WHY CANT WE JUST GET RID OF $x_{2}'$ REDUCE THIS 2D DATA INTO 1D DATA.  \n",
    "That would mean we will be loosing some data but hopefully the amount we loosing is not significant and benifits of loosing data might be a lot more.  \n",
    "SO what can be the benifits of having a lots of data.  \n",
    "1. Speed - Less data, less amount to compute.  \n",
    "2. Memory - IF we have huge data, we cant store it, if we have less, we can store it easily.  \n",
    "3. Visualization - Lets say we have 10D data, We will reduce it to 2D or 3D not so we want to train it, but we can visualize it easily.  \n",
    "\n",
    "THe idea of PCA is to reduce the dimensionality.  \n",
    "That doesnot mean we will pick choose few features but we will create it.  \n",
    "\n",
    "Few things to understand while we use PCA.  \n",
    "When we use PCA and we pick $x_{1}'$ and loose $x_{2}'$, which means we move all points on $x_{1}'$. We are loosing info but the amount is not that much.  \n",
    "We should MINIMIZE the amount of info we are usinng.  \n",
    "The info we are loosing is the perpendicular distance from the points.  \n",
    "Thats the value of $x_{2}'$ we want to reduce.  \n",
    "So we try to find the line or direction where the value is minimum.  \n",
    "Going from 3 to 2D or going from 2 to 1D, where we have 1 - 2 features,we will actually loose a lot of data data,  \n",
    "but if we have lets say 10000 features or 1000, and if we reduce the data, we will not loose that much.  \n",
    "We will get an improvement in memory and speed.  \n",
    "We perform really well when we have higher dimension.  \n",
    "SO if we reduce from 1000 to 100, this 100 will have a lot more information than compared to when we are just capturing 2 new dimension.  \n",
    "SO when we deal with small data and we use PCA, we will see a loss in accuracy compared to when we apply PCA for larger features.  \n",
    "Few things to take care.  \n",
    "First of all this is not really Linear Regression. Linear Regression is Supervised while this is Unsupervised.  \n",
    "That means we have X, Y we predict Y with given X.  \n",
    "In case of PCA we are not looking at Y we are looking at features while we use PCA, once we are done with PCa, we will train a model, we will train a model.  \n",
    "In case of Linear Regresssion, things that we minimize is difference in Y values, not the perpendicular but the difference.  \n",
    "One more thing to care about is Feature Scaling.  \n",
    "Lets say we have one feature which takes value from 1000 to 2000 and we put similalr scale to Y axis.  \n",
    "So we have $x_{1}$ which has values like 1000, 1100, 1200 etc.  \n",
    "And we have $x_{2}'$ which have values like 0, 1, 0.5 and so on.  \n",
    "What we will se when we plot this data, we will see $x_{1}$ overpowering $x_{2}$ and as $x_{2}$ is very small, OUR PCA WILL GIVE US $x_{1}$ AS THE DIRECTION WHERE WE ARE SEEING THE MAXIMUM VARIATION.  \n",
    "THE PROBLEM HERE IS WE ARE NOT REALIZING THE CHANGE IN BETWEEN 0 AND 1 MIGHT MUCH BIGGER THAN CHANGE FROM 1000 TO 1100.  \n",
    "SO THAT MEANS IT IS REALLY IMPORTANT TO APPLY FEATURE SCALING.  \n",
    "SO WE NEED TO APPLY FEATURE SCALE BEFORE USING PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
