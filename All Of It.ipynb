{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cdeccb",
   "metadata": {},
   "source": [
    "# `Regression`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94375310",
   "metadata": {},
   "source": [
    "Linear Regression is the most basic algorithm in Machine Learning. It is a regression algorithm, which means that it is useful when we are required to predict continuous values, that is, the output variable ‘y’ is continuous in nature.\n",
    "\n",
    "A few examples of the regression problem can be the following\n",
    "1. “What is the market value of the house?”\n",
    "2. “Stock price prediction”\n",
    "3. “Sales of a shop”\n",
    "4. “Predicting height of a person”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8161742",
   "metadata": {},
   "source": [
    "Terms to be used here:\n",
    "1. **Features** - These are the independent variables in any dataset represented by $x_1$ , $x_2$ , $x_3$,\n",
    "$x_4$,... $x_n$ for ‘n’ features.\n",
    "2. **Target / Output Variable** - This is the dependent variable whose value depends on the\n",
    "independent variable by a relation (given below) and is represented by ‘y’.\n",
    "3. **Function or Hypothesis** of Linear Regression is represented by -\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + … + m_n.x_n + b$\n",
    "\n",
    "\n",
    "$\\qquad$Note: Hypothesis is a function that tries to fit the data.\n",
    "4. **Intercept** - Here b is the intercept of the line. We usually include this ‘b’ in the equation\n",
    "of ‘m’ and take ‘x’ values for that ‘m’ to be 1. So modified form of above equation is as\n",
    "follows:\n",
    "y = mx\n",
    "Where $mx = m_1.x_1 + m_2.x_2 + m_3.x_3 + … + m_n.x_n + m_{n+1}.x_{n + 1}$. Here $m_{n + 1}$ is b and $x_{n + 1} = 1$\n",
    "5. **Training Data** - This data contains a set of dependent variables that is ‘x’ and a set of\n",
    "output variable, ‘y’. This data is given to the machine for it to learn or get trained on\n",
    "some function (here the function is the equation given above) so that in future on giving\n",
    "some new values of ‘x’ , our machine is able to predict values of ‘y’\n",
    "based on that function.\n",
    "6. **Testing Data** - Once our model is ready, we need to get an idea of how well it performs. For this we use testing data. The machine is given the value of 'x', on which it predicts the value of 'y'. We then compare the predicted 'y' with the testing 'y' to get an idea of the error and accuracy of our machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7cc8f0",
   "metadata": {},
   "source": [
    "Linear regression assumes linear relation between x and y.\n",
    "The hypothesis function for linear regression is:\n",
    "\n",
    "$$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + … + m_n.x_n + b$$\n",
    "\n",
    "where m1 , m2 , m3 are called the parameters and b is the intercept of the line. This equation\n",
    "shows that the output variable y is linearly dependent on the features x1 , x2 , x3. The more you\n",
    "are dependent on a particular feature, more will be the value of corresponding m for that feature.\n",
    "We can find out which feature is more important or which feature is more affecting the result by\n",
    "varying the values of m one at a time and see if it is affecting the result, that is , the value of y.\n",
    "So, here in order to predict the values of y for given features values ( x values) we use this\n",
    "equation. But what we are missing here is the values of parameters (m1 , m2 , m3 , … and b).\n",
    "So, we will be using our training data (where the values of x and y are already given) to find out\n",
    "values of parameters and later on predict the value of y for a set of new values of x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ff890",
   "metadata": {},
   "source": [
    "The core idea is to obtain a line that best fits the data. The best fit line is considered to be the line for which the error between the predicted values and the observed values is minimum. It is also called the **regression line** and the errors are also known as **residuals**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0c8a1",
   "metadata": {},
   "source": [
    "**Types of Linear Regression**\n",
    "\n",
    "Linear regression can be further divided into two types of the algorithm:\n",
    "\n",
    "1. **Simple Linear Regression**:\n",
    "If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.\n",
    "\n",
    "2. **Multiple Linear regression**:\n",
    "If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d53b48",
   "metadata": {},
   "source": [
    "### 2. Analysis of LR using dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708de1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data.csv', delimiter = ',')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets split the data in two columns\n",
    "\n",
    "x = data[:, 0]   ## This means give us all the rows and first column.\n",
    "y = data[:, 1]   ## This means give us all the rows and second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d70290",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape  ## its a 1d array with 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will create a dataset fro training and testing\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(x, y)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b881da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating algorithm object\n",
    "alg1 = LinearRegression()\n",
    "alg1.fit(X_train, Y_train)\n",
    "\n",
    "## As we see, iit gave error because we gave 1d array, but it requires 2d array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It wants data to be 2da array. One option is we can convert the data into 2d array.Reshape your data either using array.\n",
    "## reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
    "## After reshaping, we need to split train and test data again and we need to run the training again.\n",
    "\n",
    "x = data[:, 0].reshape(-1, 1)\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(x, y)\n",
    "alg1.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "## Linear Regression should have figured out the line y = mx+c. To look at m and c, there is a way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b317f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg1.coef_  ## this is the gradient or m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aaeb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg1.intercept_  ## this is the y intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets plot the training data along with this parameters.\n",
    "\n",
    "## FIrst we plot the line y = mx+c.\n",
    "## As we saw, m and c is calculated by training data\n",
    "\n",
    "m = alg1.coef_[0]\n",
    "c = alg1.intercept_\n",
    "\n",
    "x_line = np.arange(30, 70, 0.1)     ## taken 30 to 70 because data starts from 30 and to get the best fit, data must fall to the line.\n",
    "y_line = m*x_line + c\n",
    "\n",
    "plt.plot(x_line, y_line, 'r')\n",
    "train_1d = X_train.reshape(75)      ## Xtrain is in the form of 2d while ytrain is in 1d,\n",
    "                                    ##it will be a problem so to avoid the problem, we are converting it into 1d array again.\n",
    "\n",
    "plt.scatter(X_train, Y_train)       ## Plotting the training data.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa93434",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This time we are plotting the testing data\n",
    "\n",
    "m = alg1.coef_[0]\n",
    "c = alg1.intercept_\n",
    "\n",
    "x_line = np.arange(30, 70, 0.1)     ## taken 30 to 70 because data starts from 30 and to get the best fit, data must fall to the line.\n",
    "y_line = m*x_line + c\n",
    "\n",
    "plt.plot(x_line, y_line, 'r')\n",
    "train_1d = X_test.reshape(25)      ## Xtrain is in the form of 2d while ytrain is in 1d,\n",
    "                                    ##it will be a problem so to avoid the problem, we are converting it into 1d array again.\n",
    "\n",
    "plt.scatter(X_test, Y_test)       ## Plotting the training data.\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Lets us say that we are predicting for the point which is at the line.\n",
    "## The points are the actual price while the prediction is on the line. For eg, the point near to 35, the prediction of that\n",
    "## point will be on the line, while the actual data is on that point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38478e51",
   "metadata": {},
   "source": [
    "### 3. Coefficient of Determination(Lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23862cce",
   "metadata": {},
   "source": [
    "Lets say i have a dataset withh n features, it is almost impossible to plot this features against the output.  \n",
    "So we need an objective way to find out how the algorithm is performing.  \n",
    "So we need a way to score our input and whatever output we get, once we passed it through algorithm, the prediction we are getting, we need a way to compare this and objectively come to a score to find out how good and bad our predictions are.  \n",
    "To find this score we  use Coefficient of Determination.  \n",
    "It is defined as 1 - u/v where u is  $$\\sum_{} (y^T_{i} - y^p_{i})^2$$ \n",
    "where y^T is True and y^P  is predicted \n",
    "and v is $$\\sum_{} (y^T_{i} - y^T_{mean})^2$$   \n",
    "\n",
    "  \n",
    "    \n",
    "where u signifies error in our prediction and v signifies error where predictions is effictively mean.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04526ad6",
   "metadata": {},
   "source": [
    "To see how good or bad Algo is performing, we were just drawing the line and seeing how far the actual Data points are from the line.  \n",
    "Thats not really the best way that we can find how good or bad is the Algorithm.  \n",
    "The problem with this lets say a dataset has n features.  \n",
    "It is almost impossible to plot these n features against the output.  \n",
    "So we can look at 2d graphs, we can even look at 3d graphs, but anything more than that it will be impossible.  \n",
    "We need an objective way of finding out our Algorithm is performing.  \n",
    "We need a way so we can score our input, so whatever the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068439c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To find the coefficient of determination, we have function score in the module.\n",
    "\n",
    "## finding score in test data.\n",
    "score_test = alg1.score(X_test, Y_test)\n",
    "\n",
    "## finding score in training data.\n",
    "score_training = alg1.score(X_train, Y_train)\n",
    "\n",
    "score_training, score_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da04a0",
   "metadata": {},
   "source": [
    "### **Simple Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579a0d3",
   "metadata": {},
   "source": [
    "Simple Linear Regression is a type of regression algorithm that models the relationship between a dependent variable and a single independent variable. \n",
    "The key point in Simple Linear Regression is that the dependent variable must be a continuous/real value. However, the independent variable can be measured on continuous or categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3560e",
   "metadata": {},
   "source": [
    "Let us assume that there is only one feature in the dataset, that is, x. So the equation goes as follows:\n",
    "Y = mX + b\n",
    "Let’s say if we scatter the points (x,y) from our training data, then what linear regression tries to\n",
    "do is, find a line with such a value of m and b that error of each data point $(x, y_{actual})$ is minimum when compared with $(x, y_{predicted})$. Error here means combined error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8c8d2",
   "metadata": {},
   "source": [
    "Here it is difficult to find out which line is the best fit just by looking at the different lines.\n",
    "So our algorithm finds out the m and b for the line of best fit by calculating the combined error\n",
    "function and minimizing it. There can be three ways of calculating error function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e431dd62",
   "metadata": {},
   "source": [
    "1. **Sum of residuals** $\\sum(Y_{actual} \\,–\\, Y_{predict})$ – This is usually not used since it might result in cancelling out the positive and\n",
    "negative errors.\n",
    "2. **Sum of the absolute value of residuals** $\\sum|Y_{actual} \\,–\\, Y_{predict}|$ – Taking absolute value would\n",
    "prevent cancellation of positive and negative errors\n",
    "3. **Sum of square of residuals** $\\sum|Y_{actual} \\,–\\, Y_{predict}|^2$ – This is the method mostly used in\n",
    "practice since here we penalize higher error values much more as compared to smaller ones,\n",
    "so that there is a significant difference between making big errors and small errors, which\n",
    "makes it easy to differentiate and select the best fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c34fe",
   "metadata": {},
   "source": [
    "**Note :** $Y_{predict}$ here is the values of Y predicted by our machine for some m or b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3421fd9",
   "metadata": {},
   "source": [
    " **Simple Linear Regression Using Sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e47a5",
   "metadata": {},
   "source": [
    "The process takes place in the following steps:\n",
    "\n",
    "1. Loading the Data\n",
    "2. Splitting the Data\n",
    "4. Generate The Model\n",
    "5. Evaluate The accuracy  \n",
    "**Loading the data**\n",
    "\n",
    "Here we will use a dummy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt(\"https://files.codingninjas.in/data-6984.csv\", delimiter = \",\")\n",
    "X = data[:, 0].reshape(-1, 1)\n",
    "Y = data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488ba84",
   "metadata": {},
   "source": [
    "**Splitting the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size = 0.3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33449fdd",
   "metadata": {},
   "source": [
    "**Generating the model** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25872293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    " \n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, Y_train)\n",
    " \n",
    "y_predict = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db97aa",
   "metadata": {},
   "source": [
    "**Evaluation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a04b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_self_implement = mean_squared_error(Y_test, y_predict)\n",
    " \n",
    "print(\"MSE\", mse_self_implement)\n",
    " \n",
    "weights = reg.coef_           # An array of weights corresponding to each feature \n",
    "intercept = reg.intercept_\n",
    "print(\"Coeffecient for x is:\", weights[0])\n",
    "print(\"Intercept value is:\", intercept)               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf600b1",
   "metadata": {},
   "source": [
    "To be more clear on how the data points look like on the graph, let us plot the graphs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.scatter(X_test, Y_test)\n",
    "plt.plot(X_test, y_predict)\n",
    "plt.xlabel(\"X test\")\n",
    "plt.ylabel(\"Predicted y (Line) / Test y (Scatter)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f339db",
   "metadata": {},
   "source": [
    "To get a feel of how good or bad our model is, we have plotted a graph. Since this dataset is small and single featured, the plot has given us an excellent idea regarding the correctness. But this may not always be the case. Hence we need a definite and objective way of finding this inference. For this we use something called the **Coefficient of Determination**.  \n",
    "  \n",
    "**Coefficient of Determination**   \n",
    "  \n",
    "The coefficient of determination, $R^2$, is used to analyze how differences in one variable can be explained by a difference in a second variable.\n",
    "\n",
    "Hence, the score can be calculated using the following formula:\n",
    "\n",
    "$$1 - \\frac{\\sum\\,(y\\,^T_i - y\\,^P_i)^2}{\\sum\\,(y\\,^T_i - y\\,^T_{mean})^2}$$\n",
    "\n",
    "T : true/actual value.\n",
    "\n",
    "P : predicted value.\n",
    "\n",
    "$y\\,^T_{mean}$ represents that we are predicting a score as worse as the mean value. (Meaning, all the answers are mean value of the data).\n",
    "\n",
    "To prove this, let us put \n",
    "$$ y\\,^P_i = y\\,^T_{mean}$$\n",
    "\n",
    "This will give the score as 0, which is the worst possible score.\n",
    "\n",
    "The usefulness of $R^2$ is its ability to find the likelihood of future events falling within the predicted outcomes. The idea is that if more samples are added, the coefficient would show the probability of a new point falling on the line.\n",
    "Even if there is a strong connection between the two variables, determination does not prove causality. For example, a study on birthdays may show a large number of birthdays happen within a time frame of one or two months. This does not mean that the passage of time or the change of seasons causes extra births.\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e75ecd",
   "metadata": {},
   "source": [
    "### 4. Cost Function  \n",
    "\n",
    "A cost function is defined as: a function that maps an event or values of one or more variables onto a real number intuitively representing some “cost” associated with the event.\n",
    "\n",
    "In Linear Regression the objective is to find a line of best fit for some given inputs, or X values, and any number of Y values, or outputs.\n",
    "  \n",
    "The different values for coefficient of lines gives different lines of regression, and the cost function is used to find the values of the coefficient for the best fit line. Cost function optimizes the regression coefficients. It measures how a linear regression model is performing.  \n",
    "  \n",
    "The error can be calculated using the formula :\n",
    "$\\sum_i$( $y_i$ - ($m_1.x_1 + m_2.x_2 + m_3.x_3 + … + m_n.x_n + b$ ) )\n",
    "\n",
    "But the error with this is - the negative and positive values will cancel each other. \n",
    "\n",
    "One possible way to fix this is by taking the Mod, but with this also the error \n",
    "will get added linearly which might give us incorrect analysis. So we will take the Mean Squared error(MSE). \n",
    "\n",
    "$Cost(C)= \\sum_i$ $( y_i - (m.x_i + b))^2$  \n",
    "  \n",
    "Let's find the value of m and b corresponding to which the cost function will be minimum. \n",
    "We will take the partial derivative of the cost function with respect to m and b separately to find their minimum values.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial m} = \\sum_i \\frac{\\partial}{\\partial m} ( y_i - (m.x_i + b))^2$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial m} =  \\sum_i 2( y_i - (m.x_i + b)) \\frac{\\partial}{\\partial m} ( y_i - (m.x_i + b))$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial m} =  \\sum_i 2( y_i - (m.x_i + b))(-x_i) = 0$$\n",
    "\n",
    "divide the whole equation by -2*N\n",
    "\n",
    "$$\\sum(x_iy_i/N - m \\sum x_i^2/N - b\\sum x_i/N = 0  $$\n",
    "\n",
    " \n",
    " $$(x*y).mean()- m(x^2.mean())- b(x.mean())=0$$\n",
    "\n",
    "Similarly by doing $dC/db=0$, we  will get\n",
    "\n",
    "$$ b= y.mean()-m*x.mean()$$  \n",
    "  \n",
    "Merging the above two equations,\n",
    "$$m=\\frac{(x*y).mean() - x.mean(). y.mean()}{x^2.mean()-x.mean().x.mean()}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f253a2",
   "metadata": {},
   "source": [
    "### 5. Coding Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for single variable \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85445b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:, 0].reshape(-1,1)\n",
    "y = data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fecd1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.3)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63045adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train):\n",
    "    num = (x_train*y_train).mean() - x_train.mean() * y_train.mean()\n",
    "    den = (x_train**2).mean() - x_train.mean()**2\n",
    "    m = num/den\n",
    "    c  = y_train.mean() - m*x_train.mean()\n",
    "    return m, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, m, c):\n",
    "    return m*x+c   # here x is numpy 1d array. This will multiply each x element with m. And + c will add c to each of the \n",
    "                   # element like we multiplied m.\n",
    "    \n",
    "def score(y_truth, y_pred):\n",
    "    u = ((y_truth - y_pred)**2).sum()  # this will do elementwise subtraction\n",
    "    v = ((y_truth - y_truth.mean())**2).sum()\n",
    "    return 1- u/v\n",
    "\n",
    "def cost(x, y, m , c):\n",
    "    return ((y -m *x - c)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ea9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, c = fit(x_train, y_train)\n",
    "# test data \n",
    "y_pred = predict(x_test, m, c)\n",
    "print('Test Score:' ,score(y_test, y_pred))\n",
    "\n",
    "# train data\n",
    "y_train_pred = predict(x_train, m, c)\n",
    "print('Train Score: ', score(y_train, y_train_pred))\n",
    "print(\"M : \", m , \"C: \", c)\n",
    "print(\"COst on Training Data \", cost(x_train, y_train, m ,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d791e53",
   "metadata": {},
   "source": [
    "### **Implementing Simple Linear Regression from Scratch** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e2f4a",
   "metadata": {},
   "source": [
    "Here we will use a dummy data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fc7a0",
   "metadata": {},
   "source": [
    "**Importing the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aabfb2",
   "metadata": {},
   "source": [
    "**Loading the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a635271",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"https://files.codingninjas.in/data-6984.csv\", delimiter=\",\")\n",
    "X = data[:, 0]\n",
    "Y = data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f298b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5466293",
   "metadata": {},
   "source": [
    "**Split the dataset into training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e79e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size = 0.3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0611b49",
   "metadata": {},
   "source": [
    "**Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccab2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to find the best fit line using the training data\n",
    "def fit(x_train, y_train):\n",
    "    num = (x_train * y_train).mean() - x_train.mean() * y_train.mean()\n",
    "    den = (x_train ** 2).mean() - x_train.mean() ** 2\n",
    "    m = num / den\n",
    "    c = y_train.mean() - m * x_train.mean()\n",
    "    return m, c    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e61ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predicts the value of 'y' corresponding to each 'x'\n",
    "def predict(x, m, c):\n",
    "    return m * x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85dc707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the score using the Y(actual) and Y(predited), coefficient of determination.\n",
    "def score(y_truth, y_pred): \n",
    "    u = ((y_truth - y_pred)**2).sum()\n",
    "    v = ((y_truth - y_truth.mean())**2).sum()\n",
    "    return 1 - u/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb38269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost (x, y, m , c):\n",
    "    return ((y - m * x - c)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f27a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, c = fit(X_train, Y_train)\n",
    "# Test data\n",
    "y_test_pred = predict(X_test, m, c)\n",
    "print(\"Test Score: \",score(Y_test, y_test_pred))\n",
    "\n",
    "# Train data\n",
    "y_train_pred = predict(X_train, m, c)\n",
    "print(\"Train Score:\", score(Y_train, y_train_pred))\n",
    "print(\"M:\", m)\n",
    "print(\"C:\", c)\n",
    "print(\"Cost on training data:\", cost(X_train,Y_train, m, c ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874032d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_classifier = mean_squared_error(Y_test, y_test_pred)\n",
    "# We calculated MSE earlier (In self implementation) \n",
    "print(\"MSE calculated using self implemented code :\",mse_self_implement)\n",
    "print(\"MSE calculated using Sklearn's implementation:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d73d4",
   "metadata": {},
   "source": [
    "### `Ridge Regression`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5280eda",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ridge regression is a regularization technique used in linear regression to address multicollinearity and prevent overfitting. It achieves this by adding a regularization term to the ordinary least squares (OLS) objective function. The regularization term is based on the L2 norm (squared Euclidean norm) of the coefficient vector.\n",
    "\n",
    "Let's derive the mathematical equation for ridge regression using variables \\(y\\), \\(m\\), \\(X\\), and \\(\\lambda\\):\n",
    "\n",
    "In linear regression, we have the equation:\n",
    "\n",
    "$$\n",
    "y = Xm + e\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(y\\) is the vector of observed values of the dependent variable,\n",
    "- \\(X\\) is the matrix of predictor variables,\n",
    "- \\(m\\) is the vector of coefficients (weights),\n",
    "- \\(e\\) is the vector of errors or residuals.\n",
    "\n",
    "The OLS objective function aims to minimize the sum of squared residuals:\n",
    "\n",
    "$$\n",
    "\\text{minimize: } \\sum(y - Xm)^2\n",
    "$$\n",
    "\n",
    "To introduce ridge regularization, we modify the objective function by adding a penalty term based on the L2 norm of the coefficient vector:\n",
    "\n",
    "$$\n",
    "\\text{minimize: } \\sum(y - Xm)^2 + \\lambda \\sum m^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\lambda$, (lambda) is the regularization parameter that controls the strength of regularization.\n",
    "\n",
    "The first term represents the sum of squared residuals, similar to the OLS objective function. The second term is the penalty term, which is the sum of the squared coefficients multiplied by the regularization parameter \\(\\lambda\\).\n",
    "\n",
    "\n",
    "This equation provides the ridge regression coefficient estimate, which incorporates the regularization term to address multicollinearity and control overfitting. The regularization parameter \\(\\lambda\\) controls the trade-off between fitting the data and the magnitude of the coefficients.\n",
    "\n",
    "By modifying the OLS objective function and adding the regularization term, ridge regression provides a mechanism to stabilize and improve the performance of linear regression models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607fcaf3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The ridge regression objective function in its transposed form is:  \n",
    "$$\n",
    "\\text{minimize: } (y - Xm)^T(y - Xm) + \\lambda m^Tm\n",
    "$$\n",
    "  \n",
    "Expand the first term:   \n",
    "  \n",
    "$$\n",
    "(y - Xm)^T(y - Xm) = y^Ty - 2m^TX^Ty + m^TX^TXm  \n",
    "$$\n",
    "  \n",
    "Expand the second term:  \n",
    "$$\n",
    "\\lambda m^Tm\n",
    "$$  \n",
    "  \n",
    "Combining the expanded terms:  \n",
    "$$\n",
    "\\text{minimize: } y^Ty - 2m^TX^Ty + m^TX^TXm + \\lambda m^Tm\n",
    "$$   \n",
    "  \n",
    "To find the optimal values for the coefficients, take the derivative with respect to $m$:  \n",
    "$$ \n",
    "\\frac{{\\partial}}{{\\partial m}}(-2X^Ty + 2X^TXm + 2\\lambda m) = 0\n",
    "$$  \n",
    "  \n",
    "Simplify the derivative:   \n",
    "$$\n",
    "-2X^Ty + 2X^TXm + 2\\lambda m = 0\n",
    "$$  \n",
    "  \n",
    "Divide the equation by 2:  \n",
    "$$\n",
    "-X^Ty + X^TXm + \\lambda m = 0\n",
    "$$\n",
    "  \n",
    "Rearrange the terms:  \n",
    "$$\n",
    "X^TXm + \\lambda m = X^Ty\n",
    "$$\n",
    "  \n",
    "Factor out $m$:\n",
    "$$\n",
    "(X^TX + \\lambda I)m = X^Ty\n",
    "$$  \n",
    "  \n",
    "To solve for $m$, multiply both sides by the inverse of $X^TX + \\lambda I$:\n",
    "$$\n",
    "m = (X^TX + \\lambda I)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "These steps outline the simplification process of the transposed form of the ridge regression derivation. The final equation provides the ridge regression coefficient estimate, which incorporates the regularization term to address multicollinearity and control overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e31f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'alpha': 0.1, 'copy_X': True, 'fit_intercept': True, 'max_iter': 100, 'positive': False, 'random_state': 42, 'solver': 'auto', 'tol': 0.001}\n",
      "Mean Squared Error: 21.89840819758996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "336 fits failed out of a total of 1344.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "336 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1011, in fit\n",
      "    return super().fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 712, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='svd' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [-37.65889149 -37.64374673          nan          nan -23.91642995\n",
      " -23.91642995 -23.91642995 -23.91642995 -37.65889149 -37.64374673\n",
      "          nan          nan -23.91642995 -23.91642995 -23.91642995\n",
      " -23.91642995 -72.60998816 -62.62146354          nan          nan\n",
      " -26.05940022 -26.05940022 -26.05940022 -26.05940022 -72.60998816\n",
      " -62.62146354          nan          nan -26.05940022 -26.05940022\n",
      " -26.05940022 -26.05940022 -37.65889149 -37.64374673          nan\n",
      "          nan -23.91642995 -23.91642995 -23.91642995 -23.91642995\n",
      " -37.65889149 -37.64374673          nan          nan -23.91642995\n",
      " -23.91642995 -23.91642995 -23.91642995 -72.60998816 -62.62146354\n",
      "          nan          nan -26.05940022 -26.05940022 -26.05940022\n",
      " -26.05940022 -72.60998816 -62.62146354          nan          nan\n",
      " -26.05940022 -26.05940022 -26.05940022 -26.05940022 -37.65713649\n",
      " -37.64241957          nan          nan -23.99822075 -23.99822075\n",
      " -23.99822075 -23.99822075 -37.65713649 -37.64241957          nan\n",
      "          nan -23.99822075 -23.99822075 -23.99822075 -23.99822075\n",
      " -72.61000007 -62.62219227          nan          nan -26.02127784\n",
      " -26.02127784 -26.02127784 -26.02127784 -72.61000007 -62.62219227\n",
      "          nan          nan -26.02127784 -26.02127784 -26.02127784\n",
      " -26.02127784 -37.65713649 -37.64241957          nan          nan\n",
      " -23.99822075 -23.99822075 -23.99822075 -23.99822075 -37.65713649\n",
      " -37.64241957          nan          nan -23.99822075 -23.99822075\n",
      " -23.99822075 -23.99822075 -72.61000007 -62.62219227          nan\n",
      "          nan -26.02127784 -26.02127784 -26.02127784 -26.02127784\n",
      " -72.61000007 -62.62219227          nan          nan -26.02127784\n",
      " -26.02127784 -26.02127784 -26.02127784 -37.65625953 -37.6412242\n",
      "          nan          nan -24.11448431 -24.11448431 -24.11448431\n",
      " -24.11448431 -37.65625953 -37.6412242           nan          nan\n",
      " -24.11448431 -24.11448431 -24.11448431 -24.11448431 -72.61001501\n",
      " -62.62315497          nan          nan -25.9911296  -25.9911296\n",
      " -25.9911296  -25.9911296  -72.61001501 -62.62315497          nan\n",
      "          nan -25.9911296  -25.9911296  -25.9911296  -25.9911296\n",
      " -37.65625953 -37.6412242           nan          nan -24.11448431\n",
      " -24.11448431 -24.11448431 -24.11448431 -37.65625953 -37.6412242\n",
      "          nan          nan -24.11448431 -24.11448431 -24.11448431\n",
      " -24.11448431 -72.61001501 -62.62315497          nan          nan\n",
      " -25.9911296  -25.9911296  -25.9911296  -25.9911296  -72.61001501\n",
      " -62.62315497          nan          nan -25.9911296  -25.9911296\n",
      " -25.9911296  -25.9911296 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "X = data\n",
    "y= target\n",
    "# Define the Ridge regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0],\n",
    "    'fit_intercept': [True, False],\n",
    "    #'normalize': [True, False],\n",
    "    'solver': ['auto', 'svd'],\n",
    "    'tol': [1e-3, 1e-4],\n",
    "    'copy_X': [True, False],\n",
    "    'max_iter': [100, 200],\n",
    "    'random_state': [42],\n",
    "    'positive': [True, False]\n",
    "}\n",
    "\n",
    "# Define the K-fold cross-validation\n",
    "kfold = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform GridSearchCV with K-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=kfold, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X)\n",
    "mse = mean_squared_error(y, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fdd6f0",
   "metadata": {},
   "source": [
    "### `Lasso Regression`\n",
    "\n",
    "Lasso regression, short for Least Absolute Shrinkage and Selection Operator, is a linear regression technique that incorporates a regularization term to promote sparsity in the coefficient estimates. It aims to minimize the sum of squared residuals while simultaneously minimizing the sum of the absolute values of the coefficients.\n",
    "\n",
    "The Lasso regression objective function can be written as follows:\n",
    "\n",
    "$$\n",
    "\\text{minimize: } \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - \\sum_{j=1}^{m}x_{ij}m_j)^2 + \\lambda \\sum_{j=1}^{m}|m_j|\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "- $n$ represents the number of samples in the dataset.\n",
    "- $m$ denotes the number of predictor variables.\n",
    "- $y_i$ is the observed value for the $i^{th}$ sample.\n",
    "- $x_{ij}$ represents the value of the $j^{th}$ predictor variable for the $i^{th}$ sample.\n",
    "- $m_j$ denotes the coefficient estimate for the $j^{th}$ predictor variable.\n",
    "- $\\lambda$ is the regularization parameter that controls the amount of regularization applied.\n",
    "\n",
    "To derive the mathematical equation for the Lasso regression coefficient estimates, we need to minimize the objective function with respect to the coefficients $m_j$. Taking the derivative with respect to $m_j$ yields:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial m_j} \\left( \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - \\sum_{j=1}^{m}x_{ij}m_j)^2 + \\lambda \\sum_{j=1}^{m}|m_j| \\right) = 0\n",
    "$$\n",
    "\n",
    "Simplifying the derivative, we get:\n",
    "\n",
    "$$\n",
    "-\\frac{1}{n} \\sum_{i=1}^{n}x_{ij}(y_i - \\sum_{j=1}^{m}x_{ij}m_j) + \\lambda \\frac{m_j}{|m_j|} = 0\n",
    "$$\n",
    "\n",
    "Next, we consider the subgradient due to the non-differentiability introduced by the absolute value term. The subgradient of the absolute value function is:\n",
    "\n",
    "$$\n",
    "\\text{sgn}(m_j) = \n",
    "\\begin{cases}\n",
    "-1, & \\text{if } m_j < 0 \\\\\n",
    "[-1, 1], & \\text{if } m_j = 0 \\\\\n",
    "1, & \\text{if } m_j > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "By substituting the subgradient into the equation and rearranging terms, we arrive at the Lasso regression coefficient estimates:\n",
    "\n",
    "$$\n",
    "m_j = \\frac{1}{n} \\sum_{i=1}^{n}x_{ij}(y_i - \\sum_{k \\neq j}x_{ik}m_k) - \\frac{\\lambda}{2n}\\text{sgn}(m_j)\n",
    "$$\n",
    "\n",
    "This equation demonstrates how the Lasso regression coefficients are estimated by balancing the least squares loss term with the regularization term. The L1 regularization encourages sparsity by promoting coefficients to be exactly zero for certain predictors, effectively performing variable selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822c7c3",
   "metadata": {},
   "source": [
    "\n",
    "The Lasso regression objective function in its transposed form is:\n",
    "   $$\n",
    "   \\text{minimize: } \\frac{1}{2n} \\left( \\mathbf{y} - \\mathbf{Xm} \\right)^T \\left( \\mathbf{y} - \\mathbf{Xm} \\right) + \\lambda \\left\\| \\mathbf{m} \\right\\|_1\n",
    "   $$\n",
    "   where $\\mathbf{y}$ is the vector of observed values, $\\mathbf{X}$ is the matrix of predictor variables, $\\mathbf{m}$ is the vector of coefficient estimates, $n$ is the number of samples, and $\\lambda$ is the regularization parameter.\n",
    "\n",
    "Expand the first term:\n",
    "   $$\n",
    "   \\frac{1}{2n} \\left( \\mathbf{y}^T \\mathbf{y} - 2\\mathbf{m}^T \\mathbf{X}^T \\mathbf{y} + \\mathbf{m}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{m} \\right)\n",
    "   $$\n",
    "\n",
    "Expand the second term:\n",
    "   $$\n",
    "   \\lambda \\left( \\left| m_1 \\right| + \\left| m_2 \\right| + \\ldots + \\left| m_m \\right| \\right)\n",
    "   $$\n",
    "\n",
    "Simplify the second term using the L1 norm notation:\n",
    "   $$\n",
    "   \\lambda \\left\\| \\mathbf{m} \\right\\|_1\n",
    "   $$\n",
    "\n",
    "Combining both terms, we get the simplified transposed form of the Lasso regression objective function:\n",
    "   $$\n",
    "   \\text{{minimize: }} \\frac{1}{2n} \\left( \\mathbf{y}^T \\mathbf{y} - 2\\mathbf{m}^T \\mathbf{X}^T \\mathbf{y} + \\mathbf{m}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{m} \\right) + \\lambda \\left\\| \\mathbf{m} \\right\\|_1\n",
    "   $$\n",
    "\n",
    "Each step in the simplification process helps to break down the original equation and make it more explicit. By expanding and rearranging the terms, we arrive at the final simplified form of the transposed Lasso regression objective function, which involves the sum of squared residuals and the L1 norm of the coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348563e",
   "metadata": {},
   "source": [
    "##### `Detailed Derivation`  \n",
    "\n",
    "\n",
    "Start with the Lasso regression objective function:\n",
    "$$\n",
    "\\text{minimize: } \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - \\sum_{j=1}^{m}x_{ij}m_j)^2 + \\lambda \\sum_{j=1}^{m}|m_j|\n",
    "$$\n",
    "\n",
    "Take the derivative with respect to the coefficient $m_j$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial m_j} \\left( \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - \\sum_{j=1}^{m}x_{ij}m_j)^2 + \\lambda \\sum_{j=1}^{m}|m_j| \\right) = 0\n",
    "$$\n",
    "\n",
    "Expand the squared term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial m_j} \\left( \\frac{1}{2n} \\sum_{i=1}^{n} (y_i^2 - 2y_i \\sum_{j=1}^{m}x_{ij}m_j + \\sum_{j=1}^{m}\\sum_{k=1}^{m}x_{ij}x_{ik}m_jm_k) + \\lambda \\sum_{j=1}^{m}|m_j| \\right) = 0\n",
    "$$\n",
    "\n",
    "Simplify the derivative of the squared term:\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} x_{ij}(y_i - \\sum_{k\\neq j}x_{ik}m_k) + \\lambda \\frac{\\partial}{\\partial m_j} |m_j| = 0\n",
    "$$\n",
    "\n",
    "Consider the subgradient of the absolute value term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial m_j} |m_j| = \\text{sgn}(m_j)\n",
    "$$\n",
    "(The sign function is a mathematical function that returns the sign of a real number. It returns -1 if the number is negative, 0 if the number is zero, and 1 if the number is positive. In the context of Lasso regression, the sign function is used to determine the sign of the coefficients of the linear model being estimated. Specifically, it is used to shrink some of the coefficients to exactly zero, leading to sparse models.)\n",
    "Substitute the subgradient into the equation:\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} x_{ij}(y_i - \\sum_{k\\neq j}x_{ik}m_k) + \\lambda \\text{sgn}(m_j) = 0\n",
    "$$\n",
    "\n",
    "Rearrange terms to isolate $m_j$:\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} x_{ij}(y_i - \\sum_{k\\neq j}x_{ik}m_k) = -\\lambda \\text{sgn}(m_j)\n",
    "$$\n",
    "\n",
    "Remove the absolute value by considering the three cases for the sign of $m_j$:\n",
    "   - If $m_j > 0$:\n",
    "     $$\n",
    "     \\frac{1}{n} \\sum_{i=1}^{n} x_{ij}(y_i - \\sum_{k\\neq j}x_{ik}m_k) = -\\lambda\n",
    "     $$\n",
    "   - If $m_j < 0$:\n",
    "     $$\n",
    "     \\frac{1}{n} \\sum_{i=1}^{n} x_{ij}(y_i - \\sum_{k\\neq j}x_{ik}m_k) = \\lambda\n",
    "     $$\n",
    "   - If $m_j = 0$:\n",
    "     $$\n",
    "     -\\frac{\\lambda}{n} \\leq \\frac{1}{n} \\sum_{i=1}^{n} x_{ij}(y_i - \\sum_{\n",
    "k\\neq j}x_{ik}m_k) \\leq \\frac{\\lambda}{n}\n",
    "     $$\n",
    "\n",
    "Solve for $m_j$ for each case:\n",
    "   - If $m_j > 0$:\n",
    "     $$\n",
    "     m_j = \\frac{1}{n} \\sum_{i=1}^{n} x_{ij}(y_i - \\sum_{k\\neq j}x_{ik}m_k) - \\lambda\n",
    "     $$\n",
    "   - If $m_j < 0$:\n",
    "     $$\n",
    "     m_j = \\frac{1}{n} \\sum_{i=1}^{n} x_{ij}(y_i - \\sum_{k\\neq j}x_{ik}m_k) + \\lambda\n",
    "     $$\n",
    "   - If $m_j = 0$:\n",
    "     $$\n",
    "     |m_j| \\leq \\frac{\\lambda}{n}\n",
    "     $$\n",
    "\n",
    "These equations represent the Lasso regression coefficient estimates. The coefficient $m_j$ is determined based on the contribution of the predictor $x_{ij}$ to the residual error, adjusted by the regularization term. Depending on the sign and magnitude of the coefficients, the Lasso regression promotes sparsity by encouraging some coefficients to be exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47512e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.1, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'precompute': True, 'tol': 0.001}\n",
      "Best Score: 0.8913166946448434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the Lasso regression model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5],  # Range of values for regularization parameter alpha\n",
    "    'fit_intercept': [True, False],  # Whether to include an intercept term in the model\n",
    "    #'normalize': [True, False],  # Whether to normalize the predictor variables  # deprecated\n",
    "    'precompute': [True, False],  # Whether to use precomputed Gram matrix for faster calculations\n",
    "    'copy_X': [True, False],  # Whether to copy the predictor variables or overwrite them\n",
    "    'max_iter': [1000, 2000, 3000],  # Maximum number of iterations for convergence\n",
    "    'tol': [0.001, 0.01, 0.1]  # Tolerance for convergence\n",
    "}\n",
    "\n",
    "\n",
    "# Define the k-fold cross-validation object\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, cv=kfold)\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcba0ce",
   "metadata": {},
   "source": [
    "### `Elastic Net Regression`    \n",
    "Elastic Net regularization is a technique that combines both L1 and L2 regularization in a linear regression model. It adds a linear combination of the absolute and squared magnitudes of the coefficients as a penalty term to the loss function. This approach allows for feature selection (sparsity) while also providing a balance between the L1 and L2 regularization effects.\n",
    "\n",
    "Let's consider the following mathematical formulation for Elastic Net regularization in a linear regression context:\n",
    "\n",
    "Given:\n",
    "- Input matrix: \\(X\\) of size \\(n \\times p\\), where \\(n\\) is the number of samples and \\(p\\) is the number of features.\n",
    "- Target vector: \\(y\\) of length \\(n\\).\n",
    "- Coefficient vector: \\(m\\) of length \\(p\\).\n",
    "- Regularization parameters: \\(\\lambda_1\\) (L1 regularization) and \\(\\lambda_2\\) (L2 regularization).\n",
    "\n",
    "The Elastic Net loss function can be defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2n} \\left\\| y - Xm \\right\\|^2 + \\lambda_1 \\left\\| m \\right\\|_1 + \\lambda_2 \\left\\| m \\right\\|_2^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The first term represents the mean squared error between the predicted values \\(Xm\\) and the actual values \\(y\\).\n",
    "- The second term is the L1 regularization term that encourages sparsity and promotes feature selection.\n",
    "- The third term is the L2 regularization term that encourages small but non-zero coefficients.\n",
    "\n",
    "The Elastic Net regularization aims to find the optimal coefficient vector \\(m\\) by minimizing this loss function.\n",
    "\n",
    "To derive the equation, we need to minimize the loss function with respect to the coefficient vector \\(m\\). Taking the derivative of the loss function with respect to \\(m\\) and setting it to zero, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial m} \\left( \\frac{1}{2n} \\left\\| y - Xm \\right\\|^2 + \\lambda_1 \\left\\| m \\right\\|_1 + \\lambda_2 \\left\\| m \\right\\|_2^2 \\right) = 0\n",
    "$$\n",
    "\n",
    "Simplifying this equation will yield the mathematical derivation of Elastic Net regularization. However, due to the complexity of the derivation and limitations in the text format, it is not feasible to provide a step-by-step derivation with LaTeX equations here.\n",
    "\n",
    "In practice, you can use optimization algorithms or libraries that provide Elastic Net implementations, such as scikit-learn in Python, to find the optimal coefficient vector \\(m\\) by minimizing the loss function. These libraries utilize efficient numerical methods to solve the optimization problem and find the best regularization parameters \\(\\lambda_1\\) and \\(\\lambda_2\\) through techniques like cross-validation or grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357028e",
   "metadata": {},
   "source": [
    "\n",
    "Starting with the Elastic Net objective function:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m}(y_i - \\beta_0 - \\sum_{j=1}^{n}\\beta_jx_{ij})^2 + \\lambda_1 \\sum_{j=1}^{n}|\\beta_j| + \\lambda_2 \\sum_{j=1}^{n}\\beta_j^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $J(\\beta)$ is the objective function to minimize\n",
    "- $m$ is the number of samples\n",
    "- $n$ is the number of features\n",
    "- $y_i$ is the target variable for the $i^{th}$ sample\n",
    "- $\\beta_0$ is the intercept term\n",
    "- $\\beta_j$ represents the $j^{th}$ coefficient\n",
    "- $x_{ij}$ is the $j^{th}$ feature for the $i^{th}$ sample\n",
    "- $\\lambda_1$ is the L1 regularization parameter\n",
    "- $\\lambda_2$ is the L2 regularization parameter\n",
    "\n",
    "To compute the derivative, we need to differentiate each term of the objective function with respect to $\\beta_j$.\n",
    "\n",
    "Derivative of the squared error term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_j}\\left(\\frac{1}{2m} \\sum_{i=1}^{m}(y_i - \\beta_0 - \\sum_{j=1}^{n}\\beta_jx_{ij})^2\\right)\n",
    "$$\n",
    "\n",
    "The derivative of this term with respect to $\\beta_j$ is:\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^{m}(y_i - \\beta_0 - \\sum_{j=1}^{n}\\beta_jx_{ij})(-x_{ij})\n",
    "$$\n",
    "\n",
    "Derivative of the L1 regularization term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_j}\\left(\\lambda_1 \\sum_{j=1}^{n}|\\beta_j|\\right)\n",
    "$$\n",
    "\n",
    "The derivative of this term with respect to $\\beta_j$ is:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\cdot \\text{sign}(\\beta_j)\n",
    "$$\n",
    "\n",
    "where $\\text{sign}(\\beta_j)$ is the sign function of $\\beta_j$, which returns -1 for negative values, 0 for zero, and 1 for positive values.\n",
    "\n",
    "Derivative of the L2 regularization term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_j}\\left(\\lambda_2 \\sum_{j=1}^{n}\\beta_j^2\\right)\n",
    "$$\n",
    "\n",
    "The derivative of this term with respect to $\\beta_j$ is:\n",
    "\n",
    "$$\n",
    "2 \\lambda_2 \\beta_j\n",
    "$$\n",
    "\n",
    "Finally, combining the derivatives of all three terms, the derivative of the Elastic Net objective function with respect to $\\beta_j$ is:\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^{m}(y_i - \\beta_0 - \\sum_{j=1}^{n}\\beta_jx_{ij})(-x_{ij}) + \\lambda_1 \\cdot \\text{sign}(\\beta_j) + 2 \\lambda_2 \\beta_j\n",
    "$$\n",
    "\n",
    "(The sign function is a mathematical function that returns the sign of a real number. It returns -1 if the number is negative, 0 if the number is zero, and 1 if the number is positive. In the context of Lasso regression, the sign function is used to determine the sign of the coefficients of the linear model being estimated. Specifically, it is used to shrink some of the coefficients to exactly zero, leading to sparse models.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9efd06",
   "metadata": {},
   "source": [
    "Transposed form.  \n",
    "\n",
    "\n",
    "Start with the linear regression equation:\n",
    "$$\n",
    "\\hat{y} = X \\cdot m + c\n",
    "$$\n",
    "where $\\hat{y}$ represents the predicted output, $X$ represents the design matrix of input features, $m$ represents the vector of regression coefficients, and $c$ represents the intercept term.\n",
    "\n",
    "Introduce the L1 and L2 regularization terms:\n",
    "$$\n",
    "\\hat{y} = X \\cdot m + c + \\lambda_1 \\cdot \\left\\| m \\right\\|_1 + \\lambda_2 \\cdot \\left\\| m \\right\\|_2^2\n",
    "$$\n",
    "where $\\lambda_1$ and $\\lambda_2$ represent the regularization parameters.\n",
    "\n",
    "Rearrange the equation to separate the regularization terms:\n",
    "$$\n",
    "\\hat{y} = X \\cdot m + c + \\lambda_1 \\cdot \\left\\| m \\right\\|_1 + \\lambda_2 \\cdot m^T \\cdot m\n",
    "$$\n",
    "\n",
    "Transpose the equation:\n",
    "$$\n",
    "\\hat{y} = m^T \\cdot X^T + c + \\lambda_1 \\cdot \\left\\| m \\right\\|_1 + \\lambda_2 \\cdot m^T \\cdot m\n",
    "$$\n",
    "\n",
    "Simplify the equation to obtain the final transposed form of the elastic net equation: \n",
    "$$\n",
    "\\hat{y} = m^T \\cdot X^T + c + \\lambda_1 \\cdot \\left\\| m \\right\\|_1 + \\lambda_2 \\cdot m^T \\cdot m\n",
    "$$\n",
    "\n",
    "Please note that in the transposed form, the design matrix $X$ is transposed to $X^T$ to align with the vector $m$ during matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d5263b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'alpha': 0.1, 'fit_intercept': False, 'l1_ratio': 0.2, 'precompute': True, 'selection': 'random'}\n",
      "Best Score:  0.9124633761619002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the Elastic Net model\n",
    "model = ElasticNet()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 10.0],    # Regularization parameter\n",
    "    'l1_ratio': [0.2, 0.4, 0.6, 0.8],    # Mixing parameter between L1 and L2 regularization\n",
    "    'fit_intercept': [True, False],      # Whether to include an intercept term in the model\n",
    "    #'normalize': [True, False],          # Whether to normalize the input features\n",
    "    'precompute': [True, False],         # Whether to precompute the Gram matrix to speed up calculations\n",
    "    'selection': ['cyclic', 'random']    # Method for feature selection during optimization\n",
    "}\n",
    "\n",
    "# Perform k-fold cross-validation with GridSearchCV\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=kfold)\n",
    "\n",
    "# Fit the model to find the best parameters\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8435a",
   "metadata": {},
   "source": [
    "### `Laplacian Regularization`\n",
    "Laplacian regularization is a technique used in machine learning to encourage smoothness or continuity in the learned model. It is particularly useful in tasks where the underlying data has a spatial or temporal structure. Let's dive into the details and derivation of Laplacian regularization.\n",
    "\n",
    "Consider a regression problem where we have a set of input features $X$ and corresponding output targets $y$. We aim to estimate the parameters $\\theta$ of the model. Laplacian regularization introduces a penalty term that encourages the learned function to have a smooth variation over the input space.\n",
    "\n",
    "To explain Laplacian regularization, we first define a graph $G$ that represents the relationships or connections between the input data points. Each data point corresponds to a node in the graph, and the connections between nodes indicate the similarity or proximity between the corresponding data points.\n",
    "\n",
    "Now, let's denote $W$ as the weighted adjacency matrix of the graph $G$. The elements $W_{ij}$ of the matrix represent the strength of the connection between nodes $i$ and $j$. Typically, these weights are determined based on the distance or similarity measure between the data points.\n",
    "\n",
    "The Laplacian regularization term can be defined as follows:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{reg}} = \\theta^T L \\theta$$\n",
    "\n",
    "Here, $L$ is the graph Laplacian matrix, given by $L = D - W$, where $D$ is the diagonal matrix of node degrees, with $D_{ii} = \\sum_j W_{ij}$. The Laplacian matrix captures the local structure and smoothness properties of the graph.\n",
    "\n",
    "The objective function for Laplacian regularization is:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2}(y - X\\theta)^T(y - X\\theta) + \\alpha \\theta^T L \\theta$$\n",
    "\n",
    "In this equation, the first term represents the data fidelity term, which measures the difference between the predicted output $X\\theta$ and the true output $y$. The second term is the Laplacian regularization term, which penalizes the deviation of the learned parameters from smoothness.\n",
    "\n",
    "To find the optimal parameters, we need to minimize the objective function $J(\\theta)$. Taking the derivative of $J(\\theta)$ with respect to $\\theta$ and setting it to zero, we can derive the expression for the optimal parameters:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta} = -X^T(y - X\\theta) + \\alpha L \\theta = 0$$\n",
    "\n",
    "Rearranging the equation, we get:\n",
    "\n",
    "$$\\theta = (X^TX + \\alpha L)^{-1}X^Ty$$\n",
    "\n",
    "This equation provides the solution for the optimal parameters $\\theta$ that minimizes the objective function with Laplacian regularization.\n",
    "\n",
    "The Laplacian regularization term encourages smoothness by penalizing sharp variations between neighboring data points. The strength of regularization is controlled by the hyperparameter $\\alpha$. A larger value of $\\alpha$ emphasizes smoothness more strongly, while a smaller value allows for more flexibility and less smoothness.\n",
    "\n",
    "By incorporating Laplacian regularization, the learned model is encouraged to have a smooth variation over the input space, capturing the underlying structure or dependencies in the data. This can lead to improved generalization performance, especially when dealing with spatial or temporal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e879a4",
   "metadata": {},
   "source": [
    "\n",
    "Define the Laplacian regularization term:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{reg}} = \\theta^T L \\theta$$\n",
    "\n",
    "Define the objective function with Laplacian regularization:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2}(y - X\\theta)^T(y - X\\theta) + \\alpha \\theta^T L \\theta$$\n",
    "\n",
    "Calculate the derivative of the objective function with respect to \\(\\theta\\):\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta} = -X^T(y - X\\theta) + \\alpha L \\theta$$\n",
    "\n",
    "Set the derivative to zero and solve for $\\theta$:\n",
    "\n",
    "$$-X^T(y - X\\theta) + \\alpha L \\theta = 0$$\n",
    "\n",
    "Rearrange the equation to isolate $\\theta$:\n",
    "\n",
    "$$\\theta = (X^TX + \\alpha L)^{-1}X^Ty$$\n",
    "\n",
    "This equation provides the solution for the optimal parameters $\\theta$ that minimizes the objective function with Laplacian regularization.\n",
    "\n",
    "By incorporating this Laplacian regularization term into the objective function, the learned model is encouraged to exhibit smoothness or continuity over the input space, capturing the underlying structure or dependencies in the data. The hyperparameter $\\alpha$ controls the strength of regularization, allowing for a trade-off between smoothness and flexibility in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc2539",
   "metadata": {},
   "source": [
    "### `Bootstrap Aggregation`  \n",
    "Bootstrap aggregation, also known as bagging, is an ensemble learning technique that combines multiple models to reduce variance and improve the overall predictive performance. It involves training multiple models on different subsets of the training data, created through random sampling with replacement, and then averaging the predictions of these models to obtain the final prediction. Let's go through the details and derivation of bootstrap aggregation.\n",
    "\n",
    "Suppose we have a training dataset with $N$ samples and $D$ features, denoted as $X = \\{x_1, x_2, ..., x_N\\}$ and corresponding targets $y = \\{y_1, y_2, ..., y_N\\}$.\n",
    "\n",
    "The steps involved in bootstrap aggregation are as follows:\n",
    "\n",
    "Step 1: Randomly draw a subset of the training data of size $N$ with replacement. This means that each sample in the subset is chosen independently with the same probability, and some samples may appear multiple times while others may be omitted.\n",
    "\n",
    "Step 2: Build a base model using the selected subset of data. This could be any machine learning algorithm, such as a decision tree, random forest, or support vector machine.\n",
    "\n",
    "Step 3: Repeat steps 1 and 2 $M$ times to create $M$ different subsets and train $M$ base models.\n",
    "\n",
    "Step 4: Given a new test sample, make predictions using all $M$ models and combine the predictions. The combination can be done through averaging (for regression problems) or voting (for classification problems).\n",
    "\n",
    "The goal of bootstrap aggregation is to reduce the variance of the predictions by leveraging the diversity among the $M$ \n",
    "base models.   \n",
    "In simple terms, the sentence means that the aim of bootstrap aggregation (bagging) is to make more accurate predictions by reducing the variability in the predictions. This is done by using multiple different models that each learn and predict in their own unique way. By combining the predictions of these diverse models, the overall prediction becomes more reliable and less likely to be influenced by individual model biases or limitations. The idea is to take advantage of the differences among the models to improve the overall accuracy and stability of the predictions.\n",
    "The idea is that by training each model on a slightly different subset of the data, the models will capture different aspects of the underlying patterns and provide more robust predictions when combined.  \n",
    "  \n",
    "The derivation of bootstrap aggregation involves understanding the effect of random sampling with replacement on the variance of the aggregated predictions. Let's denote the predicted value of the $m^{th}$ model for the $i^{th}$ sample as $y_{im}$. The aggregated prediction for the $i^{th}$ sample is then given by:\n",
    "\n",
    "$$\\hat{y}_i = \\frac{1}{M} \\sum_{m=1}^{M} y_{im}$$\n",
    "\n",
    "The variance of the aggregated prediction is calculated as follows:\n",
    "\n",
    "$$\\text{Var}(\\hat{y}_i) = \\frac{1}{M^2} \\sum_{m=1}^{M} \\text{Var}(y_{im}) + \\frac{2}{M^2} \\sum_{m=1}^{M-1} \\sum_{n=m+1}^{M} \\text{Cov}(y_{im}, y_{in})$$\n",
    "\n",
    "Now, since we are using bootstrap sampling with replacement, each $y_{im}$ is obtained from a different subset of data. Therefore, the covariance terms in the second summation will be close to zero, resulting in:\n",
    "\n",
    "$$\\text{Var}(\\hat{y}_i) \\approx \\frac{1}{M} \\text{Var}(y_{im})$$\n",
    "\n",
    "As $M$ increases, the variance of the aggregated prediction decreases, leading to a more stable and reliable prediction.\n",
    "\n",
    "Bootstrap aggregation is a powerful technique that helps improve the performance and robustness of machine learning models by reducing variance and leveraging the diversity among models. It is widely used in ensemble methods such as random forests and bagging.   \n",
    "  \n",
    "The voting process differs depending on whether the task is a classification problem or a regression problem.\n",
    "  \n",
    "Classification: In classification tasks, each model in the ensemble predicts the class label for a given input sample. The most common voting strategies are:  \n",
    "\n",
    "a. Majority Voting: Each model's prediction is considered as one vote, and the class label with the most votes is chosen as the final prediction. For example, if there are three models, and their predictions for a sample are \"Class A,\" \"Class A,\" and \"Class B,\" the majority voting would select \"Class A\" as the final prediction.\n",
    "\n",
    "b. Weighted Voting: Each model's prediction is assigned a weight, and the class label with the highest weighted sum is chosen as the final prediction. The weights can be determined based on the performance or confidence of the individual models. For instance, more accurate models may have higher weights assigned to their predictions.  \n",
    "\n",
    "Regression: In regression tasks, each model in the ensemble predicts a continuous value for a given input sample. The most common voting strategy is:  \n",
    "\n",
    "a. Average Voting: Each model's prediction is considered, and the final prediction is obtained by averaging the predictions of all the models. For example, if there are three models, and their predictions for a sample are 5.2, 4.8, and 5.0, the average voting would yield a final prediction of (5.2 + 4.8 + 5.0) / 3 = 5.0.  \n",
    "\n",
    "It's important to note that voting in bootstrap aggregation assumes equal weightage for each model's prediction. However, alternative methods such as assigning different weights to models or using more sophisticated combination techniques like stacking or boosting can also be explored.  \n",
    "\n",
    "The goal of voting in bootstrap aggregation is to leverage the collective knowledge and diversity of the models in the ensemble to make a more accurate and robust prediction. By combining the predictions of multiple models, the ensemble is less likely to be influenced by the biases or limitations of individual models, leading to improved overall performance.   \n",
    "\n",
    "---\n",
    "To calculate the variance in bootstrap aggregation, you need to perform the following steps:\n",
    "\n",
    "1. Create multiple bootstrap samples: Generate multiple subsets of the original dataset by randomly sampling with replacement. Each subset should be of the same size as the original dataset.\n",
    "\n",
    "2. Train individual models: Train a separate model on each bootstrap sample. You can use any machine learning algorithm of your choice.\n",
    "\n",
    "3. Make predictions: Use each individual model to make predictions on the original dataset or on new, unseen data.\n",
    "\n",
    "4. Calculate the variance: Calculate the variance of the predictions across all the models. Variance measures the spread or variability of the predictions. It can be computed using the following formula:\n",
    "\n",
    "   $$\n",
    "   \\text{{Variance}} = \\frac{{\\sum_{i=1}^{M} (y_i - \\bar{y})^2}}{{M-1}}\n",
    "   $$\n",
    "\n",
    "   Here, $y_i$ represents the prediction of the $i$-th model, $\\bar{y}$ represents the average prediction across all models, and $M$ represents the total number of models.\n",
    "\n",
    "   The variance captures how much the predictions differ from each other. A lower variance indicates that the models have similar predictions, while a higher variance suggests more diversity among the models.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say we have three models in our bootstrap aggregation ensemble, and their predictions on a specific data point are 5, 6, and 7. To calculate the variance, we follow these steps:\n",
    "\n",
    "1. Calculate the average prediction:\n",
    "   $\\bar{y} = \\frac{{5 + 6 + 7}}{3} = 6$\n",
    "\n",
    "2. Calculate the squared differences between each prediction and the average prediction:\n",
    "   $(5 - 6)^2 = 1$\n",
    "   $(6 - 6)^2 = 0$\n",
    "   $(7 - 6)^2 = 1$\n",
    "\n",
    "3. Sum up the squared differences:\n",
    "   $1 + 0 + 1 = 2$\n",
    "\n",
    "4. Divide the sum by the number of models minus one:\n",
    "   $\\text{{Variance}} = \\frac{2}{3 - 1} = 1$\n",
    "\n",
    "In this example, the variance of the predictions is 1. A variance of 1 indicates that the models' predictions vary by an average of 1 unit from the ensemble's average prediction.  \n",
    "\n",
    "---\n",
    "After combining the predictions in bagging, the next step typically depends on the specific task you are working on, whether it is a classification or regression problem. Here are the common steps taken after combining predictions in bagging:\n",
    "\n",
    "1. **Classification**:\n",
    "   - Majority Voting: If you used majority voting to combine the predictions, you can simply select the class label that received the most votes as the final prediction.\n",
    "   - Probability Estimation: If your models provide probability estimates, you can average the probabilities across the ensemble and choose the class label with the highest average probability as the final prediction.\n",
    "\n",
    "2. **Regression**:\n",
    "   - Averaging: If you used averaging to combine the predictions, you can directly use the average of the predicted values as the final prediction.\n",
    "\n",
    "3. **Post-processing**:\n",
    "   - Depending on the problem and specific requirements, you might perform post-processing steps on the combined predictions. For example, you might apply thresholding, scaling, or other transformations to ensure the predictions align with the desired output format or business rules.\n",
    "\n",
    "It's important to note that the post-processing steps may vary based on the specific problem and the characteristics of the dataset. The primary goal is to obtain the final prediction that best represents the combined knowledge and predictions of the ensemble models.\n",
    "\n",
    "Ultimately, the combined predictions in bagging aim to improve the accuracy and stability of the model by leveraging the diversity among the individual models.  \n",
    "\n",
    "---\n",
    "Certainly! Let's consider a regression problem and demonstrate an example of bagging with numerical values. \n",
    "\n",
    "Suppose we have a dataset with the following target values:\n",
    "\n",
    "[10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n",
    "\n",
    "We will create three bootstrap samples by randomly selecting samples with replacement. Each sample will have the same number of data points as the original dataset.\n",
    "\n",
    "Bootstrap Sample 1: [10, 14, 16, 18, 24, 26, 26, 28, 28, 28]\n",
    "\n",
    "Bootstrap Sample 2: [12, 14, 16, 18, 18, 18, 20, 22, 22, 24]\n",
    "\n",
    "Bootstrap Sample 3: [10, 10, 14, 14, 16, 18, 18, 22, 24, 26]\n",
    "\n",
    "Now, we train three separate regression models on these bootstrap samples:\n",
    "\n",
    "Model 1: Trained on Bootstrap Sample 1\n",
    "\n",
    "Model 2: Trained on Bootstrap Sample 2\n",
    "\n",
    "Model 3: Trained on Bootstrap Sample 3\n",
    "\n",
    "Let's assume the models make the following predictions on a new data point:\n",
    "\n",
    "Model 1 prediction: 25\n",
    "\n",
    "Model 2 prediction: 22\n",
    "\n",
    "Model 3 prediction: 20\n",
    "\n",
    "To obtain the final prediction using bagging, we can average the predictions of the three models:\n",
    "\n",
    "Final prediction = (25 + 22 + 20) / 3 = 22.33\n",
    "\n",
    "In this example, after combining the predictions from three individual models using bagging, we obtain a final prediction of approximately 22.33.\n",
    "\n",
    "Note that this is a simplified example with a small dataset, but in practice, bagging involves creating a larger number of bootstrap samples and training a collection of models on each sample to further reduce variance and improve prediction accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "Suppose we have a dataset with 100 samples, each labeled as either \"A\" or \"B\". We want to build a bagging ensemble with three decision tree models.\n",
    "\n",
    "1. **Bootstrap Sampling**: We randomly select subsets from the original dataset with replacement to create three bootstrap samples. Let's say the samples selected for each bootstrap sample are as follows:\n",
    "\n",
    "   Bootstrap Sample 1: [A, A, B, B, A, B, A, B, A, B]\n",
    "\n",
    "   Bootstrap Sample 2: [A, A, A, B, B, B, A, B, A, A]\n",
    "\n",
    "   Bootstrap Sample 3: [A, B, B, B, B, A, A, B, A, B]\n",
    "\n",
    "2. **Training Individual Models**: We train three decision tree models on these bootstrap samples:\n",
    "\n",
    "   Model 1: Trained on Bootstrap Sample 1\n",
    "\n",
    "   Model 2: Trained on Bootstrap Sample 2\n",
    "\n",
    "   Model 3: Trained on Bootstrap Sample 3\n",
    "\n",
    "3. **Voting for Prediction**: Given a new, unseen sample, we pass it through each model and collect their predictions. Let's assume the models make the following predictions:\n",
    "\n",
    "   Model 1 prediction: A\n",
    "\n",
    "   Model 2 prediction: B\n",
    "\n",
    "   Model 3 prediction: B\n",
    "\n",
    "4. **Majority Voting**: We use majority voting to determine the final prediction. In this case, \"B\" receives two votes, while \"A\" receives one vote. Therefore, the final prediction for the ensemble is \"B\".\n",
    "\n",
    "In this numerical example, we utilized bagging by creating bootstrap samples, training individual models on those samples, and using majority voting to make the final prediction. The process showcases the main steps involved in bagging and demonstrates how it combines the predictions of multiple models to arrive at a more robust and accurate prediction.  \n",
    "\n",
    "---\n",
    "Certainly! Let's consider a regression problem and provide an example where variance is calculated and used to assess the performance of the bagging ensemble.\n",
    "\n",
    "Suppose we have a dataset of 50 samples with corresponding target values. We want to build a bagging ensemble of four regression models using bootstrap aggregation.\n",
    "\n",
    "Here is the dataset:\n",
    "\n",
    "| Sample | Target |\n",
    "|--------|--------|\n",
    "|   1    |   10   |\n",
    "|   2    |   12   |\n",
    "|   3    |   14   |\n",
    "|   4    |   16   |\n",
    "|   5    |   18   |\n",
    "|   6    |   20   |\n",
    "|   7    |   22   |\n",
    "|   8    |   24   |\n",
    "|   9    |   26   |\n",
    "|   10   |   28   |\n",
    "|   ...  |   ...  |\n",
    "|   50   |   96   |\n",
    "\n",
    "1. **Bootstrap Sampling**: We create four bootstrap samples by randomly selecting samples with replacement from the original dataset. Let's say the samples selected for each bootstrap sample are as follows:\n",
    "\n",
    "   Bootstrap Sample 1: [12, 18, 24, 16, 20, 10, 28, 22, 24, 26, ...]\n",
    "\n",
    "   Bootstrap Sample 2: [22, 12, 14, 26, 20, 18, 16, 10, 22, 14, ...]\n",
    "\n",
    "   Bootstrap Sample 3: [10, 16, 18, 28, 22, 10, 24, 26, 14, 18, ...]\n",
    "\n",
    "   Bootstrap Sample 4: [20, 24, 12, 18, 26, 14, 16, 22, 24, 10, ...]\n",
    "\n",
    "2. **Training Individual Models**: We train four regression models on these bootstrap samples:\n",
    "\n",
    "   Model 1: Trained on Bootstrap Sample 1\n",
    "\n",
    "   Model 2: Trained on Bootstrap Sample 2\n",
    "\n",
    "   Model 3: Trained on Bootstrap Sample 3\n",
    "\n",
    "   Model 4: Trained on Bootstrap Sample 4\n",
    "\n",
    "3. **Prediction and Variance Calculation**: We make predictions using each model on the original dataset and calculate the variance of their predictions. Let's assume the models make the following predictions:\n",
    "\n",
    "   Model 1 predictions: [11, 17, 23, 15, 19, 9, 27, 21, 23, 25, ...]\n",
    "\n",
    "   Model 2 predictions: [21, 11, 13, 25, 19, 17, 15, 9, 21, 13, ...]\n",
    "\n",
    "   Model 3 predictions: [9, 15, 17, 27, 21, 9, 23, 25, 13, 17, ...]\n",
    "\n",
    "   Model 4 predictions: [19, 23, 11, 17, 25, 13, 15, 21, 23, 9, ...]\n",
    "\n",
    "   We then calculate the variance of the predictions across the ensemble of models.\n",
    "\n",
    "   Variance = (sum of squared differences) / (number of models - 1)\n",
    "\n",
    "   Variance = (sum of [(prediction - mean prediction)^2]) / (number of models - 1)\n",
    "\n",
    "   Let's calculate the variance:\n",
    "\n",
    "   Variance = ((1^2 + 5^2 + 9^2 + 3^2 + 7^2 + (-3)^2 + 15^2 + 9^2 + 11^2 + 13^\n",
    "\n",
    "2 + ...) / (4 - 1)\n",
    "\n",
    "4. **Using Variance**: Once we have the variance, we can use it as a measure of the ensemble's performance. A lower variance indicates that the predictions of the individual models are more similar and therefore more reliable.\n",
    "\n",
    "In this example, we used bootstrap aggregation to create bootstrap samples, trained individual regression models, made predictions, and calculated the variance of the ensemble's predictions. The variance can be used to assess the stability and consistency of the ensemble and provides insights into the level of agreement among the individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a264763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    " from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a base classifier\n",
    "base_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Create a bagging classifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Fit the bagging classifier to the training data\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the bagging classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0bec1",
   "metadata": {},
   "source": [
    "### `AdaBoost`  \n",
    "Adaboost, short for Adaptive Boosting, is an ensemble learning algorithm that combines weak learners (classifiers with weak predictive power) to create a strong classifier. It is an iterative algorithm that assigns weights to each training sample, focusing on the misclassified samples in subsequent iterations. Here, I'll explain the derivation of Adaboost in LaTeX format:\n",
    "\n",
    "Step 1: Initialize the weights for each training sample as $w_i = \\frac{1}{N}$, where $N$ is the total number of samples.\n",
    "\n",
    "Step 2: For $t = 1$ to $T$ (the number of iterations):\n",
    "  \n",
    "  a) Train a weak learner $G_t(x)$ on the training data using the current sample weights.\n",
    "  \n",
    "  b) Calculate the weighted error of the weak learner as $\\epsilon_t = \\sum_{i=1}^{N} w_i^{(t)} \\cdot I(y_i \\neq G_t(x_i))$, where $w_i^{(t)}$ is the weight of sample $i$ at iteration $t$ and $I(\\cdot)$ is the indicator function.\n",
    "  \n",
    "  c) Calculate the weight of the weak learner as $\\alpha_t = \\frac{1}{2} \\ln \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$.\n",
    "  \n",
    "  d) Update the sample weights as $w_i^{(t+1)} = \\frac{w_i^{(t)} \\cdot \\exp(-\\alpha_t \\cdot y_i \\cdot G_t(x_i))}{Z_t}$, where $Z_t$ is the normalization factor (the sum of all updated weights).\n",
    "  \n",
    "Step 3: Repeat steps 2 until $T$ iterations are completed.\n",
    "\n",
    "Step 4: The final boosted model is given by $F(x) = \\text{sign} \\left(\\sum_{t=1}^{T} \\alpha_t \\cdot G_t(x)\\right)$.\n",
    "\n",
    "The derivation of Adaboost involves minimizing the exponential loss function by iteratively updating the weights of misclassified samples. The weight update equation ensures that the subsequent weak learners focus more on the misclassified samples from the previous iterations. The weight $\\alpha_t$ of each weak learner depends on its weighted error, emphasizing more accurate weak learners.\n",
    "\n",
    "The final prediction is obtained by aggregating the predictions of all weak learners, where each weak learner's contribution is weighted by its importance (determined by $\\alpha_t$). The sign function ensures that the predictions are binary.\n",
    "\n",
    "Adaboost effectively combines the weak learners to create a strong ensemble model that improves the overall prediction performance. It is widely used in classification tasks and has shown good generalization capabilities.\n",
    "\n",
    "Please note that this is a brief summary of the derivation process. The actual derivation involves more mathematical details and proofs, which can be found in the original paper by Freund and Schapire (1997) titled \"A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146ae5e",
   "metadata": {},
   "source": [
    "         +---------------------+\n",
    "         |                     |\n",
    "         |    Initialize       |\n",
    "         |   sample weights    |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Training          |\n",
    "         |   Iterations        |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Weak Learner      |\n",
    "         |   Training          |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Calculate         |\n",
    "         |   Weighted Error    |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Calculate         |\n",
    "         |   Weak Learner      |\n",
    "         |   Weight            |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Update            |\n",
    "         |   Sample Weights    |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Repeat            |\n",
    "         |   Iterations        |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Final             |\n",
    "         |   Prediction        |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Ensemble          |\n",
    "         |   Model             |\n",
    "         |                     |\n",
    "         +---------------------+\n",
    "         \n",
    "         \n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Here's how it works in simple terms:\n",
    "\n",
    "1. Imagine you have a group of people who are not very good at solving a problem individually. These people are like the weak learners in Adaboost.\n",
    "\n",
    "2. You give each person a different set of data and ask them to make predictions about the problem. Their predictions might not be very accurate on their own.\n",
    "\n",
    "3. Now, you analyze the predictions made by each person and pay more attention to the predictions that were wrong. You want to focus on the areas where they struggled.\n",
    "\n",
    "4. In the next round, you give more importance to the predictions that were incorrect. This means you try to make those predictions better by adjusting the parameters of the weak learners.\n",
    "\n",
    "5. You repeat this process for several rounds, each time adjusting the parameters of the weak learners based on their previous performance.\n",
    "\n",
    "6. Finally, you combine the predictions from all the weak learners, giving more weight to those who performed better overall.\n",
    "\n",
    "By doing this, Adaboost creates a strong model that learns from the mistakes of the weak learners and improves its accuracy over time. It's like having a group of people work together, where each person focuses on the areas they are not good at, and the group as a whole becomes better at solving the problem.\n",
    "         \n",
    "         \n",
    "\n",
    "1. Initialize Sample Weights:\n",
    "   - Each training sample is assigned an initial weight, typically set to 1/N, where N is the total number of samples.\n",
    "\n",
    "2. Training Iteration:\n",
    "   - Adaboost performs a series of training iterations.\n",
    "\n",
    "3. Weak Learner Training:\n",
    "   - In each iteration, a weak learner (e.g., decision stump) is trained on the training data.\n",
    "   - The weak learner aims to minimize the weighted error, taking into account the sample weights.\n",
    "\n",
    "4. Calculate Weighted Error:\n",
    "   - The weighted error of the weak learner is calculated as the sum of weights of misclassified samples.\n",
    "   - It measures the performance of the weak learner on the weighted training data.\n",
    "\n",
    "5. Calculate Weak Learner Weight:\n",
    "   - The weight of the weak learner is determined based on its weighted error.\n",
    "   - The weight emphasizes more accurate weak learners by assigning higher weights to those with lower errors.\n",
    "   - The weight is calculated using a formula: alpha = 0.5 * ln((1 - weighted_error) / weighted_error).\n",
    "\n",
    "6. Update Sample Weights:\n",
    "   - The sample weights are updated to emphasize the misclassified samples from the current weak learner.\n",
    "   - The weights of correctly classified samples are reduced, while the weights of misclassified samples are increased.\n",
    "   - The updated weights ensure that the misclassified samples have higher weights for the next iteration.\n",
    "\n",
    "7. Repeat Iterations:\n",
    "   - Steps 3 to 6 are repeated for a specified number of iterations or until a stopping criterion is met.\n",
    "\n",
    "8. Final Prediction:\n",
    "   - After all iterations are completed, the weak learners are combined to form the final boosted model.\n",
    "   - The prediction for a new sample is determined by aggregating the predictions of all weak learners, weighted by their respective alpha values.\n",
    "   - Typically, a weighted majority vote or weighted sum is used to make the final prediction.\n",
    "\n",
    "9. Ensemble Model:\n",
    "   - The ensemble model consists of the combination of weak learners, each with its respective weight.\n",
    "   - The ensemble model is capable of making more accurate predictions than individual weak learners.\n",
    "\n",
    "The diagram illustrates the step-by-step process of Adaboost, highlighting the training iterations, the calculation of weighted error and weak learner weight, and the updating of sample weights. Ultimately, Adaboost creates an ensemble model by combining weak learners to make accurate predictions on unseen data.\n",
    "\n",
    "-------------\n",
    "\n",
    "Given: Data x  \n",
    "**Step 1:**  \n",
    "- Initialize weights $w_{i}$ = for every i  \n",
    "- Start with the null classifier $f_{0}(x)$ = $g_{0}(x)$ = 0  \n",
    "  \n",
    "\n",
    "\n",
    "|Row# | F1|  F2   | F3   | Y  | Wgt|\n",
    "|-----|---|-------|------|----|----|\n",
    "|1    | 83|  0.30 | 73   | +  |0.1 |\n",
    "|2    | 91|  0.06 |  7   | +  |0.1 |\n",
    "|3    | 98|  0.41 | 42   |+   |0.1 |\n",
    "|4    | 95| 0.16  |29    |+   |0.1 |\n",
    "|5    | 89|   0.71|  99  |  + |0.1 |\n",
    "|6    | 73|   0.81| 37   |  - |0.1 |\n",
    "|7    | 58|   0.66|  82  | -  |0.1 |\n",
    "|8    | 32|  0.65 | 36   | -  |0.1 |\n",
    "|9    | 13|  0.11 |91    | -  |0.1 |\n",
    "|10   | 82| 0.28  |91    |-   |0.1 |\n",
    "\n",
    "\n",
    "**Step 2:**\n",
    "- For t = 1 to T:  \n",
    "    Generate training dataset by sampling with $w_i$  \n",
    "    - Undersampling those which are correctly predicted datapoints.(whose weights are lower)    \n",
    "    - oversampling those which are Incorrect datapoints.(whose weights are higher)      \n",
    "    - If the weights are higher, it will be oversampling, if it is lower, it will be undersampling. \n",
    "- Fit some weak learner $g_t$ ($g_t$ is algorithm of choice. In this case, it is decision tree.)  \n",
    " \n",
    " \n",
    "**Step 3:**\n",
    "- Lets assume $g_t$ makes some prediction     \n",
    "\n",
    "|Row# | F1|  F2   | F3   | Y  | Wgt|$g_t$ Prediction| Error|\n",
    "|-----|---|-------|------|----|----|----------------|------|\n",
    "|1    | 83|  0.30 | 73   | +  |0.1 |+               |0     |\n",
    "|2    | 91|  0.06 |  7   | +  |0.1 |+               |0     |\n",
    "|3    | 98|  0.41 | 42   |+   |0.1 |-               |1     |\n",
    "|4    | 95| 0.16  |29    |+   |0.1 |-               |1     | \n",
    "|5    | 89|   0.71|  99  |  + |0.1 |-               |1     |\n",
    "|6    | 73|   0.81| 37   |  - |0.1 |-               |0     |\n",
    "|7    | 58|   0.66|  82  | -  |0.1 |-               |0     |\n",
    "|8    | 32|  0.65 | 36   | -  |0.1 |-               |0     |\n",
    "|9    | 13|  0.11 |91    | -  |0.1 |-               |0     |\n",
    "|10   | 82| 0.28  |91    |-   |0.1 |-               |0     |  \n",
    "\n",
    "- In the above table, we can see, in error column 1 means incorrect prediction and 0 for correct prediction.  \n",
    "- AdaBoost loss function = \n",
    "$L(f) = \\frac{1}{N}\\sum{e^{-y_i.f(x_i)}}$    \n",
    "\n",
    "  \n",
    "**Step 4:**\n",
    "- set $\\lambda_t$ = $\\frac{1}{2}\\log\\frac{1 - e_t}{e_t}$, $\\lambda_t$ is weight of each model. $\\lambda_t$ is also called \"Amount of Say\"  \n",
    "- $e_t$  is the Total Error is equal to the sum of the weights of the incorrectly classified samples\n",
    "\n",
    "So in this example, error, ($e_T$) becomes 0.1+0.1+0.1 = 0.3  \n",
    "And $\\lambda_t$ becomes 0.42.  \n",
    "**Step 5:**  \n",
    "- update the weights.\n",
    "    - $w_i$ = $w_i * e^{\\lambda_t}$ if wrongly classified by $g_i$\n",
    "    - $w_i$ = $w_i * e^{- \\lambda_t}$ if correctly classified by $g_i$  \n",
    "    \n",
    "    \n",
    "|Row# | F1|  F2   | F3   | Y  | Wgt|New Weight| Error|\n",
    "|-----|---|-------|------|----|----|----------|------|\n",
    "|1    | 83|  0.30 | 73   | +  |0.1 |0.065     |0     |\n",
    "|2    | 91|  0.06 |  7   | +  |0.1 |0.065     |0     |\n",
    "|3    | 98|  0.41 | 42   |+   |0.1 |0.153     |1     |\n",
    "|4    | 95| 0.16  |29    |+   |0.1 |0.153     |1     | \n",
    "|5    | 89|   0.71|  99  |  + |0.1 |0.153     |1     |\n",
    "|6    | 73|   0.81| 37   |  - |0.1 |0.065     |0     |\n",
    "|7    | 58|   0.66|  82  | -  |0.1 |0.065     |0     |\n",
    "|8    | 32|  0.65 | 36   | -  |0.1 |0.065     |0     |\n",
    "|9    | 13|  0.11 |91    | -  |0.1 |0.065     |0     |\n",
    "|10   | 82| 0.28  |91    |-   |0.1 |0.065     |0     |  \n",
    "\n",
    "So weight for correct classifier = 0.065 and for incorrect classifier = 0.153  \n",
    "\n",
    "**step 6:**  \n",
    "- Normalize the $w_i$ to sum to one  \n",
    "- We need to multiply each New weight to sum of new weights  \n",
    "    - Normalized Weights = $\\frac{New\\ Weight_i}{Total\\ of\\ New\\ Weight}$\n",
    "    - Total of Normalized weights adds to 1.  \n",
    "    \n",
    "|Row# | F1|  F2   | F3   | Y  | Wgt|New Weight   | Normalized Sample weights|\n",
    "|-----|---|-------|------|----|----|-------------|--------------------------|\n",
    "|1    | 83|  0.30 | 73   | +  |0.1 |0.065        |0.071                     |\n",
    "|2    | 91|  0.06 |  7   | +  |0.1 |0.065        |0.071                     |\n",
    "|3    | 98|  0.41 | 42   |+   |0.1 |0.153        |0.167                     |\n",
    "|4    | 95| 0.16  |29    |+   |0.1 |0.153        |0.167                     | \n",
    "|5    | 89|   0.71|  99  |  + |0.1 |0.153        |0.167                     |\n",
    "|6    | 73|   0.81| 37   |  - |0.1 |0.065        |0.071                     |\n",
    "|7    | 58|   0.66|  82  | -  |0.1 |0.065        |0.071                     |\n",
    "|8    | 32|  0.65 | 36   | -  |0.1 |0.065        |0.071                     |\n",
    "|9    | 13|  0.11 |91    | -  |0.1 |0.065        |0.071                     |\n",
    "|10   | 82| 0.28  |91    |-   |0.1 |0.065        |0.071                     |  \n",
    "|     |   |       |      |    |    |Total = 0.917|  Total = 1               |  \n",
    "\n",
    "\n",
    "**Step 7:**  \n",
    "- The New Model is $f_t = f_{t-1} + \\lambda_t g_t$  \n",
    "- Output of the final model is  $f_T(x) = sgn(\\sum^{T}_{t=1}\\lambda_t g_t)$  .   \n",
    "\n",
    " \n",
    "--------\n",
    "Summary:  \n",
    "\n",
    "\n",
    "- Initialize weights $w_{i}$ = for every i  \n",
    "- Start with the null classifier $f_{0}(x)$ = $g_{0}(x)$ = 0  \n",
    "- For t = 1 to T:  \n",
    "    - Generate training dataset by sampling with $w_i$ \n",
    "    - set $\\lambda_t$ = $\\frac{1}{2}\\log\\frac{1 - e_t}{e_t}$ \n",
    "    - update the weights.\n",
    "        - $w_i$ = $w_i * e^{\\lambda_t}$ if wrongly classified by $g_i$\n",
    "        - $w_i$ = $w_i * e^{- \\lambda_t}$ if correctly classified by $g_i$\n",
    "    - Normalize the $w_i$ to sum to one  \n",
    "    - The New Model is $f_t = f_{t-1} + \\lambda_t g_t$  \n",
    "\n",
    "- Output of the final model is  $f_T(x) = sgn(\\sum^{T}_{t=1}\\lambda_t g_t)$  .   \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e2442",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/a-mathematical-explanation-of-adaboost-4b0c20ce4382   \n",
    "\n",
    "----\n",
    "AdaBoost (Adaptive Boosting) is a popular ensemble learning algorithm that combines multiple weak classifiers to create a strong classifier. It was introduced by Yoav Freund and Robert Schapire in 1996. Here are some important properties and details about AdaBoost:\n",
    "\n",
    "1. Ensemble Learning: AdaBoost belongs to the family of ensemble learning methods, which combine the predictions of multiple individual models (weak learners) to make a final prediction. In AdaBoost, weak learners are usually simple models, such as decision stumps (a decision tree with a single split).\n",
    "\n",
    "2. Adaptive Weighting: AdaBoost assigns weights to each training example, with higher weights given to misclassified examples in the previous iterations. This adaptive weighting strategy enables AdaBoost to focus on the difficult examples and improve its performance over iterations.\n",
    "\n",
    "3. Iterative Training: AdaBoost trains weak learners iteratively. In each iteration, it adjusts the weights of the training examples based on the previous iteration's performance and then trains a new weak learner on the updated weights.\n",
    "\n",
    "4. Weighted Voting: During the prediction phase, AdaBoost combines the predictions of weak learners using a weighted voting scheme. The weights of the weak learners depend on their performance in training, with more accurate models having higher weights.\n",
    "\n",
    "5. Model Combination: AdaBoost combines weak learners using a linear combination, where each weak learner contributes to the final prediction based on its weight. The final model is a weighted sum of the weak learners' predictions.\n",
    "\n",
    "6. Error Minimization: AdaBoost aims to minimize the weighted error rate of the ensemble model during training. It assigns higher weights to misclassified examples, forcing subsequent weak learners to focus on those examples.\n",
    "\n",
    "7. Boosting Algorithm: AdaBoost is a boosting algorithm, which means it focuses on improving the performance of the ensemble model by iteratively correcting the mistakes made by previous weak learners. Each subsequent weak learner tries to compensate for the errors of the previous learners.\n",
    "\n",
    "8. Model Independence: AdaBoost assumes that the weak learners are relatively independent of each other. It seeks to combine weak learners that make different types of errors, thereby creating a diverse ensemble that leads to better overall performance.\n",
    "\n",
    "9. Overfitting Avoidance: AdaBoost includes measures to prevent overfitting, such as early stopping criteria or limiting the number of iterations. This helps ensure that the ensemble model generalizes well to unseen data.\n",
    "\n",
    "10. Widely Used: AdaBoost has been successfully applied to various machine learning tasks, including classification, regression, and object detection. It has shown effectiveness in both academic research and practical applications.\n",
    "\n",
    "AdaBoost is a powerful algorithm with several attractive properties, but it is worth noting that it may be sensitive to noisy data or outliers. Additionally, it can be computationally expensive, especially if the weak learners are complex models. Nonetheless, with proper parameter tuning and careful handling of data, AdaBoost can produce robust and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1722df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an AdaBoost classifier\n",
    "adaboost_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef02f068",
   "metadata": {},
   "source": [
    "# `Gradient Boosting`  \n",
    "Gradient Boosting is a powerful machine learning algorithm that combines weak learners to create a strong learner. It sequentially builds an ensemble of models by minimizing a loss function using gradient descent. Here's a step-by-step explanation of Gradient Boosting with derivation:  \n",
    "\n",
    "1. Initialization:\n",
    "We start by initializing the model with a constant value: $F_0(x) = \\text{constant}$.\n",
    "\n",
    "2. Compute Initial Predictions:\n",
    "We compute the initial predictions of the model: $F_0(x)$.\n",
    "\n",
    "3. Calculate Residuals:\n",
    "The residuals are calculated by subtracting the predicted values from the true values: $r = y - F_0(x)$.\n",
    "\n",
    "4. Weak Learner Training:\n",
    "A weak learner, such as a decision tree, is trained to predict the residuals, minimizing the loss function: $\\hat{r} = \\text{argmin}_h \\sum_i L(y_i, F_0(x_i) + h(x_i))$.\n",
    "\n",
    "5. Weak Learner Prediction:\n",
    "The weak learner's prediction, denoted as $h(x)$, approximates the residuals: $\\hat{r} \\approx h(x)$.\n",
    "\n",
    "6. Update Model:\n",
    "The model is updated by adding a fraction (learning rate, typically less than 1) of the weak learner's prediction to the previous model's predictions: $F_1(x) = F_0(x) + \\text{learning_rate} \\cdot h(x)$.\n",
    "\n",
    "7. Updated Residuals:\n",
    "The updated residuals are calculated by subtracting the updated predictions from the true values: $r = y - F_1(x)$.\n",
    "\n",
    "8. Repeat Iterations:\n",
    "Steps 4 to 7 are repeated for a specified number of iterations, each time training a new weak learner to predict the updated residuals and updating the model.\n",
    "\n",
    "9. Final Model:\n",
    "The final model is an ensemble of weak learners: $F(x) = F_0(x) + \\text{learning_rate} \\cdot h_1(x) + \\text{learning_rate} \\cdot h_2(x) + \\ldots + \\text{learning_rate} \\cdot h_n(x)$.\n",
    "\n",
    "10. Predictions:\n",
    "To make predictions on new data, we use the ensemble model and sum the predictions of all weak learners: $F(x)$.\n",
    "\n",
    "11. Learning Rate:\n",
    "The learning rate controls the contribution of each weak learner to the final prediction. A lower learning rate reduces the impact of each weak learner.\n",
    "\n",
    "12. Gradient Descent:\n",
    "The algorithm utilizes gradient descent to minimize the loss function by updating the model's predictions based on the gradients of the loss function with respect to the predictions.\n",
    "\n",
    "13. Fitting Negative Gradients:\n",
    "The weak learners are trained to fit the negative gradients, focusing on the parts of the data that the previous models struggle to capture.\n",
    "\n",
    "14. Loss Function:\n",
    "The loss function used in Gradient Boosting can vary depending on the problem, such as mean squared error (MSE) for regression or log loss for classification.\n",
    "\n",
    "15. Flexibility:\n",
    "Gradient Boosting is a flexible algorithm that can handle various types of data and can be customized by adjusting parameters like the learning rate, number of iterations, and the depth of weak learners.\n",
    "\n",
    "16. Complex Relationships:\n",
    "The algorithm can capture complex relationships and non-linear patterns in the data.\n",
    "\n",
    "17. Overfitting:\n",
    "However, Gradient Boosting can be prone to overfitting if the number of iterations is too high or the weak learners are too complex.\n",
    "\n",
    "18. Regularization Techniques:\n",
    "Regularization techniques, such as tree depth limitations and shrinkage (reducing the learning rate), can be applied to mitigate overfitting and improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc88565",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Given a dataset with input features $X$ and target variable $y$, Gradient Boosting sequentially builds an ensemble of models by minimizing a loss function using gradient descent. Let's denote the initial model as $F_0(x)$.\n",
    "\n",
    "1. Initialize the model with a constant value:\n",
    "$$F_0(x) = \\text{constant}$$\n",
    "\n",
    "2. Compute the initial predictions of the model:\n",
    "$$F_0(x) = \\frac{1}{N}\\sum_{i=1}^{N}y_i$$\n",
    "\n",
    "3. Calculate the residuals (negative gradients):\n",
    "$r = y - F_0(x)$\n",
    "\n",
    "4. Train a weak learner, such as a decision tree, to predict the residuals $r$. The weak learner minimizes the loss function $L(y, F(x))$.\n",
    "\n",
    "5. Obtain the weak learner's prediction, denoted as $h(x)$, which approximates the residuals $r$.\n",
    "\n",
    "6. Update the model by adding a fraction (learning rate, usually $< 1$) of the weak learner's prediction to the previous model's predictions:\n",
    "$$F_1(x) = F_0(x) + \\text{learning_rate} \\times h(x)$$\n",
    "\n",
    "7. Update the residuals by subtracting the updated predictions from the true values:\n",
    "$r = y - F_1(x)$\n",
    "\n",
    "8. Repeat steps 4 to 7 for a specified number of iterations, each time training a new weak learner to predict the updated residuals and updating the model:\n",
    "$$F_t(x) = F_{t-1}(x) + \\text{learning_rate} \\times h_t(x)$$\n",
    "$$r = y - F_t(x)$$\n",
    "\n",
    "9. The final model is an ensemble of weak learners:\n",
    "$$F(x) = F_0(x) + \\text{learning_rate} \\times h_1(x) + \\text{learning_rate} \\times h_2(x) + \\ldots + \\text{learning_rate} \\times h_t(x)$$\n",
    "\n",
    "10. To make predictions on new data, use the ensemble model $F(x)$.\n",
    "\n",
    "11. The learning rate controls the contribution of each weak learner to the final prediction.\n",
    "\n",
    "12. The algorithm employs gradient descent to iteratively minimize the loss function by updating the model's predictions based on the gradients of the loss function with respect to the predictions.\n",
    "\n",
    "13. The weak learners are trained to fit the negative gradients, focusing on the parts of the data that the previous models struggle to capture.\n",
    "\n",
    "14. By sequentially adding weak learners and adjusting the predictions, Gradient Boosting improves the model's performance with each iteration.\n",
    "\n",
    "15. The loss function used in Gradient Boosting can vary depending on the problem, such as mean squared error (MSE) for regression or log loss for classification.\n",
    "\n",
    "16. Gradient Boosting is a flexible algorithm that can handle various types of data and can be customized by adjusting parameters like the learning rate, number of iterations, and the depth of weak learners.\n",
    "\n",
    "17. The algorithm is known for its ability to handle complex relationships and capture non-linear patterns in the data.\n",
    "\n",
    "18. However, Gradient Boosting can be prone to overfitting if the number of iterations is too high or the weak learners are too complex.\n",
    "\n",
    "19. Regularization techniques, such as tree depth limitations and shrinkage (reducing the learning rate), can be applied to mitigate overfitting and improve generalization performance.\n",
    "\n",
    "20. The ensemble model produced by Gradient Boosting combines the predictions of multiple weak learners, resulting in a more accurate and robust model for prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21332b",
   "metadata": {},
   "source": [
    "Let's go through the detailed numerical example of gradient boosting for the regression problem. We'll use the same dataset as before:\n",
    "\n",
    "| X1 | X2 | y  |\n",
    "|----|----|----|\n",
    "| 1  | 2  | 5  |\n",
    "| 2  | 3  | 9  |\n",
    "| 3  | 4  | 14 |\n",
    "| 4  | 5  | 18 |\n",
    "| 5  | 6  | 22 |\n",
    "\n",
    "Step 1: Initialize the model  \n",
    "  \n",
    "We initialize our model by setting all predictions to the mean of the target variable, which is 12.8.  \n",
    "Step 2: Calculate residuals  \n",
    "  \n",
    "Calculate the residuals by subtracting the initial predictions from the actual target values. The residuals for our training data points are:  \n",
    "Residual 1: Actual value - Initial prediction = 5 - 12.8 = -7.8   \n",
    "Residual 2: Actual value - Initial prediction = 9 - 12.8 = -3.8  \n",
    "Residual 3: Actual value - Initial prediction = 14 - 12.8 = 1.2  \n",
    "Residual 4: Actual value - Initial prediction = 18 - 12.8 = 5.2  \n",
    "Residual 5: Actual value - Initial prediction = 22 - 12.8 = 9.2  \n",
    "Step 3: Build a weak learner (decision tree)  \n",
    "    \n",
    "We train a weak learner, a decision tree, to predict the residuals.  \n",
    "Let's assume the decision tree predicts the following residuals:  \n",
    "Residual 1: -1.2  \n",
    "Residual 2: 0.8  \n",
    "Residual 3: 2.5  \n",
    "Residual 4: 0.3  \n",
    "Residual 5: -2.1  \n",
    "  \n",
    "Step 4: Update the model  \n",
    "We add the predictions of the decision tree to the previous model's predictions.  \n",
    "The updated predictions become:  \n",
    "Updated prediction 1 = Initial prediction 1 + Decision tree prediction 1 = 12.8 - 1.2 = 11.6  \n",
    "Updated prediction 2 = Initial prediction 2 + Decision tree prediction 2 = 12.8 + 0.8 = 13.6  \n",
    "Updated prediction 3 = Initial prediction 3 + Decision tree prediction 3 = 12.8 + 2.5 = 15.3  \n",
    "Updated prediction 4 = Initial prediction 4 + Decision tree prediction 4 = 12.8 + 0.3 = 13.1  \n",
    "Updated prediction 5 = Initial prediction 5 + Decision tree prediction 5 = 12.8 - 2.1 = 10.7   \n",
    "  \n",
    "Step 5: Repeat steps 2-4 (Iterations)  \n",
    "We repeat steps 2 to 4 to further improve the model:    \n",
    "  \n",
    "Calculate new residuals based on the updated predictions.  \n",
    "Build a new decision tree to predict the new residuals.  \n",
    "Update the model's predictions by adding the new decision tree's predictions.  \n",
    "Let's assume we perform a second iteration:   \n",
    "  \n",
    "New residuals:  \n",
    "Residual 1: Actual value - Updated prediction 1 = 5 - 11.6 = -6.6  \n",
    "Residual 2: Actual value - Updated prediction 2 = 9 - 13.6 = -4.6  \n",
    "Residual 3: Actual value - Updated prediction 3 = 14 - 15.3 = -1.3  \n",
    "Residual 4: Actual value - Updated prediction 4 = 18 - 13.1 = 4.9  \n",
    "Residual 5: Actual value - Updated prediction 5 = 22 - 10.7 = 11.3  \n",
    "  \n",
    "New decision tree predictions:  \n",
    "Residual 1: -0.8  \n",
    "Residual 2: 1.2  \n",
    "Residual 3: 1.8  \n",
    "Residual 4: 0.5  \n",
    "Residual 5: -3.9  \n",
    "  \n",
    "Updated predictions:  \n",
    "Updated prediction 1 = Updated prediction 1 + Decision tree prediction 1 = 11.6 - 0.8 = 10.8  \n",
    "Updated prediction 2 = Updated prediction 2 + Decision tree prediction 2 = 13.6 + 1.2 = 14.8  \n",
    "Updated prediction 3 = Updated prediction 3 + Decision tree prediction 3 = 15.3 + 1.8 = 17.1  \n",
    "Updated prediction 4 = Updated prediction 4 + Decision tree prediction 4 = 13.1 + 0.5 = 13.6  \n",
    "Updated prediction 5 = Updated prediction 5 + Decision tree prediction 5 = 10.7 - 3.9 = 6.8  \n",
    "We can continue iterating and repeating these steps until a stopping criterion is met or the desired level of performance is achieved. The final model is an ensemble of weak learners (decision trees) combined to make predictions.  \n",
    "  \n",
    "Please note that the above example demonstrates a simplified version of gradient boosting for regression. In practice, various optimizations, regularization techniques, and additional algorithmic variations are employed to improve the performance and stability of the gradient boosting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb0518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Gradient Boosting classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a98514",
   "metadata": {},
   "source": [
    "### `XGBoost`  \n",
    "\n",
    "\n",
    "$$\\textbf{XGBoost with Mathematical Derivation and Steps}$$\n",
    "\n",
    "1. Initialize the ensemble model with a constant value:\n",
    "$$F_0(x) = \\text{constant}$$\n",
    "\n",
    "2. Calculate the initial prediction of the model:\n",
    "$$F_0(x) = \\frac{1}{N}\\sum_{i=1}^{N}y_i $$\n",
    "\n",
    "3. For each iteration t:\n",
    "\n",
    "   a. Compute the negative gradients and Hessians:\n",
    "   $$g_i = -\\left[\\frac{\\partial L(y_i, F_{t-1}(x_i))}{\\partial F_{t-1}(x_i)}\\right]_{F_{t-1}(x_i)=y_i} $$\n",
    "   $$h_i = \\left[\\frac{\\partial^2 L(y_i, F_{t-1}(x_i))}{\\partial F_{t-1}^2(x_i)}\\right]_{F_{t-1}(x_i)=y_i}$$\n",
    "\n",
    "   b. Fit a weak learner (e.g., decision tree) to the negative gradients using the objective function:\n",
    "   $$ \\text{Obj}_t = \\sum_{i=1}^{N} [g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)] + \\Omega(f_t) $$\n",
    "\n",
    "   c. Update the ensemble model by adding a fraction (learning rate) of the weak learner's predictions:\n",
    "   $$ F_t(x) = F_{t-1}(x) + \\text{learning_rate} \\times f_t(x) $$\n",
    "\n",
    "4. The final model is the ensemble of weak learners:\n",
    "$$F(x) = F_0(x) + \\text{learning_rate} \\times f_1(x) + \\text{learning_rate} \\times f_2(x) + \\ldots + \\text{learning_rate} \\times f_T(x) $$\n",
    "  \n",
    "  \n",
    "  \n",
    "5. To make predictions on new data, use the final ensemble model $F(x)$.\n",
    "\n",
    "6. XGBoost incorporates regularization techniques like L1 and L2 regularization to control the complexity of weak learners.\n",
    "\n",
    "7. The objective function is optimized through gradient-based optimization, which iteratively minimizes the loss function.\n",
    "\n",
    "8. The learning rate controls the contribution of each weak learner to the final prediction.\n",
    "\n",
    "9. XGBoost applies the second-order approximation of the objective function using the Taylor expansion.\n",
    "\n",
    "10. The algorithm focuses on correcting the mistakes made by previous weak learners by fitting subsequent weak learners to the negative gradients.\n",
    "\n",
    "11. The ensemble model is trained iteratively, with each weak learner addressing the residuals of the previous models.\n",
    "\n",
    "12. XGBoost is capable of handling different types of loss functions and is widely used in both regression and classification problems.\n",
    "\n",
    "13. It offers advantages such as parallel processing, regularization techniques, and handling missing values.\n",
    "\n",
    "14. XGBoost has gained popularity for its ability to handle large-scale datasets and produce accurate predictions.\n",
    "\n",
    "15. However, XGBoost can be computationally expensive due to the iterative nature of the algorithm.\n",
    "\n",
    "16. Regularization techniques help prevent overfitting by controlling the complexity of the model.\n",
    "\n",
    "17. Feature importance analysis can be performed using XGBoost to understand the predictive power of different features.\n",
    "\n",
    "---------\n",
    "\n",
    "XGBoost, short for Extreme Gradient Boosting, is a powerful machine learning algorithm that belongs to the family of gradient boosting methods. It is designed to optimize and improve the performance of supervised learning tasks, particularly in regression and classification problems.\n",
    "\n",
    "XGBoost works by building an ensemble of weak prediction models, typically decision trees, and combining their predictions to create a more accurate and robust final model. It uses a gradient boosting framework to iteratively train these weak learners and address the mistakes made by the previous models.\n",
    "\n",
    "The algorithm follows these key steps:\n",
    "\n",
    "1. Initialization: The ensemble model is initialized with a constant value or an initial prediction. This serves as the starting point for subsequent iterations.\n",
    "\n",
    "2. Gradient Calculation: The negative gradients (residuals) and Hessians (second-order derivatives) are computed based on the loss function used for the specific problem. These gradients indicate the direction and magnitude of the errors made by the current ensemble.\n",
    "\n",
    "3. Weak Learner Training: A weak learner, typically a decision tree, is trained to fit the negative gradients by minimizing a specific objective function. The objective function includes terms that penalize both the errors and the complexity of the weak learner. Regularization techniques like L1 and L2 regularization are often applied to control overfitting.\n",
    "\n",
    "4. Ensemble Update: The weak learner's predictions are added to the ensemble model with a fraction known as the learning rate. This fraction determines the contribution of each weak learner to the final prediction. The ensemble model is updated iteratively with each new weak learner.\n",
    "\n",
    "5. Final Prediction: Once the desired number of weak learners is trained, the final prediction is computed by summing the predictions of all weak learners in the ensemble, weighted by the learning rate. This ensemble model captures the complex relationships in the data and produces a more accurate prediction than any individual weak learner.\n",
    "\n",
    "XGBoost offers several advantages, including:\n",
    "\n",
    "- Handling of missing values: XGBoost has built-in capabilities to handle missing data, reducing the need for data preprocessing.\n",
    "\n",
    "- Feature importance analysis: It provides insights into the importance of different features in the prediction, aiding feature selection and interpretation.\n",
    "\n",
    "- Regularization: XGBoost incorporates regularization techniques to prevent overfitting and improve generalization.\n",
    "\n",
    "- Parallel processing: The algorithm supports parallel processing, making it efficient for large-scale datasets.\n",
    "\n",
    "- Hyperparameter tuning: XGBoost provides options to tune hyperparameters like learning rate, maximum tree depth, and number of iterations, allowing users to optimize model performance.\n",
    "\n",
    "Overall, XGBoost has become a popular choice in various machine learning competitions and real-world applications due to its high predictive accuracy, scalability, and flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3f0cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.5-py3-none-win_amd64.whl (70.9 MB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fdbdbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the XGBoost classifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8218fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'learning_rate': 0.001, 'max_depth': 4, 'n_estimators': 50}\n",
      "Best Accuracy:  0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris_data = load_iris()\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Set the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and accuracy score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Accuracy: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fca255",
   "metadata": {},
   "source": [
    "### `Sparse Group Lasso Regularization`\n",
    "Sparse Group Lasso regularization is a regularization technique that combines the benefits of group lasso and lasso regularization to encourage both sparsity and group-level sparsity in a model. It is particularly useful when dealing with datasets where the features naturally group together.\n",
    "\n",
    "In sparse group lasso regularization, the feature space is divided into groups, and the regularization penalty is applied at both the individual feature level (lasso) and the group level (group lasso). This encourages the model to select relevant groups of features and, within those groups, select only a subset of important individual features.\n",
    "\n",
    "The regularization term for sparse group lasso is a combination of the lasso penalty and the group lasso penalty. The objective function can be written as:\n",
    "\n",
    "$$\n",
    "\\min_{m} \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - m^T x_i)^2 + \\lambda_1 \\sum_{j=1}^{p} \\left\\| m_j \\right\\|_1 + \\lambda_2 \\sum_{g=1}^{G} \\sqrt{p_g} \\left\\| m_g \\right\\|_2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $m$ represents the vector of regression coefficients.\n",
    "- $y$ represents the target variable.\n",
    "- $x_i$ represents the feature vector of the $i$-th sample.\n",
    "- $n$ represents the number of samples.\n",
    "- $p$ represents the total number of features.\n",
    "- $m_j$ represents the $j$-th individual feature coefficient.\n",
    "- $m_g$ represents the $g$-th group of feature coefficients.\n",
    "- $p_g$ represents the number of features within the $g$-th group.\n",
    "- $\\lambda_1$ and $\\lambda_2$ are the regularization parameters that control the strength of the penalties.\n",
    "\n",
    "The first term in the objective function is the least squares loss term, aiming to minimize the difference between the predicted values and the actual target values. The second term is the lasso penalty, promoting sparsity at the individual feature level. The third term is the group lasso penalty, encouraging sparsity at the group level. The $\\sqrt{p_g}$ term in the group lasso penalty helps to account for the varying group sizes.\n",
    "\n",
    "By jointly optimizing these terms, the sparse group lasso regularization can effectively select relevant groups of features while encouraging sparsity within those groups, leading to a more interpretable and compact model.  \n",
    "\n",
    "---\n",
    "Step 1: Start with the objective function for linear regression:\n",
    "$$\n",
    "\\min_{m} \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - m^T x_i)^2\n",
    "$$\n",
    "where $m$ represents the vector of regression coefficients, $y$ represents the target variable, $x_i$ represents the feature vector of the $i$-th sample, and $n$ is the number of samples.\n",
    "\n",
    "Step 2: Introduce the L1 regularization (lasso penalty) at the individual feature level:\n",
    "$$\n",
    "\\min_{m} \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - m^T x_i)^2 + \\lambda_1 \\sum_{j=1}^{p} \\left\\| m_j \\right\\|_1\n",
    "$$\n",
    "where $p$ represents the total number of features, $m_j$ represents the $j$-th individual feature coefficient, and $\\lambda_1$ is the regularization parameter controlling the strength of the lasso penalty.\n",
    "\n",
    "Step 3: Introduce the group lasso penalty at the group level:\n",
    "$$\n",
    "\\min_{m} \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - m^T x_i)^2 + \\lambda_1 \\sum_{j=1}^{p} \\left\\| m_j \\right\\|_1 + \\lambda_2 \\sum_{g=1}^{G} \\sqrt{p_g} \\left\\| m_g \\right\\|_2\n",
    "$$\n",
    "where $m_g$ represents the $g$-th group of feature coefficients, $p_g$ represents the number of features within the $g$-th group, $G$ represents the total number of groups, and $\\lambda_2$ is the regularization parameter controlling the strength of the group lasso penalty.\n",
    "\n",
    "Step 4: Simplify the notation and combine the terms:\n",
    "$$\n",
    "\\min_{m} \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - m^T x_i)^2 + \\lambda_1 \\sum_{j=1}^{p} \\left\\| m_j \\right\\|_1 + \\lambda_2 \\sum_{g=1}^{G} \\sqrt{p_g} \\left\\| m_g \\right\\|_2\n",
    "$$\n",
    "\n",
    "This final equation represents the objective function for Sparse Group Lasso regularization in linear regression. Each term contributes to the overall objective by promoting small and sparse individual feature coefficients through lasso regularization, as well as encouraging sparse group-level coefficients through the group lasso penalty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07ddeb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sgl\n",
      "  Downloading sgl-0.9.17.tar.gz (8.3 kB)\n",
      "Building wheels for collected packages: sgl\n",
      "  Building wheel for sgl (setup.py): started\n",
      "  Building wheel for sgl (setup.py): finished with status 'done'\n",
      "  Created wheel for sgl: filename=sgl-0.9.17-py3-none-any.whl size=9278 sha256=c5663c12af71bd1d9a01589d22bd95daf0fc300d29ae7504a2a170bb3f4b57a9\n",
      "  Stored in directory: c:\\users\\hashim razi\\appdata\\local\\pip\\cache\\wheels\\35\\61\\a1\\c3531fa3cb33e740f6803024a4713cd9bf907545844c1934a7Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script realpython.exe is installed in 'C:\\Users\\HASHIM RAZI\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully built sgl\n",
      "Installing collected packages: sgl\n",
      "Successfully installed sgl-0.9.17\n"
     ]
    }
   ],
   "source": [
    "pip install sgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0f215bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SparseGroupLasso' from 'sgl' (C:\\Users\\HASHIM RAZI\\AppData\\Roaming\\Python\\Python39\\site-packages\\sgl\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msgl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseGroupLasso\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the Iris dataset\u001b[39;00m\n\u001b[0;32m      6\u001b[0m iris \u001b[38;5;241m=\u001b[39m load_iris()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SparseGroupLasso' from 'sgl' (C:\\Users\\HASHIM RAZI\\AppData\\Roaming\\Python\\Python39\\site-packages\\sgl\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sgl import SparseGroupLasso\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a SparseGroupLasso object\n",
    "sgl = SparseGroupLasso()\n",
    "\n",
    "# Define the group structure\n",
    "groups = np.array([0, 0, 1, 1])  # Assuming first two features belong to one group, last two features belong to another group\n",
    "\n",
    "# Set the regularization parameters\n",
    "alpha = 0.1  # L1 regularization parameter\n",
    "beta = 0.1   # Group Lasso regularization parameter\n",
    "\n",
    "# Fit the Sparse Group Lasso model\n",
    "sgl.fit(X, y, groups, alpha, beta)\n",
    "\n",
    "# Get the coefficient estimates\n",
    "coef = sgl.coef_\n",
    "\n",
    "# Print the coefficient estimates\n",
    "print(\"Coefficient estimates:\", coef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ddd9e5",
   "metadata": {},
   "source": [
    "### `Bayesian Regularization`  \n",
    "Certainly! Here's an explanation of Bayesian regularization with the derivation in LaTeX form, using variables \"m\" for regression coefficients, \"x\" for input features, and \"c\" for the intercept term:\n",
    "\n",
    "The linear regression equation can be written as:\n",
    "\n",
    "$$\n",
    "y = x \\cdot m + c + \\epsilon\n",
    "$$\n",
    "\n",
    "where $y$ is the target variable, $x$ is the input feature vector, $m$ is the vector of regression coefficients, $c$ is the intercept term, and $\\epsilon$ is the error term.\n",
    "\n",
    "To incorporate Bayesian regularization, we assume a prior distribution over the regression coefficients, denoted as:\n",
    "\n",
    "$$\n",
    "m \\sim \\mathcal{N}(0, \\alpha^{-1}I)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{N}$ represents a normal (Gaussian) distribution, \"0\" is the mean, $\\alpha^{-1}$ is the precision (inverse variance), and \"I\" is the identity matrix.\n",
    "\n",
    "The goal is to estimate the posterior distribution of the regression coefficients given the data, incorporating both the prior knowledge and the observed data. By applying Bayesian inference, we can derive the posterior distribution as:\n",
    "\n",
    "$$\n",
    "p(m | x, y) \\propto p(y | x, m) \\cdot p(m)\n",
    "$$\n",
    "\n",
    "where $p(y | x, m)$ is the likelihood function, representing the probability of the observed data given the regression coefficients.\n",
    "\n",
    "Assuming that the errors $\\epsilon$ are normally distributed with zero mean and constant variance $\\sigma^2$, the likelihood function can be written as:\n",
    "\n",
    "$$\n",
    "p(y | x, m) = \\mathcal{N}(y | x \\cdot m + c, \\sigma^2I)\n",
    "$$\n",
    "\n",
    "Substituting the likelihood and the prior distribution into the posterior equation, we get:\n",
    "\n",
    "$$\n",
    "p(m | x, y) \\propto \\mathcal{N}(y | x \\cdot m + c, \\sigma^2I) \\cdot \\mathcal{N}(m | 0, \\alpha^{-1}I)\n",
    "$$\n",
    "\n",
    "To simplify the expression, we can take the logarithm of the posterior distribution:\n",
    "\n",
    "$$\n",
    "\\log p(m | x, y) \\propto \\log \\left( \\mathcal{N}(y | x \\cdot m + c, \\sigma^2I) \\cdot \\mathcal{N}(m | 0, \\alpha^{-1}I) \\right)\n",
    "$$\n",
    "\n",
    "Expanding the logarithm and dropping constant terms, we have:\n",
    "\n",
    "$$\n",
    "\\log p(m | x, y) \\propto -\\frac{1}{2\\sigma^2} \\left\\| y - x \\cdot m - c \\right\\|^2 - \\frac{1}{2\\alpha} \\left\\| m \\right\\|^2\n",
    "$$\n",
    "\n",
    "The goal of Bayesian regularization is to find the maximum a posteriori (MAP) estimate of the regression coefficients. This can be achieved by maximizing the logarithm of the posterior distribution, which is equivalent to minimizing the negative log-posterior:\n",
    "\n",
    "$$\n",
    "\\text{argmin}_m \\left\\{ \\frac{1}{2\\sigma^2} \\left\\| y - x \\cdot m - c \\right\\|^2 + \\frac{1}{2\\alpha} \\left\\| m \\right\\|^2 \\right\\}\n",
    "$$\n",
    "\n",
    "This minimization problem can be solved using optimization techniques such as gradient descent or closed-form solutions.\n",
    "\n",
    "Please note that this derivation assumes a specific form of the likelihood function and prior distribution. Different assumptions may lead to variations in the derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1976a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.038267632150381474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the Bayesian Ridge regression model\n",
    "reg = BayesianRidge()\n",
    "\n",
    "# Fit the model on the training data\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b08fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5fcf10f",
   "metadata": {},
   "source": [
    "### **Multiple Linear Regression**\n",
    "Multiple Linear Regression, also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. Multiple regression is an extension of simple linear regression that uses just one explanatory variable. We will learn about various ways to optimise such regressions in the upcoming modules. \n",
    "\n",
    "For now, try implementing a regression model using scikit learn's function for linear regression. You may use the diabetes dataset that can be retreived from sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6249d90",
   "metadata": {},
   "source": [
    "### Questions for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea19d6",
   "metadata": {},
   "source": [
    "##### `What is linear regression, and what are some applications of it?`    \n",
    "  \n",
    "Linear regression is a statistical method for modeling the relationship between a dependent variable (also known as the response variable) and one or more independent variables (also known as the predictor or explanatory variables). The goal of linear regression is to find a linear relationship between the independent variables and the dependent variable, so that we can make predictions of the dependent variable given values of the independent variables.\n",
    "\n",
    "Linear regression is a widely used method in statistical analysis and has a variety of applications in different fields, such as:\n",
    "\n",
    "1. Economics: Linear regression is used in economics to model the relationship between economic variables, such as supply and demand, price and quantity, and income and expenditure.\n",
    "\n",
    "2. Finance: Linear regression is used in finance to model stock prices, bond yields, and other financial data.\n",
    "\n",
    "3. Marketing: Linear regression is used in marketing to model the relationship between advertising spending and sales, customer behavior, and other market-related variables.\n",
    "\n",
    "4. Healthcare: Linear regression is used in healthcare to model the relationship between patient outcomes and various clinical and demographic variables.\n",
    "\n",
    "5. Social sciences: Linear regression is used in social sciences to model the relationship between various social variables, such as education, income, and health.\n",
    "\n",
    "6. Engineering: Linear regression is used in engineering to model the relationship between variables such as temperature, pressure, and other physical quantities.\n",
    "\n",
    "Linear regression can be used for both simple and multiple regression analysis. Simple linear regression involves modeling the relationship between one independent variable and one dependent variable, while multiple linear regression involves modeling the relationship between multiple independent variables and one dependent variable.\n",
    "\n",
    "Overall, linear regression is a powerful and versatile method that can be used in a variety of fields for modeling and predicting relationships between variables.  \n",
    "\n",
    "---\n",
    "\n",
    "#### `How do you interpret the slope and intercept coefficients in linear regression?`  \n",
    "In linear regression, the slope and intercept coefficients are two important parameters that describe the linear relationship between the independent variable(s) and the dependent variable. Here's how you can interpret these coefficients:\n",
    "\n",
    "1. Intercept coefficient: The intercept coefficient (usually denoted as \"b0\" or \"beta-zero\") represents the value of the dependent variable when all the independent variables are zero. In other words, it represents the starting point of the linear regression line. For example, in a simple linear regression model where we are predicting the weight of a person based on their height, the intercept coefficient would represent the weight of a person when their height is zero (which is not practically possible, but is just a mathematical construct). \n",
    "\n",
    "2. Slope coefficient: The slope coefficient (usually denoted as \"b1\" or \"beta-one\") represents the change in the dependent variable for a unit increase in the independent variable. In other words, it represents the steepness of the regression line. For example, in the same simple linear regression model mentioned earlier, the slope coefficient would represent the change in weight for every unit increase in height (e.g., for every inch increase in height, the weight increases by \"b1\" pounds).\n",
    "\n",
    "The intercept and slope coefficients together define the equation of the regression line, which can be used to make predictions about the dependent variable given the values of the independent variable(s). It is important to note that the interpretation of these coefficients depends on the context of the problem and the units of the variables. Therefore, it is always recommended to interpret them in the context of the problem being studied.  \n",
    "  \n",
    "---\n",
    "#### `What are some assumptions of linear regression?`  \n",
    "Linear regression makes several assumptions about the data and the relationship between the independent and dependent variables. These assumptions are important to ensure that the model is valid and reliable. Here are some of the key assumptions of linear regression:\n",
    "\n",
    "1. Linearity: The relationship between the independent variable(s) and the dependent variable is linear. This means that the relationship can be described by a straight line. If there is a non-linear relationship, linear regression may not be appropriate.\n",
    "\n",
    "2. Independence: The observations in the data set are independent of each other. This means that the value of one observation does not depend on the value of another observation.\n",
    "\n",
    "3. Homoscedasticity: The variance of the errors is constant across all levels of the independent variable(s). In other words, the variability of the errors is the same for all values of the independent variable(s). If there is heteroscedasticity, the standard errors, confidence intervals, and hypothesis tests may be unreliable.\n",
    "\n",
    "4. Normality: The errors are normally distributed. This means that the distribution of the errors is symmetric and bell-shaped. If the errors are not normally distributed, the estimates of the coefficients may be biased.\n",
    "\n",
    "5. No multicollinearity: The independent variables are not highly correlated with each other. Multicollinearity can cause problems in linear regression, such as making the coefficients difficult to interpret and increasing the standard errors.\n",
    "\n",
    "6. No outliers: The data does not contain any extreme outliers that could have a disproportionate impact on the results.\n",
    "\n",
    "It is important to check these assumptions before interpreting the results of a linear regression model. Violations of these assumptions may indicate that the model is not appropriate, and adjustments may need to be made to improve the accuracy of the model.   \n",
    "  \n",
    "---\n",
    "#### `How do you deal with multicollinearity in linear regression?`   \n",
    "Multicollinearity refers to a situation where two or more independent variables in a linear regression model are highly correlated with each other. This can cause problems in linear regression, such as making the coefficients difficult to interpret and increasing the standard errors. Here are some ways to deal with multicollinearity in linear regression:\n",
    "\n",
    "1. Use correlation matrix: Before fitting the model, you can create a correlation matrix to check the correlation between the independent variables. If two or more variables are highly correlated, you can consider dropping one of them from the model. \n",
    "\n",
    "2. Use principal component analysis (PCA): PCA can be used to transform the original variables into a new set of uncorrelated variables, called principal components. You can then use these principal components in the regression model instead of the original variables. This can help reduce the problem of multicollinearity.\n",
    "\n",
    "3. Ridge regression: Ridge regression is a technique that adds a penalty term to the regression equation to shrink the coefficients towards zero. This helps to reduce the impact of multicollinearity on the coefficients.\n",
    "\n",
    "4. Lasso regression: Lasso regression is similar to ridge regression, but it adds a different type of penalty term to the regression equation that can lead to some coefficients being set to zero. This helps to select the most important variables and can reduce the impact of multicollinearity.\n",
    "\n",
    "5. Collect more data: If possible, collecting more data can help to reduce the impact of multicollinearity. With more data, the model can more accurately estimate the coefficients of the independent variables, even if they are highly correlated.\n",
    "\n",
    "It is important to note that there is no one-size-fits-all solution to dealing with multicollinearity. The appropriate approach depends on the specific problem and the characteristics of the data. It is also important to check the assumptions of the model after addressing multicollinearity to ensure that the model is still valid and reliable.  \n",
    "  \n",
    "---\n",
    "#### `Can we do feature engineering or add more features to deal with multicollinearity`  \n",
    "In general, adding more features to a model can help to reduce multicollinearity. This is because the additional features provide more information and can help to better capture the complexity of the relationship between the independent and dependent variables. However, it is important to add features that are relevant to the problem and are not highly correlated with the existing features.\n",
    "\n",
    "Feature engineering can also be a useful technique to deal with multicollinearity. Feature engineering involves creating new features from the existing ones that capture additional information about the relationship between the independent and dependent variables. For example, if the original features are highly correlated, you can create a new feature that is the average of the original features. This can help to reduce the correlation between the original features and improve the performance of the model.\n",
    "\n",
    "It is important to note that adding more features or performing feature engineering should be done carefully and with caution. Adding irrelevant or redundant features can actually make the problem of multicollinearity worse and decrease the performance of the model. Therefore, it is important to carefully consider which features to add and how to engineer them to ensure that they improve the performance of the model.  \n",
    "  \n",
    "---\n",
    "#### `What are the differences between simple linear regression and multiple linear regression?`  \n",
    "Simple linear regression and multiple linear regression are both techniques used in statistical modeling to explore and understand the relationship between a dependent variable and one or more independent variables. The key differences between these two types of regression are as follows:\n",
    "\n",
    "1. Number of independent variables: The main difference between simple linear regression and multiple linear regression is the number of independent variables used in the model. In simple linear regression, there is only one independent variable, whereas in multiple linear regression, there are two or more independent variables.\n",
    "\n",
    "2. Interpretation: In simple linear regression, the slope of the line represents the change in the dependent variable for a unit change in the independent variable. In multiple linear regression, the interpretation of the coefficients is slightly more complex. The coefficients represent the change in the dependent variable for a unit change in that independent variable, holding all other independent variables constant.\n",
    "\n",
    "3. Model complexity: Multiple linear regression is a more complex model than simple linear regression because it includes more independent variables. This can make the model more difficult to interpret and require more data to accurately estimate the coefficients.\n",
    "\n",
    "4. Goodness of fit: Multiple linear regression models can have a higher goodness of fit compared to simple linear regression models, as they can capture more of the variation in the dependent variable.\n",
    "\n",
    "5. Assumptions: The assumptions of simple linear regression and multiple linear regression are similar, but the multiple linear regression model has more assumptions, such as no multicollinearity among the independent variables.\n",
    "\n",
    "In summary, the key differences between simple linear regression and multiple linear regression are the number of independent variables, the interpretation of the coefficients, the model complexity, goodness of fit, and assumptions.  \n",
    "  \n",
    "---\n",
    "#### `How do you select the best subset of predictors in linear regression?`   \n",
    "Selecting the best subset of predictors in linear regression involves identifying a subset of the available predictors that provides the best performance in terms of predicting the dependent variable. Here are some methods for selecting the best subset of predictors:\n",
    "\n",
    "1. Forward selection: This method starts with an empty model and iteratively adds one predictor at a time until a desired level of performance is achieved. At each step, the predictor that most improves the performance of the model is selected.\n",
    "\n",
    "2. Backward elimination: This method starts with a full model containing all available predictors and iteratively removes one predictor at a time until a desired level of performance is achieved. At each step, the predictor whose removal least affects the performance of the model is removed.\n",
    "\n",
    "3. Stepwise regression: This method combines forward selection and backward elimination by first performing forward selection and then iteratively removing and adding predictors until a desired level of performance is achieved.\n",
    "\n",
    "4. Ridge regression: Ridge regression adds a penalty term to the regression equation to shrink the coefficients towards zero. This can help to select the most important predictors and improve the performance of the model.\n",
    "\n",
    "5. Lasso regression: Lasso regression is similar to ridge regression, but it adds a different type of penalty term to the regression equation that can lead to some coefficients being set to zero. This helps to select the most important predictors and can reduce the complexity of the model.\n",
    "\n",
    "When selecting the best subset of predictors, it is important to use a validation set or cross-validation to assess the performance of the model on new data. This can help to avoid overfitting and ensure that the selected subset of predictors generalizes well to new data. It is also important to carefully consider which predictors to include in the model and avoid including irrelevant or redundant predictors.  \n",
    "  \n",
    "---\n",
    "#### `What is regularization, and why is it used in linear regression?`  \n",
    "Regularization is a technique used in linear regression to prevent overfitting and improve the generalization performance of the model. It works by adding a penalty term to the loss function that the model is trying to minimize during the training process.\n",
    "\n",
    "In linear regression, the goal is to find a set of coefficients that best fit the data by minimizing the sum of the squared errors between the predicted values and the actual values of the dependent variable. However, in some cases, the model may overfit the data by capturing noise or irrelevant features in the training data, which can lead to poor generalization performance on new data.\n",
    "\n",
    "Regularization can help to address this problem by adding a penalty term to the loss function that discourages the model from fitting the noise or irrelevant features in the data. There are two main types of regularization commonly used in linear regression:\n",
    "\n",
    "1. L1 regularization (lasso): This adds a penalty term that is proportional to the absolute value of the coefficients, which encourages the model to set some coefficients to zero. This can help to select the most important features and reduce the complexity of the model.\n",
    "\n",
    "2. L2 regularization (ridge): This adds a penalty term that is proportional to the square of the coefficients, which encourages the model to shrink the coefficients towards zero. This can help to reduce the impact of the less important features and improve the stability of the model.\n",
    "\n",
    "Regularization can also help to deal with multicollinearity by reducing the impact of highly correlated features in the model. By adding a penalty term to the loss function, regularization can effectively reduce the variance in the estimated coefficients and improve the performance of the model on new data.  \n",
    "  \n",
    "---\n",
    "#### `What is the difference between L1 and L2 regularization in linear regression?`  \n",
    "L1 and L2 regularization are two commonly used methods for regularization in linear regression. The main difference between them is the type of penalty term that is added to the loss function of the linear regression model.\n",
    "\n",
    "L1 regularization, also known as Lasso regularization, adds a penalty term to the loss function that is proportional to the absolute value of the coefficients. This penalty term encourages some coefficients to become exactly zero, which effectively removes some of the less important features from the model. L1 regularization is a useful technique when we have many features in the model, and we want to identify the most important features to avoid overfitting. In other words, L1 regularization performs feature selection by shrinking the less important features to zero.\n",
    "\n",
    "On the other hand, L2 regularization, also known as Ridge regularization, adds a penalty term to the loss function that is proportional to the square of the coefficients. This penalty term encourages the coefficients to become small but not exactly zero, which effectively reduces the impact of the less important features in the model. L2 regularization is useful when we have a high degree of multicollinearity between the features in the model. In this case, the coefficients of highly correlated features can be highly unstable, and L2 regularization helps to stabilize the coefficients by shrinking them towards zero.\n",
    "\n",
    "In summary, L1 regularization is good for feature selection and can result in sparse models, while L2 regularization is useful for dealing with multicollinearity and can result in more stable models. However, the choice between L1 and L2 regularization depends on the specific problem at hand, and it is often useful to try both and compare their performance on validation data.   \n",
    "  \n",
    "\n",
    "---\n",
    "#### `What are some evaluation metrics for linear regression models?`  \n",
    "There are several evaluation metrics that can be used to assess the performance of a linear regression model. Here are some commonly used evaluation metrics:\n",
    "\n",
    "1. Mean Squared Error (MSE): This is the most commonly used metric for evaluating the performance of linear regression models. It measures the average of the squared differences between the predicted and actual values of the dependent variable. A lower MSE indicates better performance of the model.\n",
    "\n",
    "2. Root Mean Squared Error (RMSE): This is the square root of the MSE and is often used to give an idea of the absolute size of the error in the same units as the dependent variable. Like MSE, a lower RMSE indicates better performance.\n",
    "\n",
    "3. Mean Absolute Error (MAE): This metric measures the average of the absolute differences between the predicted and actual values of the dependent variable. MAE is less sensitive to outliers than MSE, but it does not penalize large errors as heavily as MSE.\n",
    "\n",
    "4. R-squared (R2): This metric measures the proportion of the variance in the dependent variable that is explained by the model. R2 values range from 0 to 1, with higher values indicating a better fit of the model to the data.\n",
    "\n",
    "5. Adjusted R-squared: This is a modified version of R-squared that adjusts for the number of predictors in the model. It penalizes the inclusion of irrelevant predictors and generally provides a more conservative estimate of the model's performance than R-squared.\n",
    "\n",
    "6. Mean Absolute Percentage Error (MAPE): This metric measures the average percentage difference between the predicted and actual values of the dependent variable. It is often used in cases where the scale of the dependent variable is important and can be interpreted as the average percentage error of the model.\n",
    "\n",
    "Overall, the choice of evaluation metric depends on the specific problem and the goals of the analysis. It is often useful to consider multiple metrics and compare the performance of different models on a validation dataset to select the best model.  \n",
    "\n",
    "\n",
    "1. Mean Squared Error (MSE): \n",
    "\n",
    "    $MSE = \\frac{1}{n} \\sum\\limits_{i=1}^n (y_{pred}^{(i)} - y_{actual}^{(i)})^2$\n",
    "\n",
    "2. Root Mean Squared Error (RMSE): \n",
    "\n",
    "    $RMSE = \\sqrt{MSE}$\n",
    "\n",
    "3. Mean Absolute Error (MAE): \n",
    "\n",
    "    $MAE = \\frac{1}{n} \\sum\\limits_{i=1}^n |y_{pred}^{(i)} - y_{actual}^{(i)}|$\n",
    "\n",
    "4. R-squared (R2): \n",
    "\n",
    "    $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    $SS_{res} = \\sum\\limits_{i=1}^n (y_{actual}^{(i)} - y_{pred}^{(i)})^2$\n",
    "    \n",
    "    $SS_{tot} = \\sum\\limits_{i=1}^n (y_{actual}^{(i)} - \\bar{y})^2$\n",
    "    \n",
    "5. Adjusted R-squared: \n",
    "\n",
    "    $Adjusted\\ R^2 = 1 - \\frac{(1 - R^2) \\times (n - 1)}{n - p - 1}$\n",
    "    \n",
    "    where `n` is the number of observations and `p` is the number of predictors in the model.\n",
    "\n",
    "6. Mean Absolute Percentage Error (MAPE):\n",
    "\n",
    "    $MAPE = \\frac{1}{n} \\sum\\limits_{i=1}^n \\left|\\frac{y_{pred}^{(i)} - y_{actual}^{(i)}}{y_{actual}^{(i)}}\\right| \\times 100$\n",
    "    \n",
    "    where `y_actual` is not equal to zero.  \n",
    "    \n",
    "---\n",
    "#### `What is the difference between R-squared and adjusted R-squared?`  \n",
    "R-squared (R2) and adjusted R-squared (R2adj) are both metrics used to evaluate the performance of a linear regression model, but they have some differences.\n",
    "\n",
    "R-squared is a measure of how well the regression line fits the data. It is the proportion of the variance in the dependent variable that is explained by the independent variables in the model. R-squared ranges from 0 to 1, with higher values indicating a better fit. A value of 1 means that the regression line perfectly fits the data.\n",
    "\n",
    "However, R-squared has a limitation. As you add more variables to the model, the R-squared value will always increase, even if the additional variables do not improve the model significantly. This means that R-squared cannot be used to compare models with different numbers of variables.\n",
    "\n",
    "This is where adjusted R-squared comes in. Adjusted R-squared takes into account the number of variables in the model and penalizes the addition of variables that do not improve the model significantly. The formula for adjusted R-squared is:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R2)*(n - 1)/(n - p - 1)]\n",
    "\n",
    "where n is the number of observations in the dataset and p is the number of independent variables in the model.\n",
    "\n",
    "Compared to R-squared, adjusted R-squared is a better metric for evaluating models with different numbers of variables because it penalizes the addition of unnecessary variables. If the addition of a variable does not improve the model significantly, adjusted R-squared will decrease, while R-squared may increase.  \n",
    "  \n",
    "---\n",
    "#### `How do you select the independent variables for a linear regression model?` \n",
    "`The process of selecting independent variables for a linear regression model is called feature selection. There are several methods for selecting independent variables, and the choice of method depends on the nature of the problem, the data, and the goals of the analysis. Here are some common methods for selecting independent variables:\n",
    "\n",
    "1. Domain knowledge: This involves using expert knowledge to select the independent variables that are most likely to be important in explaining the dependent variable. This approach is particularly useful when the domain is well understood and the relationship between the independent and dependent variables is well established.\n",
    "\n",
    "2. Correlation analysis: This involves calculating the correlation between each independent variable and the dependent variable and selecting the variables with the highest correlation. However, this method assumes that the relationship between the independent and dependent variables is linear, which may not always be the case.\n",
    "\n",
    "3. Stepwise regression: This involves a forward or backward selection process, where variables are added or removed from the model one at a time based on a criterion, such as the p-value or the F-statistic. This method can be automated, but it can also lead to overfitting and unstable results.\n",
    "\n",
    "4. Regularization: This involves adding a penalty term to the regression equation to constrain the coefficients of the independent variables, which can help to reduce the impact of irrelevant variables. There are two common types of regularization, L1 (lasso) and L2 (ridge) regularization.\n",
    "\n",
    "5. Principal component analysis (PCA): This involves reducing the dimensionality of the data by creating a new set of variables, called principal components, that capture the variation in the data. The principal components can then be used as independent variables in the regression model.\n",
    "\n",
    "It is important to note that the selection of independent variables should be based on a combination of these methods and not rely on any one approach exclusively. The ultimate goal is to select a set of independent variables that improves the performance of the model without overfitting the data.  \n",
    "  \n",
    "---\n",
    "#### `What is the difference between the mean squared error (MSE) and the root mean squared error (RMSE)?`  \n",
    "Mean squared error (MSE) and root mean squared error (RMSE) are both measures of the difference between the actual values and the predicted values in a regression model. The main difference between the two is that RMSE takes the square root of MSE to make the units of the error metric the same as the units of the target variable.\n",
    "\n",
    "MSE is calculated by taking the average of the squared differences between the actual and predicted values of the dependent variable. The formula for MSE is:\n",
    "\n",
    "MSE = (1/n) * Σ(y - ŷ)²\n",
    "\n",
    "where y is the actual value of the dependent variable, ŷ is the predicted value, and n is the number of observations.\n",
    "\n",
    "RMSE is the square root of MSE and is used to measure the average deviation of the predicted values from the actual values in the same units as the target variable. The formula for RMSE is:\n",
    "\n",
    "RMSE = sqrt(MSE)\n",
    "\n",
    "RMSE is a more interpretable metric because it has the same units as the target variable, whereas MSE does not. For example, if the target variable is measured in dollars, RMSE will be measured in dollars, while MSE will be measured in squared dollars.\n",
    "\n",
    "In general, RMSE is a better metric than MSE because it is more interpretable, but both are commonly used in evaluating the performance of regression models. A lower value of either metric indicates that the model is better at predicting the target variable.  \n",
    "  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48031b13",
   "metadata": {},
   "source": [
    "# `Logistic Regression`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee63ace",
   "metadata": {},
   "source": [
    "Certainly! Logistic regression is a popular classification algorithm used to model the relationship between input features and binary or categorical target variables. It uses a sigmoid function to map the output to a probability value between 0 and 1.\n",
    "\n",
    "In logistic regression, we want to estimate the probability that an input sample belongs to a certain class. The logistic regression model can be represented mathematically as:\n",
    "\n",
    "$$\n",
    "P(y=1 | x) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where $P(y=1 | x)$ is the probability of the target variable $y$ being 1 given the input feature vector $x$, and $z$ is the linear combination of the input features with their respective coefficients:\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "Here, $\\beta_0, \\beta_1, \\ldots, \\beta_n$ are the regression coefficients that determine the contribution of each input feature to the final prediction, and $x_1, x_2, \\ldots, x_n$ are the corresponding input features.\n",
    "\n",
    "The sigmoid function, denoted as $S(z)$, is used to transform the linear combination $z$ into a probability value between 0 and 1. It is defined as:\n",
    "\n",
    "$$\n",
    "S(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "The sigmoid function has the property that it asymptotically approaches 0 as $z$ approaches negative infinity, and approaches 1 as $z$ approaches positive infinity.\n",
    "\n",
    "In practice, to train a logistic regression model, we typically use maximum likelihood estimation to estimate the optimal values for the regression coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_n$. This involves finding the values that maximize the likelihood of observing the given training data.\n",
    "\n",
    "To make predictions with the logistic regression model, we use the estimated coefficients and input features to calculate the linear combination $z$, and then pass it through the sigmoid function to obtain the predicted probability. The class label is then determined based on a threshold applied to the predicted probability.\n",
    "\n",
    "In summary, logistic regression uses the sigmoid function to model the relationship between input features and the probability of a binary or categorical outcome. It is a widely used algorithm for binary classification tasks and can be extended to handle multi-class classification as well.   \n",
    "\n",
    "---\n",
    "\n",
    "First, let's define the logistic function and the cost function as before:\n",
    "\n",
    "Logistic function:\n",
    "$$\n",
    "h(x) = \\frac{1}{1 + e^{-(x \\cdot m + c)}}\n",
    "$$\n",
    "\n",
    "Cost function:\n",
    "$$\n",
    "J(m, c) = -\\sum_{i=1}^{N} y_i \\log(h(x_i)) + (1 - y_i) \\log(1 - h(x_i))\n",
    "$$\n",
    "\n",
    "Now, let's derive the partial derivatives step by step:\n",
    "\n",
    "1. Derivative with respect to \"m\":\n",
    "To compute the derivative of the cost function with respect to \"m\", we'll start by taking the derivative of the logistic function with respect to \"m\":\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h(x)}{\\partial m} = \\frac{\\partial}{\\partial m} \\left( \\frac{1}{1 + e^{-(x \\cdot m + c)}} \\right)\n",
    "$$\n",
    "\n",
    "Using the chain rule, we have:\n",
    "\n",
    "$$\n",
    "= \\frac{e^{-(x \\cdot m + c)}}{(1 + e^{-(x \\cdot m + c)})^2} \\cdot (-x)\n",
    "$$\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$$\n",
    "= -\\frac{x e^{-(x \\cdot m + c)}}{(1 + e^{-(x \\cdot m + c)})^2}\n",
    "$$\n",
    "\n",
    "Next, we'll compute the derivative of the cost function with respect to \"m\":\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial m} = -\\sum_{i=1}^{N} \\left( y_i \\frac{1}{h(x_i)} - (1 - y_i) \\frac{1}{1 - h(x_i)} \\right) \\frac{\\partial h(x_i)}{\\partial m}\n",
    "$$\n",
    "\n",
    "Substituting the derivative of the logistic function we computed earlier:\n",
    "\n",
    "$$\n",
    "= -\\sum_{i=1}^{N} \\left( y_i \\frac{1}{h(x_i)} - (1 - y_i) \\frac{1}{1 - h(x_i)} \\right) \\left( -\\frac{x_i e^{-(x_i \\cdot m + c)}}{(1 + e^{-(x_i \\cdot m + c)})^2} \\right)\n",
    "$$\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^{N} \\left( y_i - h(x_i) \\right) x_i\n",
    "$$\n",
    "\n",
    "So, the derivative of the cost function with respect to \"m\" is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial m} = \\sum_{i=1}^{N} \\left( h(x_i) - y_i \\right) x_i\n",
    "$$\n",
    "\n",
    "2. Derivative with respect to \"c\":\n",
    "Similarly, to compute the derivative of the cost function with respect to \"c\", we'll start by taking the derivative of the logistic function with respect to \"c\":\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h(x)}{\\partial c} = \\frac{\\partial}{\\partial c} \\left( \\frac{1}{1 + e^{-(x \\cdot m + c)}} \\right)\n",
    "$$\n",
    "\n",
    "Using the chain rule, we have:\n",
    "\n",
    "$$\n",
    "= \\frac{e^{-(x \\cdot m + c)}}{(1 + e^{-(x \\cdot m + c)})^2} \\cdot (-1)\n",
    "$$\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$$\n",
    "= -\\frac{e^{-(x \\cdot m + c)}}{1 + e^{-(x \\cdot m + c)}}\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "Next, we'll compute the derivative of the cost function with respect to \"c\":\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial c} = -\\sum_{i=1}^{N} \\left( y_i \\frac{1}{h(x_i)} - (1 - y_i) \\frac{1}{1 - h(x_i)} \\right) \\frac{\\partial h(x_i)}{\\partial c}\n",
    "$$\n",
    "\n",
    "Substituting the derivative of the logistic function we computed earlier:\n",
    "\n",
    "$$\n",
    "= -\\sum_{i=1}^{N} \\left( y_i \\frac{1}{h(x_i)} - (1 - y_i) \\frac{1}{1 - h(x_i)} \\right) \\left( -\\frac{e^{-(x_i \\cdot m + c)}}{1 + e^{-(x_i \\cdot m + c)}} \\right)\n",
    "$$\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^{N} \\left( y_i - h(x_i) \\right)\n",
    "$$\n",
    "\n",
    "So, the derivative of the cost function with respect to \"c\" is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial c} = \\sum_{i=1}^{N} \\left( h(x_i) - y_i \\right)\n",
    "$$\n",
    "\n",
    "These derivatives represent the gradients of the cost function with respect to \"m\" and \"c\", respectively. By using these derivatives, we can update the coefficients \"m\" and \"c\" iteratively using optimization algorithms like gradient descent to minimize the cost function and improve the performance of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ad98f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Cost:287.2681214753093\n",
      "Epoch:50, Cost:143.41917286649874\n",
      "Epoch:100, Cost:108.72404955545852\n",
      "Epoch:150, Cost:91.76493717899409\n",
      "Epoch:200, Cost:81.31897601756947\n",
      "Epoch:250, Cost:74.09852392876914\n",
      "Epoch:300, Cost:68.74517434798051\n",
      "Epoch:350, Cost:64.58185475365113\n",
      "Epoch:400, Cost:61.229105576051595\n",
      "Epoch:450, Cost:58.45631130375573\n",
      "Epoch:500, Cost:56.114608646630266\n",
      "Epoch:550, Cost:54.10327268400567\n",
      "Epoch:600, Cost:52.351510165760295\n",
      "Epoch:650, Cost:50.807978682058284\n",
      "Epoch:700, Cost:49.434447688400034\n",
      "Epoch:750, Cost:48.20180353191513\n",
      "Epoch:800, Cost:47.08744225451344\n",
      "Epoch:850, Cost:46.073516178401924\n",
      "Epoch:900, Cost:45.1457234384824\n",
      "Epoch:950, Cost:44.29245291919557\n",
      "Epoch:1000, Cost:43.50416783566102\n",
      "Epoch:1050, Cost:42.772953224036804\n",
      "Epoch:1100, Cost:42.09217831048018\n",
      "Epoch:1150, Cost:41.45624087272234\n",
      "Epoch:1200, Cost:40.860371092906426\n",
      "Epoch:1250, Cost:40.30047922522861\n",
      "Epoch:1300, Cost:39.77303597575571\n",
      "Epoch:1350, Cost:39.27497761209431\n",
      "Epoch:1400, Cost:38.80362998426953\n",
      "Epoch:1450, Cost:38.35664716117067\n",
      "Epoch:1500, Cost:37.93196147385321\n",
      "Epoch:1550, Cost:37.527742542702605\n",
      "Epoch:1600, Cost:37.1423634402032\n",
      "Epoch:1650, Cost:36.774372566143214\n",
      "Epoch:1700, Cost:36.42247012973452\n",
      "Epoch:1750, Cost:36.08548837278808\n",
      "Epoch:1800, Cost:35.76237485054663\n",
      "Epoch:1850, Cost:35.45217822686363\n",
      "Epoch:1900, Cost:35.154036148838394\n",
      "Epoch:1950, Cost:34.86716485055492\n",
      "Epoch:2000, Cost:34.590850201960485\n",
      "Epoch:2050, Cost:34.324439971402064\n",
      "Epoch:2100, Cost:34.06733711209469\n",
      "Epoch:2150, Cost:33.81899391621257\n",
      "Epoch:2200, Cost:33.57890690719682\n",
      "Epoch:2250, Cost:33.34661236264197\n",
      "Epoch:2300, Cost:33.121682377836656\n",
      "Epoch:2350, Cost:32.903721394510484\n",
      "Epoch:2400, Cost:32.69236313123193\n",
      "Epoch:2450, Cost:32.48726786171279\n",
      "Epoch:2500, Cost:32.288119995403896\n",
      "Epoch:2550, Cost:32.09462592153043\n",
      "Epoch:2600, Cost:31.906512083361104\n",
      "Epoch:2650, Cost:31.723523254245496\n",
      "Epoch:2700, Cost:31.545420990932616\n",
      "Epoch:2750, Cost:31.371982243053168\n",
      "Epoch:2800, Cost:31.202998100492287\n",
      "Epoch:2850, Cost:31.038272662805966\n",
      "Epoch:2900, Cost:30.87762201689396\n",
      "Epoch:2950, Cost:30.720873310910687\n",
      "Epoch:3000, Cost:30.56786391390564\n",
      "Epoch:3050, Cost:30.418440651987055\n",
      "Epoch:3100, Cost:30.27245911292363\n",
      "Epoch:3150, Cost:30.12978301206838\n",
      "Epoch:3200, Cost:29.990283613326802\n",
      "Epoch:3250, Cost:29.85383919962303\n",
      "Epoch:3300, Cost:29.720334587949505\n",
      "Epoch:3350, Cost:29.589660684641824\n",
      "Epoch:3400, Cost:29.46171407700224\n",
      "Epoch:3450, Cost:29.33639665782064\n",
      "Epoch:3500, Cost:29.21361527971375\n",
      "Epoch:3550, Cost:29.093281436530354\n",
      "Epoch:3600, Cost:28.975310969359178\n",
      "Epoch:3650, Cost:28.859623794930734\n",
      "Epoch:3700, Cost:28.746143654429947\n",
      "Epoch:3750, Cost:28.63479788093543\n",
      "Epoch:3800, Cost:28.525517183878986\n",
      "Epoch:3850, Cost:28.418235449076647\n",
      "Epoch:3900, Cost:28.312889553021844\n",
      "Epoch:3950, Cost:28.209419190257403\n",
      "Epoch:4000, Cost:28.10776671275439\n",
      "Epoch:4050, Cost:28.00787698032533\n",
      "Epoch:4100, Cost:27.90969722118986\n",
      "Epoch:4150, Cost:27.81317690188965\n",
      "Epoch:4200, Cost:27.718267605824\n",
      "Epoch:4250, Cost:27.624922919738715\n",
      "Epoch:4300, Cost:27.533098327563437\n",
      "Epoch:4350, Cost:27.442751111042117\n",
      "Epoch:4400, Cost:27.35384025665016\n",
      "Epoch:4450, Cost:27.26632636833525\n",
      "Epoch:4500, Cost:27.18017158565641\n",
      "Epoch:4550, Cost:27.09533950693237\n",
      "Epoch:4600, Cost:27.011795117041917\n",
      "Epoch:4650, Cost:26.929504719547072\n",
      "Epoch:4700, Cost:26.848435872838227\n",
      "Epoch:4750, Cost:26.768557330022308\n",
      "Epoch:4800, Cost:26.68983898229844\n",
      "Epoch:4850, Cost:26.612251805584762\n",
      "Epoch:4900, Cost:26.535767810178495\n",
      "Epoch:4950, Cost:26.460359993248105\n",
      "[-0.5581796  -0.60408723 -0.54587466 -0.56695992 -0.22173514 -0.01611509\n",
      " -0.52594664 -0.62687945 -0.03547238  0.39150562 -0.65389037  0.03785234\n",
      " -0.52509882 -0.55070413 -0.06124474  0.38870343  0.08429219 -0.04308232\n",
      "  0.21791782  0.43745024 -0.74665852 -0.80345149 -0.69105457 -0.71527201\n",
      " -0.69457171 -0.20798606 -0.60283917 -0.63406509 -0.47173915 -0.16521601\n",
      "  0.50403692]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding logistic regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets,preprocessing\n",
    "\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "breast_cancer.DESCR.split(\"\\n\")\n",
    "\n",
    "X = breast_cancer.data\n",
    "Y = breast_cancer.target\n",
    "\n",
    "X_df = pd.DataFrame(X)\n",
    "X_df.describe()\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "X_train,X_val,Y_train,Y_val = model_selection.train_test_split(X,Y)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "\n",
    "X_train_scaled.shape\n",
    "\n",
    "\n",
    "#### Inserting column of ones in dataset\n",
    "\n",
    "X_train_scaled = np.append(X_train_scaled,np.ones(X_train_scaled.shape[0]).reshape(-1,1),axis=1)\n",
    "\n",
    "\n",
    "X_train_scaled.shape,X_train_scaled[0].shape\n",
    "\n",
    "\n",
    "def sigmoid(agg):\n",
    "    \n",
    "    return 1/(1+np.exp(-agg))\n",
    "\n",
    "def cost(X_train,Y_train,m):\n",
    "    \n",
    "    cost_ = 0\n",
    "    N = X_train.shape[0]\n",
    "    for i in range(N):\n",
    "        agg = (X_train[i]*m).sum()\n",
    "        h = sigmoid(agg)\n",
    "        cost = -Y_train[i]*np.log(h) - (1-Y_train[i])*np.log(1-h)\n",
    "        cost_ += cost\n",
    "    \n",
    "    return cost_\n",
    "\n",
    "def step_gradient(X_train,Y_train,lr,m):\n",
    "    \n",
    "    N = X_train.shape[0]\n",
    "    slope_m = np.zeros(X_train.shape[1])\n",
    "    for i in range(N):\n",
    "        agg = (X_train[i]*m).sum()\n",
    "        h = sigmoid(agg)\n",
    "        slope_m+=(-1/N)*(Y_train[i]-h)*X_train[i]\n",
    "        \n",
    "    m = m - lr*slope_m\n",
    "    return m\n",
    "\n",
    "def fit(X_train,Y_train,epochs=100,lr=0.01):\n",
    "    \n",
    "    m = np.zeros(X_train.shape[1])\n",
    "    cost_array = []\n",
    "    unit = epochs//100\n",
    "    for i in range(epochs):\n",
    "        m = step_gradient(X_train,Y_train,lr,m)\n",
    "        cost_ = cost(X_train,Y_train,m)\n",
    "        cost_array.append(cost_)\n",
    "        if i%unit==0:\n",
    "            print(\"Epoch:{}, Cost:{}\".format(i,cost_))\n",
    "    \n",
    "    return m,cost_array\n",
    "\n",
    "def predict(X_test,m):\n",
    "    \n",
    "    y_pred = []\n",
    "    N = X_test.shape[0]\n",
    "    for i in range(N):\n",
    "        agg = (X_test[i]*m).sum()\n",
    "        h = sigmoid(agg)\n",
    "        if h>=0.5:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            \n",
    "    return np.array(y_pred)\n",
    "\n",
    "def accuracy(Y_test,Y_pred):\n",
    "    \n",
    "    correct = 0\n",
    "    N = Y_test.shape[0]\n",
    "    correct = (Y_test==Y_pred).sum()\n",
    "    \n",
    "    return (correct/N)*100\n",
    "\n",
    "m,cost_array = fit(X_train_scaled,Y_train,5000,0.01)\n",
    "print(m)\n",
    "\n",
    "plt.plot(cost_array)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "y_pred_train = predict(X_train_scaled,m)\n",
    "\n",
    "\n",
    "accuracy(Y_train,y_pred_train)\n",
    "\n",
    "\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "X_val_scaled = np.append(X_val_scaled,np.ones(X_val_scaled.shape[0]).reshape(-1,1),axis=1)\n",
    "\n",
    "\n",
    "y_pred_val = predict(X_val_scaled,m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdc474e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'C': 0.2, 'penalty': 'l2'}\n",
      "Best Score:  0.9806551777674274\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'penalty': [ 'l2'], # only supports l2\n",
    "    'C': [0.1, 1.0, 10.0, 0.2, 0.3, 0.44]\n",
    "}\n",
    "\n",
    "# Perform k-fold cross-validation with GridSearchCV\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=kfold, scoring='accuracy')\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Print the best hyperparameters and best score\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211163d3",
   "metadata": {},
   "source": [
    "# `Decision Tree`\n",
    "\n",
    "Certainly! Let's explain decision trees and the key metrics used in decision tree algorithms, including entropy, information gain, and Gini index.\n",
    "\n",
    "Decision Trees:\n",
    "Decision trees are supervised machine learning algorithms used for both classification and regression tasks. They make predictions by recursively splitting the feature space into regions based on feature values, resulting in a tree-like structure. Each internal node represents a test on a feature, and each leaf node represents a prediction or outcome.\n",
    "\n",
    "Entropy:\n",
    "Entropy is a measure of impurity or disorder in a set of class labels. In the context of decision trees, entropy is used as a metric to evaluate the quality of a split at each internal node. The entropy of a set \\(S\\) with respect to class labels is calculated using the following formula:\n",
    "\n",
    "$$\\text{Entropy}(S) = - \\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
    "\n",
    "where $C$ is the number of distinct classes in the set, and $p_i$ is the proportion of examples belonging to class $i$ in set $S$. The entropy is maximum (1) when the class distribution is uniform and decreases to 0 when all examples belong to a single class.\n",
    "\n",
    "Information Gain:\n",
    "Information gain is a metric that measures the reduction in entropy achieved by a particular feature split. The information gain of a set $S$ with respect to a feature $A$ is computed as:\n",
    "\n",
    "$$ \\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropy}(S_v) $$\n",
    "\n",
    "where $\\text{Values}(A)$ represents the possible values of feature $A$, and $S_v$ is the subset of examples in $S$ with feature $A$ taking value $v$. The information gain is the difference between the entropy of the set before the split and the weighted sum of the entropies of the resulting subsets. A higher information gain indicates a more informative split.\n",
    "\n",
    "Gini Index:\n",
    "The Gini index is another metric used to evaluate the impurity of a set of class labels. Similar to entropy, the Gini index is used to measure the quality of a split at each internal node. The Gini index of a set $S$ with respect to class labels is calculated using the following formula:\n",
    "\n",
    "$$\\text{Gini}(S) = 1 - \\sum_{i=1}^{C} p_i^2 $$\n",
    "\n",
    "where $C$ is the number of distinct classes in the set, and $p_i$ is the proportion of examples belonging to class $i$ in set $S$. The Gini index is minimized (0) when all examples belong to a single class.\n",
    "\n",
    "Other Metrics:\n",
    "Besides entropy, information gain, and Gini index, there are additional metrics used in decision trees, depending on the specific implementation or problem:\n",
    "\n",
    "- Misclassification Error: It measures the proportion of misclassified examples in a set and is given by:\n",
    "\n",
    "  $$ \\text{Misclassification Error}(S) = 1 - \\max(p_1, p_2, \\ldots, p_C) $$\n",
    "\n",
    "  where $C$ is the number of distinct classes in the set, and $p_i$ is the proportion of examples belonging to class \\(i\\) in set $S$.\n",
    "\n",
    "- Gain Ratio: It adjusts the information gain metric to account for bias towards features with a large number of values. The gain ratio is calculated by dividing the information gain by the intrinsic information of the feature. The intrinsic information measures the potential information contained in the feature before splitting.\n",
    "\n",
    "These metrics are used to evaluate the quality of\n",
    "\n",
    " splits in decision trees and determine the best feature to split on at each internal node. The goal is to find splits that maximize information gain, minimize entropy or Gini index, or minimize other impurity measures, resulting in a more informative and accurate decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62a3a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "Best Score:  0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Create a KFold object for k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=kf)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f0250",
   "metadata": {},
   "source": [
    "### `Random Forest `  \n",
    "Random Forest is an ensemble learning method that combines multiple decision trees to create a more robust and accurate model. It is widely used for both classification and regression tasks. Here's an explanation of Random Forest:\n",
    "\n",
    "1. Ensemble Learning:\n",
    "Ensemble learning is a technique that combines multiple individual models to make predictions. Random Forest is an ensemble learning method that combines the predictions of multiple decision trees to make a final prediction. This combination of models helps to reduce overfitting and improve the overall performance and generalization of the model.\n",
    "\n",
    "2. Decision Trees:\n",
    "Random Forest is built upon the concept of decision trees. A decision tree is a flowchart-like model that makes predictions by recursively splitting the feature space based on different features and their values. Each internal node represents a feature test, and each leaf node represents a prediction or outcome.\n",
    "\n",
    "3. Randomness in Random Forest:\n",
    "Random Forest introduces randomness in two main aspects:\n",
    "\n",
    "   a. Random Subset of Features: At each split in a decision tree, only a random subset of features is considered for splitting. This random feature selection helps to reduce the correlation between trees and encourages diversity among them.\n",
    "\n",
    "   b. Bootstrap Aggregation (Bagging): Random Forest uses a technique called bootstrap aggregating or bagging. It involves creating multiple subsets of the original training data by randomly sampling with replacement. Each subset is then used to train a separate decision tree in the forest. This helps to introduce variability in the training data and improve the robustness of the model.\n",
    "\n",
    "4. Prediction in Random Forest:\n",
    "To make a prediction using a Random Forest, each decision tree in the forest independently predicts the outcome based on its own set of rules. For classification tasks, the final prediction is made by majority voting, where the class that receives the most votes across all the trees is selected as the final prediction. For regression tasks, the final prediction is the average or mean of the predictions from all the trees.\n",
    "\n",
    "5. Advantages of Random Forest:\n",
    "   - Random Forest is robust against overfitting and tends to generalize well to unseen data.\n",
    "   - It can handle a large number of features and is not sensitive to outliers.\n",
    "   - Random Forest provides a measure of feature importance, which can be useful for feature selection.\n",
    "   - It can be parallelized and easily scaled to large datasets.\n",
    "\n",
    "Random Forest is a powerful and versatile algorithm that has shown excellent performance in a wide range of machine learning tasks. It is known for its simplicity, interpretability, and ability to handle complex data relationships.   \n",
    "\n",
    "  ---\n",
    "  Random Forest is an ensemble learning algorithm that combines multiple decision trees to make predictions. Here are the steps involved in executing the Random Forest algorithm:\n",
    "\n",
    "1. **Data Preparation**: Random Forest requires a labeled dataset to train the model. The dataset is typically split into features (input variables) and labels (output variable).\n",
    "\n",
    "2. **Bootstrap Sampling**: Random Forest employs a technique called bootstrap sampling, where multiple random subsets of the original dataset are created by sampling with replacement. Each subset, known as a bootstrap sample, has the same size as the original dataset but may contain duplicate instances.\n",
    "\n",
    "3. **Tree Construction**: For each bootstrap sample, a decision tree is constructed. Each tree is trained on a subset of features randomly selected from the total feature set. This random subset of features helps introduce diversity among the trees.\n",
    "\n",
    "4. **Decision Tree Training**: The decision tree is trained using a recursive process called recursive binary splitting. At each node of the tree, the algorithm selects the best split among a subset of features based on a splitting criterion (e.g., Gini impurity or information gain). This process continues recursively until a stopping criterion is met, such as reaching a maximum tree depth or the minimum number of samples required to split a node.\n",
    "\n",
    "5. **Ensemble Aggregation**: Once all the decision trees are constructed, predictions are made by aggregating the individual predictions of each tree. For classification tasks, the class with the majority vote among the trees is selected as the final prediction. For regression tasks, the average of the predicted values from all the trees is taken.\n",
    "\n",
    "6. **Out-of-Bag Evaluation**: Random Forest has a built-in evaluation mechanism called out-of-bag (OOB) evaluation. Since each decision tree is trained on a different bootstrap sample, the instances that were not included in the bootstrap sample (out-of-bag instances) can be used to evaluate the performance of the model without the need for a separate validation set.\n",
    "\n",
    "7. **Feature Importance**: Random Forest can provide an estimate of feature importance. During the tree construction process, the algorithm keeps track of how much each feature contributes to reducing impurity or error. This information can be used to rank the features based on their importance.\n",
    "\n",
    "8. **Prediction**: Once the Random Forest model is trained, it can be used to make predictions on new, unseen data by passing the data through each individual decision tree and aggregating the results.\n",
    "\n",
    "The steps mentioned above outline the general execution process of the Random Forest algorithm. The number of decision trees, the maximum tree depth, and other hyperparameters can be specified to customize the Random Forest model for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98936e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Score:  0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Create a KFold object for k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=kf)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928abac",
   "metadata": {},
   "source": [
    "# `Naive Bayes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6856151",
   "metadata": {},
   "source": [
    "Certainly! Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem with the assumption of independence among features. Here's an explanation of Naive Bayes with the mathematical derivation in LaTeX format:\n",
    "\n",
    "Assumptions:\n",
    "- We have a dataset with $n$ instances and $d$ features.\n",
    "- The features are denoted as $x_1, x_2, \\ldots, x_d$.\n",
    "- The class variable is denoted as $y$ with $c$ possible classes.\n",
    "\n",
    "Bayes' Theorem:\n",
    "Bayes' theorem provides a way to calculate the posterior probability of a class given the features. The formula for Bayes' theorem is:\n",
    "\n",
    "$$ P(y|x_1, x_2, \\ldots, x_d) = \\frac{P(y)P(x_1, x_2, \\ldots, x_d|y)}{P(x_1, x_2, \\ldots, x_d)} $$\n",
    "\n",
    "Naive Bayes Assumption:\n",
    "Naive Bayes makes a simplifying assumption that the features $x_1, x_2, \\ldots, x_d$ are conditionally independent given the class variable $y$. Mathematically, this can be expressed as:\n",
    "\n",
    "$$ P(x_1, x_2, \\ldots, x_d|y) = P(x_1|y)P(x_2|y)\\ldots P(x_d|y) $$\n",
    "\n",
    "Applying the Naive Bayes assumption, the posterior probability can be calculated as:\n",
    "\n",
    "$$ P(y|x_1, x_2, \\ldots, x_d) = \\frac{P(y)P(x_1|y)P(x_2|y)\\ldots P(x_d|y)}{P(x_1, x_2, \\ldots, x_d)} $$\n",
    "\n",
    "Classifier Decision Rule:\n",
    "To classify a new instance with feature values $x_1^*, x_2^*, \\ldots, x_d^*$, we use a classifier decision rule that selects the class with the maximum posterior probability. Mathematically, the decision rule is:\n",
    "\n",
    "$$ \\hat{y} = \\underset{y}{\\arg\\max} \\ P(y)P(x_1^*|y)P(x_2^*|y)\\ldots P(x_d^*|y) $$\n",
    "\n",
    "Estimating Class and Feature Probabilities:\n",
    "To use Naive Bayes, we need to estimate the class probabilities \\(P(y)\\) and the feature probabilities $P(x_i|y)$ from the training data.\n",
    "\n",
    "- Estimating Class Probability: The class probability \\(P(y)\\) can be estimated by counting the number of instances belonging to each class and dividing it by the total number of instances.\n",
    "\n",
    "$$ P(y) = \\frac{\\text{Count of instances with class } y}{\\text{Total number of instances}} $$\n",
    "\n",
    "- Estimating Feature Probability: The feature probability \\(P(x_i|y)\\) can be estimated differently depending on the type of feature. For continuous features, it is often assumed to follow a specific distribution (e.g., Gaussian) and estimated using techniques such as maximum likelihood estimation. For discrete features, it is estimated by counting the occurrences of each feature value given the class and dividing it by the total count of instances in that class.\n",
    "\n",
    "Naive Bayes is a simple yet powerful classification algorithm that works well in practice, especially when the independence assumption holds reasonably well.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Let's consider a classification problem with a feature vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ and a target class variable $y$ that can take on one of $C$ possible classes.\n",
    "\n",
    "The goal is to determine the posterior probability $P(y | \\mathbf{x})$, which represents the probability of class $y$ given the observed features $\\mathbf{x}$.\n",
    "\n",
    "According to Bayes' theorem, the posterior probability can be calculated as:\n",
    "\n",
    "$$P(y | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y) \\cdot P(y)}{P(\\mathbf{x})}$$\n",
    "\n",
    "where:\n",
    "- $P(y | \\mathbf{x})$ is the posterior probability (probability of class $y$ given the features $\\mathbf{x}$).\n",
    "- $P(\\mathbf{x} | y)$ is the likelihood (probability of observing features $\\mathbf{x}$ given class $y$).\n",
    "- $P(y)$ is the prior probability (probability of class $y$ occurring).\n",
    "- $P(\\mathbf{x})$ is the evidence probability (probability of observing the features $\\mathbf{x}$).\n",
    "\n",
    "The \"naive\" assumption in Naive Bayes is that the features are conditionally independent given the class. Mathematically, it can be written as:\n",
    "\n",
    "$$P(\\mathbf{x} | y) = P(x_1 | y) \\cdot P(x_2 | y) \\cdot \\ldots \\cdot P(x_n | y)$$\n",
    "\n",
    "Based on this assumption, the posterior probability can be simplified as:\n",
    "\n",
    "$$P(y | \\mathbf{x}) = \\frac{P(y) \\cdot P(x_1 | y) \\cdot P(x_2 | y) \\cdot \\ldots \\cdot P(x_n | y)}{P(\\mathbf{x})}$$\n",
    "\n",
    "To classify a new instance $\\mathbf{x}'$, we calculate the posterior probability for each class and select the class with the highest probability.\n",
    "\n",
    "In practice, the prior probabilities $P(y)$ and the likelihood probabilities $P(x_i | y)$ can be estimated from the training data using maximum likelihood estimation or other techniques.\n",
    "\n",
    "Naive Bayes classifiers are popular due to their simplicity and computational efficiency. However, the assumption of feature independence may not hold in some real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d889ca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {}\n",
      "Best Score:  0.9600000000000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Create a KFold object for k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV (empty since Naive Bayes has no hyperparameters to tune)\n",
    "param_grid = {}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=kf)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255dfceb",
   "metadata": {},
   "source": [
    "# `Multinomial Naive Bayes`  \n",
    "Multinomial Naive Bayes is an extension of the Naive Bayes algorithm that is suitable for classification tasks with discrete features. It is commonly used for text classification problems. \n",
    "\n",
    "**Its is used when we have discrete data (e.g. movie ratings ranging 1 and 5 as each rating will have certain frequency to represent). In text learning we have the count of each word to predict the class or label.**\n",
    "\n",
    "Let's consider a classification problem with a feature vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ and a target class variable $y$ that can take on one of $C$ possible classes.\n",
    "\n",
    "The goal is to determine the posterior probability $P(y | \\mathbf{x})$, which represents the probability of class $y$ given the observed features $\\mathbf{x}$.\n",
    "\n",
    "According to Bayes' theorem, the posterior probability can be calculated as:\n",
    "\n",
    "$$P(y | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y) \\cdot P(y)}{P(\\mathbf{x})}$$\n",
    "\n",
    "where:\n",
    "- $P(y | \\mathbf{x})$ is the posterior probability (probability of class $y$ given the features $\\mathbf{x}$).\n",
    "- $P(\\mathbf{x} | y)$ is the likelihood (probability of observing features $\\mathbf{x}$ given class $y$).\n",
    "- $P(y)$ is the prior probability (probability of class $y$ occurring).\n",
    "- $P(\\mathbf{x})$ is the evidence probability (probability of observing the features $\\mathbf{x}$).\n",
    "\n",
    "In Multinomial Naive Bayes, the feature vector $\\mathbf{x}$ represents the frequencies or counts of discrete features. The assumption is that the feature counts follow a multinomial distribution.\n",
    "\n",
    "The likelihood $P(\\mathbf{x} | y)$ can be modeled using the multinomial distribution, and it is calculated as:\n",
    "\n",
    "$$P(\\mathbf{x} | y) = \\prod_{i=1}^{n} P(x_i | y)^{x_i}$$\n",
    "\n",
    "where $P(x_i | y)$ is the probability of feature $x_i$ occurring given class $y$.\n",
    "\n",
    "The prior probability $P(y)$ can be estimated from the training data by calculating the relative frequencies of each class.\n",
    "\n",
    "To classify a new instance $\\mathbf{x}'$, we calculate the posterior probability for each class and select the class with the highest probability.\n",
    "\n",
    "In practice, to avoid issues with zero probabilities, techniques like Laplace smoothing or add-one smoothing are often employed to estimate the probabilities. Laplace smoothing adds a small constant to the counts to ensure non-zero probabilities.\n",
    "\n",
    "Multinomial Naive Bayes is widely used for text classification tasks, where the feature vectors represent word frequencies or term frequencies in documents. It assumes independence between features but can still perform well in practice, especially when the feature vectors are sparse and high-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c635c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'clf__alpha': 0.1, 'vect__max_features': 3000, 'vect__ngram_range': (1, 1)}\n",
      "Best Score:  0.9539234346486667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "# Create a pipeline for text preprocessing and Multinomial Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'vect__max_features': [1000, 2000, 3000],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__alpha': [0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "# Create a KFold object for k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=kf)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(data_train.data, data_train.target)\n",
    "\n",
    "# Predict the test data\n",
    "predicted = grid_search.predict(data_test.data)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858a564",
   "metadata": {},
   "source": [
    "# `Gaussian Naive Bayes`  \n",
    "Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes the features follow a Gaussian (normal) distribution. It is suitable for continuous or numeric features.    \n",
    "**Because of the assumption of the normal distribution, Gaussian Naive Bayes is used in cases when all our features are continuous. For example in Iris dataset features are sepal width, petal width, sepal length, petal length. So its features can have different values in data set as width and length can vary. We can’t represent features in terms of their occurrences. This means data is continuous. Hence we use Gaussian Naive Bayes here.**\n",
    "\n",
    "Let's consider a classification problem with a feature vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ and a target class variable $y$ that can take on one of $C$ possible classes.\n",
    "\n",
    "The goal is to determine the posterior probability $P(y | \\mathbf{x})$, which represents the probability of class $y$ given the observed features \\(\\mathbf{x}\\).\n",
    "\n",
    "According to Bayes' theorem, the posterior probability can be calculated as:\n",
    "\n",
    "$P(y | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y) \\cdot P(y)}{P(\\mathbf{x})}$\n",
    "\n",
    "where:\n",
    "- $P(y | \\mathbf{x})$ is the posterior probability (probability of class $y$ given the features $\\mathbf{x}$).\n",
    "- $P(\\mathbf{x} | y)$ is the likelihood (probability of observing features $\\mathbf{x}$ given class $y$). \n",
    "- $P(y)$ is the prior probability (probability of class $y$ occurring).\n",
    "- $P(\\mathbf{x})$ is the evidence probability (probability of observing the features $\\mathbf{x}$).\n",
    "\n",
    "In Gaussian Naive Bayes, the assumption is that the feature values given each class follow a Gaussian distribution. Mathematically, it can be written as:\n",
    "\n",
    "$$P(x_i | y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right)$$\n",
    "\n",
    "where $P(x_i | y)$ is the probability of feature $x_i$ occurring given class $y$, $\\mu_{y,i}$ is the mean of feature $x_i$ for class $y$, and $\\sigma_{y,i}^2$ is the variance of feature $x_i$ for class $y$.\n",
    "\n",
    "To classify a new instance $\\mathbf{x}'$, we calculate the posterior probability for each class and select the class with the highest probability.\n",
    "\n",
    "In practice, the prior probabilities $P(y)$, means $\\mu_{y,i}$, and variances $\\sigma_{y,i}^2$ can be estimated from the training data using maximum likelihood estimation or other techniques.\n",
    "\n",
    "Gaussian Naive Bayes is computationally efficient and can handle continuous or numeric features. However, it assumes that the features are independent given the class, which may not always hold in real-world scenarios.   \n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "In Gaussian Naive Bayes, we assume that the feature values for each class follow a Gaussian (normal) distribution. This assumption allows us to model the likelihood $P(x_i | y)$ as a Gaussian distribution for each feature $x_i$ given class $y$.\n",
    "\n",
    "The Gaussian probability density function (PDF) is given by:\n",
    "\n",
    "$$P(x_i | y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right)$$\n",
    "\n",
    "where:\n",
    "- $P(x_i | y)$ is the probability of feature $x_i$ occurring given class \\(y\\).\n",
    "- $\\mu_{y,i}$ is the mean of feature \\(x_i\\) for class \\(y\\).\n",
    "- $\\sigma_{y,i}^2$ is the variance of feature \\(x_i\\) for class \\(y\\).\n",
    "\n",
    "To calculate the posterior probability $P(y | \\mathbf{x})$ using Bayes' theorem, we need to compute three components:\n",
    "\n",
    "1. Prior Probability $P(y)$: This represents the probability of class $y$ occurring in the dataset. It can be estimated by calculating the relative frequencies of each class in the training data.\n",
    "\n",
    "2. Likelihood $P(\\mathbf{x} | y)$: This represents the probability of observing the feature vector $\\mathbf{x}$ given class $y$. In Gaussian Naive Bayes, we assume that the features are independent, given the class. Therefore, the likelihood is calculated as the product of the individual feature probabilities:\n",
    "\n",
    "   $$P(\\mathbf{x} | y) = P(x_1 | y) \\cdot P(x_2 | y) \\cdot \\ldots \\cdot P(x_n | y)$$\n",
    "\n",
    "   We can substitute the Gaussian PDF for each feature $P(x_i | y)$ to compute the likelihood.\n",
    "\n",
    "3. Evidence Probability $P(\\mathbf{x})$: This represents the probability of observing the feature vector $\\mathbf{x}$ in the dataset. It acts as a normalization factor and ensures that the posterior probabilities sum up to 1. However, in the context of classification, this term is not necessary because we only need to compare the posterior probabilities for different classes.\n",
    "\n",
    "To classify a new instance $\\mathbf{x}'$, we calculate the posterior probability $P(y | \\mathbf{x}')$ for each class $y$. We select the class with the highest posterior probability as the predicted class for $\\mathbf{x}'$.\n",
    "\n",
    "It's worth noting that Gaussian Naive Bayes assumes independence between features, which means that the presence or absence of one feature does not affect the presence or absence of other features. This assumption simplifies the computation and can work well when the features are not strongly correlated. However, if the independence assumption is violated, it may lead to suboptimal results.\n",
    "\n",
    "In practice, to apply Gaussian Naive Bayes, you can use libraries such as scikit-learn in Python, which provide ready-to-use implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b70c4cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Perform prediction on the test data\n",
    "y_pred = naive_bayes.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f1978",
   "metadata": {},
   "source": [
    "# `Bernoulli Naive Bayes`   \n",
    "Bernoulli Naive Bayes is a variant of the Naive Bayes algorithm that assumes binary features (i.e., features that take only two possible values, typically 0 and 1). \n",
    "**It assumes that all our features are binary such that they take only two values. Means 0s can represent “word does not occur in the document” and 1s as \"word occurs in the document**\n",
    "  \n",
    "Bernoulli Naive Bayes is a variant of the Naive Bayes algorithm that is specifically designed for binary (two-class) classification problems. It assumes that the features (input variables) are binary, meaning they can take on only two values, typically represented as 0 and 1.\n",
    "\n",
    "Bernoulli Naive Bayes is commonly used in text classification tasks, such as sentiment analysis, spam detection, and document categorization. It works well with text data represented as binary feature vectors, where each feature represents the presence or absence of a specific word or term in a document.\n",
    "\n",
    "Here are a few scenarios where Bernoulli Naive Bayes can be particularly useful:\n",
    "\n",
    "1. **Text Classification**: When classifying text data, Bernoulli Naive Bayes can be effective. It assumes that the presence or absence of certain words or features is indicative of the class label. For example, in sentiment analysis, the presence of positive words in a document might indicate a positive sentiment, while the absence of those words might indicate a negative sentiment.\n",
    "\n",
    "2. **Binary Feature Data**: Bernoulli Naive Bayes is well-suited for problems where the features are binary, representing yes/no, true/false, or presence/absence. For instance, in document classification, each feature may represent whether a specific word occurs in a document or not.\n",
    "\n",
    "3. **Sparse Feature Representation**: If the dataset has a large number of features and most of them are zero (sparse representation), Bernoulli Naive Bayes can handle this efficiently. It leverages the sparsity of the data to make predictions effectively.\n",
    "\n",
    "It's important to note that Bernoulli Naive Bayes assumes independence between the features given the class, which is a simplifying assumption. In situations where the features have dependencies, such as the order of words in a sentence or the co-occurrence of certain words, other variants of Naive Bayes (e.g., Multinomial Naive Bayes or Gaussian Naive Bayes) may be more appropriate.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is specifically used for binary classification problems where the features are binary and is commonly applied to text classification tasks.\n",
    "Let's consider a classification problem with a feature vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ where each $x_i$ represents a binary feature, and a target class variable $y$ that can take on one of $C$ possible classes.\n",
    "\n",
    "The goal is to determine the posterior probability $P(y | \\mathbf{x})$, which represents the probability of class $y$ given the observed features $\\mathbf{x}$.\n",
    "\n",
    "According to Bayes' theorem, the posterior probability can be calculated as:\n",
    "\n",
    "$$P(y | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y) \\cdot P(y)}{P(\\mathbf{x})}$$\n",
    "\n",
    "where:\n",
    "- $P(y | \\mathbf{x})$ is the posterior probability (probability of class $y$ given the features $\\mathbf{x}$).\n",
    "- $P(\\mathbf{x} | y)$ is the likelihood (probability of observing features \\(\\mathbf{x}\\) given class \\(y\\)).\n",
    "- $P(y)$ is the prior probability (probability of class $y$ occurring).\n",
    "- $P(\\mathbf{x})$ is the evidence probability (probability of observing the features $\\mathbf{x}$).\n",
    "\n",
    "In Bernoulli Naive Bayes, we assume that the features follow a Bernoulli distribution, which means they are binary (0 or 1). Mathematically, it can be written as:\n",
    "\n",
    "$$P(x_i | y) = \\theta_{y,i}^{x_i} \\cdot (1 - \\theta_{y,i})^{(1 - x_i)}$$\n",
    "\n",
    "where:\n",
    "- $P(x_i | y)$ is the probability of feature $x_i$ occurring given class $y$.\n",
    "- $\\theta_{y,i}$ is the parameter representing the probability of feature $x_i$ being 1 given class $y$.\n",
    "\n",
    "To classify a new instance $\\mathbf{x}'$, we calculate the posterior probability for each class and select the class with the highest probability.\n",
    "\n",
    "In practice, the prior probabilities $P(y)$ and the parameters $\\theta_{y,i}$ can be estimated from the training data using maximum likelihood estimation or other techniques.\n",
    "\n",
    "Bernoulli Naive Bayes is particularly suitable for text classification tasks, where the presence or absence of words (binary features) in a document can be used as features.\n",
    "\n",
    "Note that Bernoulli Naive Bayes assumes independence between features, similar to other Naive Bayes variants. This assumption simplifies the computation and can work well when the features are relatively independent. However, it may not be suitable for features that exhibit strong dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dff53137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7906186989281545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fetch the 20newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
    "\n",
    "# Extract the features and labels from the dataset\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "# Create a CountVectorizer for feature extraction\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Create a Bernoulli Naive Bayes classifier\n",
    "naive_bayes = BernoulliNB()\n",
    "\n",
    "# Fit the classifier to the data\n",
    "naive_bayes.fit(X, y)\n",
    "\n",
    "# Perform prediction on the training data\n",
    "y_pred = naive_bayes.predict(X)\n",
    "\n",
    "# Calculate accuracy on the training data\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cbd801d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [3, 2], [6, 4], [5, 3], [7, 2]])  # Feature values\n",
    "y = np.array([0, 0, 0, 1, 1, 1])  # Group labels\n",
    "\n",
    "# Perform Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "\n",
    "# Calculate the discriminant scores for each data point\n",
    "disc_scores = lda.transform(X)\n",
    "\n",
    "# Plotting the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plotting the decision boundary\n",
    "x_min, x_max = plt.xlim()\n",
    "y_min, y_max = plt.ylim()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, colors='black', linewidths=0.5)\n",
    "\n",
    "plt.title('Discriminant Analysis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e783ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
