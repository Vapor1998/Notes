{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9BoKnQnM3oU"
   },
   "source": [
    "#**Convulation Neural Networks (CNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IH39p4wHW-8"
   },
   "source": [
    "**What will you learn?**\n",
    "1. **Introduction** : Problems with image classification and solution.\n",
    "2. **Why CNN?**\n",
    "3. **Stride and Padding**\n",
    "4. **Channels**\n",
    "5. **Important CNN Layers** : Convulational, ReLU, Pooling, Output Layer.\n",
    "6. **Summary** : How all layers work together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtknRtyyCsJJ"
   },
   "source": [
    "##**Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem in Handling Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os1E4OviCvFp"
   },
   "source": [
    "The problem with classifying images is that, we cannot treat each pixel as a **feature**. **This is because some pixels combined together generate patterns which are not recognized if treated separately. So we need some new way to overcome this problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTJs3uxlM3rO"
   },
   "source": [
    "Yann LeCun, director of Facebook’s AI Research Group, is the pioneer of Convolutional Neural Networks. He built the first convolutional neural network called LeNet in 1988. LeNet was used for character recognition tasks like reading zip codes and digits.\n",
    "\n",
    "Have you ever wondered how facial recognition works on social media, or how object detection helps in building self-driving cars, or how disease detection is done using visual imagery in healthcare? It’s all possible thanks to convolutional neural networks (CNN). Here’s an example of convolutional neural networks that illustrates how they work:\n",
    "\n",
    "Imagine there’s an image of a bird, and you want to identify whether it’s really a bird or some other object. The first thing you do is feed the pixels of the image in the form of arrays to the input layer of the neural network (multi-layer networks used to classify things). The hidden layers carry out feature extraction by performing different calculations and manipulations. There are multiple hidden layers like the convolution layer, the ReLU layer, and pooling layer, that perform feature extraction from the image. Finally, there’s a fully connected layer that identifies the object in the image.\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/convolutional_neural_network_to_identify_the_image_of_a_bird-7737.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeUKXzOQPsSG"
   },
   "source": [
    "In machine learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks, most commonly applied to analyzing visual imagery.\n",
    "\n",
    "CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.\n",
    "  \n",
    "**CNN will give you a technique using which we will find what should be the weights using which we can combine nearby pixels into one feature so that we are able to capture interesting parts of the images.**\n",
    "\n",
    "Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n",
    "\n",
    "CNNs use relatively little pre-processing compared to other image classification algorithms. **This means that the network learns the filters that in traditional algorithms were hand-engineered.Idea behind filter is that we will have lots of filter where each filter will recognize different patterns in the image.** This independence from prior knowledge and human effort in feature design is a major advantage. They have applications in image and video recognition, recommender systems and natural language processing.\n",
    "\n",
    "A CNN consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers.\n",
    "\n",
    "Description of the process as a convolution in neural networks is by convention. Mathematically it is a cross-correlation rather than a convolution. This only has significance for the indices in the matrix, and thus which weights are placed at which index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfgaez7kWDx-"
   },
   "source": [
    "##**Why should we use CNN ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolution Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say we have a CNN unit, lets say we pass an image through this, it should do some changes in the images, maybe generate a new image, not generally an image but transformation of an image which somehow captures not just individual pixels, but combination of nearby pixels.   \n",
    "  \n",
    "Output will be a 2D array may or maynot be of same size square like metric which might or might not look like image we have, but it should apply some kind of transformation in the original image by combining the nearby pixel value.   \n",
    "\n",
    "So the way CNN apply these transformation is basically every unit will have a filter.\n",
    "Filter is nothing but some k x k in case of square or we can make it rectangle of k x m metric and we will pass this filter over the original image.   \n",
    "\n",
    "Lets say the filter have weights $w_{00}$, $w_{01}$, $w_{02}$, $w_{10}$ and so on upto $w_{kk}$. Lets say it is 3 x 3.  \n",
    "So we will pass this filter over this image. Then we will decide whether will shift it by 1, 2 or 3. But when we put this filter over some pixels, we will get a value output from this.  \n",
    "Lets say we put the filter on the first part of image. Then basically on $P_{ij}$, we will multiply with $W_{ij}$ \n",
    "$$ \\sum_{j = 0}^{k -1} \\sum_{i = 0}^{k-1}P_{ij}W_{ij}$$ \n",
    "\n",
    "Thats what happen on the first part of the image. So if we move to the next part of the image, value of $P_{ij}$ might not be the same it will be some other pixels. This summation will give us a number. That number we will put it on the first part of output image.  \n",
    "  \n",
    "So our images might have a lot of shapes such as curves, lines etc. So we will have a lots of CNN units and the idea is some of this units will start learning the curves some of units might learn curves and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9T6XlszWFkH"
   },
   "source": [
    "**Problem with Feedforward Neural Network**\n",
    "\n",
    "Suppose you are working with MNIST dataset, you know each image in MNIST is 28 x 28 x 1(black & white image contains only 1 channel). Total number of neurons in input layer will 28 x 28 = 784, this can be manageable. What if the size of image is 1000 x 1000 which means you need 10⁶ neurons in input layer. Oh! This seems a huge number of neurons are required for operation. It is computationally ineffective right. So here comes Convolutional Neural Network or CNN. In simple word what CNN does is, it extract the feature of image and convert it into lower dimension without loosing its characteristics. In the following example you can see that initial the size of the image is 224 x 224 x 3. If you proceed without convolution then you need 224 x 224 x 3 = 100, 352 numbers of neurons in input layer but after applying convolution you input tensor dimension is reduced to 1 x 1 x 1000. It means you only need 1000 neurons in first layer of feedforward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fts59O6NV0Vl"
   },
   "source": [
    "# 3. Stride and Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAAq9VA9WeI7"
   },
   "source": [
    "**Stride** denotes how many steps we are moving in each steps in convolution. By default it is one. Lets say we have a filter which is of 3 x 3, so maximum stride is 3 because if we move more than 3, lets say 4 and then we stride by 4, we will loose some space which is discarded by filter and is not used in training.   \n",
    "So if we increase the size of stride, our output will be smaller. If we use stride as 1, we will get size almost the size of image.  \n",
    "  \n",
    "So if we have stride of size s, we will end up reaching $\\frac{n}{s}$ where n is the size of the image. Not exactly $\\frac{n}{s}$  because lets say we have an image of size 20 x 20 and we move stride from 0, we can only move our stride till 17. \n",
    "\n",
    "<img src = \"https://files.codingninjas.in/stride-7746.gif\" width = 550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIsqIFq3Wgnb"
   },
   "source": [
    "We can observe that the size of output is smaller that input. To maintain the dimension of output as in input , we use padding.     \n",
    "  \n",
    "**Padding** is a process of adding zeros to the input matrix symmetrically. In the following example,the extra grey blocks denote the padding. It is used to make the dimension of output same as input.  \n",
    "  \n",
    "We generally use two different types of padding. Padding means lets say we have an image, as seen above if we apply filter, it will move by one block. Now if we can see, a pixel at position (0, 2) is a part of filters applied three times. Whereas the pixel at position (0, 0) is only used once. So padding is used to put some 0's around our image so that all the pixels values are used multiple times. And also we dont have any bias against any corner pixel value.  \n",
    "Also the black blocks in the images represent the 0's around the image.\n",
    "  \n",
    "In general, we use two different types of padding  \n",
    "1. **Valid** $->$ Means we will do no padding. Whatever is the bias we will live with that.  \n",
    "2. **Same** $->$ Means we will apply enough padding that *if the stride is 1, the input size = output size*. If the stride is not 1, it will be $\\frac{n}{s}$.    \n",
    "  \n",
    "Whenever we using convolutional layer, we need to decide three things.  \n",
    "1. K(size of filter)  \n",
    "2. Stride and Padding  \n",
    "\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/padding-7745.gif\" width = 550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1bkYpI3GrNQ"
   },
   "source": [
    "##**Channels**\n",
    "\n",
    "If the images are colored, it will not be just 2d, it will be 3d. We will have RGB values. So effectively we will have 3 values and layers. One layer of Red values one layer of Green values and one Layer of Blue values. We call it channels. So our image will have 3 channels.  \n",
    "  \n",
    "Lets say we get this image which is 3 layered deep. One option is we will apply this filter, 2d filter which not just traverse by height and width, but depth as well. Other option we use is we actually make our weights metrics to have some depth as well.  \n",
    "  \n",
    "So if there are 3 channels, our weight metrics will also have 3 channels. That means it will completely fit inside the image and there will be no space to move inside or along the depth. So if we use padding to be same and we use stride to be 1, So if we have a 28 x 28 x 3 sized image, and we want to use 4 x 4 x 3 to be our filter, and then if we apply it here, the output will be 28 x 28 x 1 while it goes through one single unit because we have made this filter to be of depth same as no of channels that input image has, it fits exactly as the depth and there is no way to move up or down inside the depth(means only one calculation). So the no of channels will reduce to 1. So from each unit, we will only get one output.  \n",
    "  \n",
    "Even if we dont have the rgb images, we will be using the same concept in case of multiple convolutional layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say an image comes through, we have 5 units in the first convolutional layer, if the image goes through the first unit, it creates one transformation of the image, when it goes to the second unit, it creates the another transformation(only one) and so on in all 5 units.  \n",
    "  \n",
    "These transformations we can almost treat them like channels. Now when we have the next convolutional layer, next layers unit  of the convolutional layer will take all the previous transformations as input. So for this unit, its actually getting 5 images. Its kind of getting an image if 5 channels. And the filter we will be applying in this layer will actually be whatever k we want  to define for height and width its ok, the depth we will be using will be 5 because we have 5 units in the previous channel.  \n",
    "  \n",
    "The output from this channel will again be just one transformation of the image.  \n",
    "What will happen is we have 28 x 28 x 3 sizedimages, it goes through these 5 units of first cnn layer, from which each unit will produce 28 x 28 x 1 (assuming stride to be 1 and padding to be same). Lets say the net layer has 10 units. Now each of these(output from 1st cnn layer) go in 2nd layer of cnn together(means each unit of 2nd layer will have all 5 outputs(of 28x28x1) from previous layer). So the input of each unit will be 28 x 28 x 5(coz we passed all 5 outputs). The output of these unit will still be 28 x 28 x 1, because the weights(stride) here will have the depth of 5(just like we discussed it will have no space to move up or down). So the net output out of these will be 28 x 28 x 10(As we have 10 units in this layer of cnn).  \n",
    "The is not entirely true, the size will not be like this, because we will actually add one more layer in between(apart from these 2 layers) which is called **pooling layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqBbfLCPGtHH"
   },
   "source": [
    "Let's assume that we are talking about 2D convolutions applied on images.\n",
    "\n",
    "In a grayscale image, the data is a matrix of dimensions w×h, where w is the width of the image and h is its height. In a color image, we normally have 3 channels: red, green and blue; this way, a color image can be represented as a matrix of dimensions w×h×c, where c is the number of channels, that is, 3.\n",
    "\n",
    "A convolution layer receives the image (w×h×c) as input, and generates as output an activation map of dimensions w′×h′×c′. The number of input channels in the convolution is c, while the number of output channels is c′. The filter for such a convolution is a tensor of dimensions f×f×c×c′, where f is the filter size (normally 3 or 5).\n",
    "\n",
    "This way, the number of channels is the depth of the matrices involved in the convolutions. Also, a convolution operation defines the variation in such depth by specifying input and output channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTXLaxRERYAS"
   },
   "source": [
    "##**Important CNN Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku9qCWtGQC68"
   },
   "source": [
    "###**Convolutional Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk7yAf3EQE-V"
   },
   "source": [
    "The convolutional layer can be thought of as the eyes of the CNN. The neurons in this layer look for specific features. If they find the features they are looking for, they produce a high activation.\n",
    "\n",
    "Convolution can be thought of as a weighted sum between two signals ( in terms of signal processing jargon ) or functions ( in terms of mathematics ). In image processing, to calculate convolution at a particular location $(x, y)$, we extract $k$ x $k$ sized chunk from the image centered at location $(x,y)$. We then multiply the values in this chunk (generally called **filter**) element-by-element with the convolution filter (also sized $k$ x $k$) and then add them all to obtain a single output.\n",
    "\n",
    "Although fully connected feedforward neural networks can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10000 weights for each neuron in the second layer. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters. For instance, regardless of image size, tiling regions of size 5 x 5, each with the same shared weights, requires only 25 learnable parameters. In this way, it resolves the vanishing or exploding gradients problem in training traditional multi-layer neural networks with many layers by using backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MPZ-rFeQE78"
   },
   "source": [
    "<img src = \"https://files.codingninjas.in/3test-7738.gif\">\n",
    "\n",
    "Initially we will assume the weights, and then we will ue forward and backward propogation to optimise these weights.\n",
    "\n",
    "Each unit on the layer would eventually learn some specific feature of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q68mo2nhQE5d"
   },
   "source": [
    "###**ReLU Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwpUe_P6QE3S"
   },
   "source": [
    "ReLU stands for the rectified linear unit. Once the feature maps are extracted, the next step is to move them to a ReLU layer. \n",
    "\n",
    "ReLU performs an element-wise operation and sets all the negative pixels to 0. It introduces non-linearity to the network, and the generated output is a rectified feature map. Below is the graph of a ReLU function:\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/relu_layer-7741.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ-LDfpWSaaA"
   },
   "source": [
    "The original image is scanned with multiple convolutions and ReLU layers for locating the features.\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/input_feature_map-7739.gif\" width = 550>\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/input_feature_map1-7740.gif\" width = 550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4uneeOSQE09"
   },
   "source": [
    "# 5. Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uX3mMzmCQEyp"
   },
   "source": [
    "Pooling is a down-sampling operation that reduces the dimensionality of the feature map. The rectified feature map now goes through a pooling layer to generate a pooled feature map.\n",
    "\n",
    "There are two types of Pooling: **Max Pooling and Average Pooling.**  \n",
    "  \n",
    "\n",
    "**Max Pooling** $->$ It means that we will decide a pool_size lets call it k. So we will go through k x k box we will find max out of these and output will have max value. Then we will shift our box by some value. If we shift by 1, it wont be be very useful because we will end up with same size of output image. Default value here is once we have taken max among k x k, we will move to the next box, we will shift by k, then find max from that box, then we will move by k and so on. And we will get much smaller image. Lets say we have 28 x 28 image, we will pool it 4 x 4(these 4 x 4 will end up with one single value, effectively 16 pixels will lead to one output), so we will end up with 7 x 7 image.  \n",
    "  \n",
    "If possible, we will add a cnn, then a pooling layer, then cnn layer then pooling layer.  \n",
    "  \n",
    "  \n",
    "**Max Pooling** returns the maximum value from the portion of the image covered by the Kernel. On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel.\n",
    "Max Pooling also performs as a Noise Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction. \n",
    "\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/pooling_filters-7743.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9QUIZ5sQEic"
   },
   "source": [
    "The pooling layer uses various filters to identify different parts of the image like edges, corners, body, feathers, eyes, and beak.\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/input_feature_map2-7742.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWC0lrtRXszP"
   },
   "source": [
    "\n",
    "\n",
    "**Average Pooling** simply performs dimensionality reduction as a noise suppressing mechanism. Hence, we can say that Max Pooling performs a lot better than Average Pooling.\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/maxavgpooling-7747.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzGAI-JuUcdL"
   },
   "source": [
    "###**Fully Connected Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHDK8FyoUheD"
   },
   "source": [
    "The next step in the process is called flattening. Flattening is used to convert all the resultant 2-Dimensional arrays from pooled feature maps into a single long continuous linear vector.\n",
    "\n",
    "The flattened matrix is fed as input to the **fully connected layer** to classify the image.\n",
    "\n",
    "Fully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am7xIWyNXY0B"
   },
   "source": [
    "###**Softmax / Logistic Layer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7I8pG_ZMXYxk"
   },
   "source": [
    "Softmax or Logistic layer is the last layer of CNN. It resides at the end of FC layer. Logistic is used for binary classification and softmax is for multi-classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_9l7VInXYvK"
   },
   "source": [
    "###**Output Layer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJuybTaCXiiu"
   },
   "source": [
    "Output layer contains the label which is in the form of one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9GKRMT1TnIg"
   },
   "source": [
    "##**How CNN recoganises an image?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkHkKqVkTt7A"
   },
   "source": [
    "1. The pixels from the image are fed to the convolutional layer that performs the convolution operation \n",
    "2. It results in a convolved map \n",
    "3. The convolved map is applied to a ReLU function to generate a rectified feature map \n",
    "4. The image is processed with multiple convolutions and ReLU layers for locating the features \n",
    "5. Different pooling layers with various filters are used to identify specific parts of the image \n",
    "6. The pooled feature map is flattened and fed to a fully connected layer to get the final output\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/cnn_recognizes_a_bird1-7744.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN - 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
