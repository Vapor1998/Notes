{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwJiHMiqEUV8"
   },
   "source": [
    "# **Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KVhBmWF22_w"
   },
   "source": [
    "**What will you learn?**\n",
    "1. **Introduction :** Intro to ANNs\n",
    "2. **Why do we need NN?**\n",
    "3. **Example with Linear Boundaries** : Negation, AND, OR\n",
    "3. **Example with Non-Linear Boundaries** : XOR\n",
    "5. **Terminology**\n",
    "6. **Propogation**\n",
    "7. **Cost Function**\n",
    "8. **Multiclass Classification** : One Hot Encoding\n",
    "9. **Sklearn Implementation** : MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klI5B815EUS7"
   },
   "source": [
    "##**Introdution**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcYZlGtYEUQa"
   },
   "source": [
    "Artificial Neural Networks are one of the main tools used in machine learning. As the “neural” part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns which are far too complex or numerous for a human programmer to extract and teach the machine to recognize.\n",
    "\n",
    "For a basic idea of how a deep learning neural network learns, imagine a factory line. After the raw materials (the data set) are input, they are then passed down the conveyer belt, with each subsequent stop or layer extracting a different set of high-level features. If the network is intended to recognize an object, the first layer might analyze the brightness of its pixels.\n",
    "\n",
    "The next layer could then identify any edges in the image, based on lines of similar pixels. After this, another layer may recognize textures and shapes, and so on. By the time the fourth or fifth layer is reached, the deep learning net will have created complex feature detectors. It can figure out that certain image elements (such as a pair of eyes, a nose, and a mouth) are commonly found together.\n",
    "\n",
    "Once this is done, the researchers who have trained the network can give labels to the output, and then use backpropagation to correct any mistakes which have been made. After a while, the network can carry out its own classification tasks without needing humans to help every time.\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/3nn-7659.gif\" width = 800>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUitQBD_EUOH"
   },
   "source": [
    "##**Why do we need NN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GVwtToUEUL2"
   },
   "source": [
    "Neural Networks have been around even before machine learning gained pace. But they were thought to be computationally too heavy and hence, brushed aside.\n",
    "\n",
    "A problem we faced during Logistic Regression was that, even though the decision function (Sigmoid) was non linear, we got a linear decision boundary. We fixed this problem by adding dummy data with higher powers.\n",
    "\n",
    "To do that, we had to experiment and decide the degree of features we needed to add. Our decision boundary shoud be such, that it performs this task on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cs3bFPdEUJj"
   },
   "source": [
    "Logistic regression had the following structure:\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/nn1-7661.jpg\" width = 500>\n",
    "\n",
    "The intuition behind Neural Networks is a follows:\n",
    "\n",
    "<img src = \"\thttps://files.codingninjas.in/nn2-7662.jpg\" width = 500>\n",
    "\n",
    "So, here the final output will not be linear with respect to $x_1, x_2, x_0$.\n",
    "The functions $f_1, f_2$ need not necessarily be Sigmoid. We can choose any function. Using this method we can create quite interesting decision boundaries without applying the dummy feature method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYhfUp19EUHJ"
   },
   "source": [
    "##**Example with Linear Decision Boundaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tufpDdojEUEt"
   },
   "source": [
    "To understand how to reach the boundaries, lets take a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYPUnaBlEUCO"
   },
   "source": [
    "###**Example 1 : Negation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aal8ton6ET_q"
   },
   "source": [
    "x | y | \n",
    ":---:|:---:|\n",
    "1|0|\n",
    "0|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekTFRxne3p1D"
   },
   "source": [
    "<img src = \"https://files.codingninjas.in/negation-7688.jpg\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3WCQyWIET9S"
   },
   "source": [
    "Here, the function used is \n",
    "$$\\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "We want to pick the correct values of $w_0$ and $w_1$ so that we reach to the correct answer.\n",
    "\n",
    "**Case 1** : When $x = 0$, we want $y = 1$. So, we want $z\\geq0$.\n",
    "$$w_0 + w_1x \\geq 0$$\n",
    "$$w_0 \\geq 0$$\n",
    "\n",
    "This means we need to keep $w_0$ at a high value, so sigmoid function closely reaches 1. Lets take $w_0 = 50$.\n",
    "\n",
    "Hence, $z = 50$, which is what we wanted.\n",
    "\n",
    "**Case 2** : When $x = 1$, we want $y = 0$. So, we want $z\\geq0$.\n",
    "$$w_0 + w_1x \\leq 0$$\n",
    "$$w_0 + w_1 \\leq 0$$\n",
    "\n",
    "Lets take $w_1$ = -100\n",
    "\n",
    "Hence, $z = -50$, which is what we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0SF7mgmET66"
   },
   "source": [
    "###**Example 2 : OR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQS2t_LT4I8S"
   },
   "source": [
    "<img src = \"https://files.codingninjas.in/or-7690.jpg\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKbiStsbET4l"
   },
   "source": [
    "$x_1$ | $x_2$ | $y$ \n",
    ":---:|:---:|:---:|\n",
    "0|0|0\n",
    "1|0|1\n",
    "0|1|1\n",
    "1|1|1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iJSHtchET2C"
   },
   "source": [
    "Here, the function used is \n",
    "$$\\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "We want to pick the correct values of $w_0$ and $w_1$ so that we reach to the correct answer.\n",
    "\n",
    "**Case 1** : When $x_1 = 0$ and $x_2 = 0$, we want $y = 0$.\n",
    "\n",
    "Therefore,\n",
    "$$ w_1x_1 + w_2x_2 + w_0(1) < 0 $$\n",
    "$$ w_1(0) + w_2(0) + w_0 < 0 $$\n",
    "$$ w_0 < 0 $$\n",
    "\n",
    "Lets take $w_0 = -50$\n",
    "\n",
    "**Case 2** : When $x_1 = 1$ and $x_2 = 0$, we want $y = 1$.\n",
    "\n",
    "Therefore,\n",
    "$$ w_1x_1 + w_2x_2 + w_0(1) > 0 $$\n",
    "$$ w_1(1) + w_2(0) + w_0 > 0 $$\n",
    "$$ w_0 + w_1 > 0 $$\n",
    "\n",
    "Lets take $w_1 = 100$\n",
    "\n",
    "**Case 3** : When $x_1 = 0$ and $x_2 = 1$, we want $y = 1$.\n",
    "\n",
    "Therefore,\n",
    "$$ w_1x_1 + w_2x_2 + w_0(1) > 0 $$\n",
    "$$ w_1(0) + w_2(1) + w_0 > 0 $$\n",
    "$$ w_0 + w_2 > 0 $$\n",
    "\n",
    "Lets take $w_2 = 100$\n",
    "\n",
    "So, we can draw the table as :\n",
    "\n",
    "$x_1$ | $x_2$ | $y$ | $z$ | $y_p$\n",
    ":---:|:---:|:---:|:---:|:---:|\n",
    "0|0|0|-50|0\n",
    "1|0|1|50|1\n",
    "0|1|1|50|1\n",
    "1|1|1|150|1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yj9ZK9E6ETzr"
   },
   "source": [
    "Try doing the calculations for **AND** Gate\n",
    "\n",
    "$x_1$ | $x_2$ | $y$ \n",
    ":---:|:---:|:---:|\n",
    "0|0|0\n",
    "1|0|0\n",
    "0|1|0\n",
    "1|1|1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYPsAqM4ETxU"
   },
   "source": [
    "##**Example with Non Linear Decision Boundaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2ghrW2AETvB"
   },
   "source": [
    "###**Example 1 : XOR**\n",
    "\n",
    "$x_1$ | $x_2$ | XOR | AND | NOR\n",
    ":---:|:---:|:---:|:---:|:---:|\n",
    "0|0|0|0|1\n",
    "1|0|1|0|0\n",
    "0|1|1|0|0\n",
    "1|1|0|1|0\n",
    "\n",
    "If we look at the table closely, when outputs of AND and NOR are 0, XOR is 1.\n",
    "If any of AND and NOR is 1, output of XOR is 0.\n",
    "\n",
    "So we can combine AND and NOR to reach XOR.\n",
    "Taking AND to be $f_1$ and NOR to be $f_2$ we can say that NOR($f_1$, $f_2$) will give the desired output.\n",
    "\n",
    "<img src = \"\thttps://files.codingninjas.in/xor-7691.jpg\">\n",
    "\n",
    "Verify the results with your own calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GxhGeJKuQds"
   },
   "source": [
    "##**Terminology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rrtbsPKuQbR"
   },
   "source": [
    "**Neuron** : A single unit in any layer is called neuron.\n",
    "\n",
    "**Input Layer** : The Input layer communicates with the external environment that presents a pattern to the neural network. Its job is to deal with all the inputs only.The input layer should represent the condition for which we are training the neural network. Every input neuron should represent some independent variable that has an influence over the output of the neural network.\n",
    "\n",
    "**Hidden Layer** : The hidden layer is the collection of neurons which has activation function applied on it and it is an intermediate layer found between the input layer and the output layer. Its job is to process the inputs obtained by its previous layer. So it is the layer which is responsible extracting the required features from the input data.\n",
    "\n",
    "\n",
    "**Output Layer** : The output layer of the neural network collects and transmits the information accordingly in way it has been designed to give. The pattern presented by the output layer can be directly traced back to the input layer. The number of neurons in output layer should be directly related to the type of work that the neural network was performing.\n",
    "\n",
    "\n",
    "Weights for each neuron will be found using some algorithm.\n",
    "Lets say we have jth layer and j+1th layer, units in $U_j$ layer and $U_{j+1}$ layer is $(U_j + 1) * U_{j+1}$ where + 1 is the bias term. This is the no of parmeters we need to train between two layers.  \n",
    "Because we have so many parameters, that can lead to overfitting in case of Neural Networks, which we need to take care of.  \n",
    "Having so many parameters to train upon can lead to overfitted model as well.    \n",
    "    \n",
    "    \n",
    "What we need to decide is :\n",
    "1. How many hidden layers we want?\n",
    "2. How many neurons in each layer?\n",
    "3. Function to be applied over hidden and output layer.\n",
    "\n",
    "\n",
    "<img src = \"\thttps://files.codingninjas.in/network-7689.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAUccCwNuQYs"
   },
   "source": [
    "##**Propogation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgMv4m2MuQWC"
   },
   "source": [
    "###**Forward Propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZsOXPtAuQTE"
   },
   "source": [
    "The input X provides the initial information that then propagates to the hidden units at each layer and finally produce the output y^.This is called Forward Propogation. The architecture of the network entails determining its depth, width, and activation functions used on each layer. Depth is the number of hidden layers. Width is the number of units (nodes) on each hidden layer since we don’t control neither input layer nor output layer dimensions. There are quite a few set of activation functions such Rectified Linear Unit, Sigmoid, Hyperbolic tangent, etc. Research has proven that deeper networks outperform networks with more hidden units. Therefore, it’s always better and won’t hurt to train a deeper network (with diminishing returns).  \n",
    "So the actual error that we calculate will be on the last output layer thats where we get to know where the error is.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI1U2vy6uQQV"
   },
   "source": [
    "###**Backward Propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twtjzRoAuQM3"
   },
   "source": [
    "Backpropagation refers to the method of calculating the gradient of neural network parameters. In short, the method traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus. The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters.  \n",
    "To update weights or to push error back through the layers we use Backward Propogation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcauU6eZuQKK"
   },
   "source": [
    "##**Cost Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjnSOq5zuQHh"
   },
   "source": [
    "For neural networks, or any other algorithm for that matter, the cost function is similar. Here, \n",
    "\n",
    "$$ Cost = Error + \\lambda Regularisation$$\n",
    "\n",
    "For regularisation, we will use $\\sum w_j^2$, which is sum of all the weights squared.\n",
    "\n",
    "Now, error function can be :\n",
    "\n",
    "$$ (y_t - y_{pred})^2 $$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$ Cost = \\frac{1}{m}\\sum(y_t - y_{pred})^2 + \\frac{\\lambda}{2m} \\sum w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RedvC5J3uQE1"
   },
   "source": [
    "##**How to handle Multiclass Classification**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iALK3tv7uQB9"
   },
   "source": [
    "To handle this, basic idea is to add additional weights. All remains same, but at output layer also, multiple units are added.\n",
    "\n",
    "Lets assume that there are 3 classes that can be predicted.\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/multiclass-7687.png\">\n",
    "\n",
    "Lets say the values predicted are:\n",
    "$y_1 = 0.1$, $y_2 = 0.15$ and  $y_3 = 0.99$.\n",
    "\n",
    "We will say that the data points belong to the max value class, in this case Class 3. The true value of the output will be in the form of a vector like [0, 0, 1].\n",
    "\n",
    "Now, for above model, data points will also be in form of a vector, as is output.\n",
    "\n",
    "If datapoint $x^1$ belongs to the 1st class, then its input vector is [1, 0, 0].\n",
    "\n",
    "\n",
    "Similarily, if $x^2$ belongs to the 3rd class, then its input vector is [0, 0, 1].\n",
    "\n",
    "Such an input is called **One Hot Encoded** input.\n",
    "\n",
    "\n",
    "Cost function changes to :\n",
    "\n",
    "$$Cost = \\sum^m_{i = 1} \\sum^k_{j = 1} f(y_{i}^j(pred),\\enspace y_i^j(true)) + \\frac{\\lambda}{2m} \\sum w_j^2 $$\n",
    "\n",
    "This extra summation $\\sum^k_{j = 1}$ penalises us if one hot encoding is incorrect. Hence, error and cost are not just to be calculated for correct units, but also for incorrect prediction of other units. So lets say we have to predict [0,1,0] for the class and we predicted [0.8, 1, 0.1] for the class, it will penalize it for the 0.8 and 0.1 because it will penalize it for all the units.\n",
    "\n",
    "So first summation sign is for the no of **Datapoints which are m** while the second summation is for the **Units in output layer which is k** So lets say we have 3 units in output layer, k will be 3. \n",
    "\n",
    "  \n",
    "##### Explaination of the Multiclass   \n",
    "Idea behind multiclass classification is so far we were using **one vs one or one vs rest** for Multiclss classification.  \n",
    "Which means we will train 10 different models for 10 different classes.  \n",
    "Thats not something we will be doing for neural networks,  \n",
    "Lets take an example of Normal Neural Network with one output layer.  \n",
    "Only thing that will change in Multiclass Classification is **it will have Multiple Output Layers**.  \n",
    "**So for Binary classes, we will only have one Output Layer. For more than two classes we will have n output layers for n classes.  \n",
    "For eg. For 3 classes, we will have 3 output layers.And these three units will produce binary data which is in the form of One Hot Encoding.**  \n",
    "So the first change will be  \n",
    "1.  Change the Training Data Y to be **One Hot Encoded**.  \n",
    "    In case we are using any Algorithm from the Library, we dont have to do One Hot Encoding Manually, it will be done automatically.  \n",
    "    If we are Implementing by ourself, we have to do it manually.  \n",
    "   \n",
    "2.  To Predict- Predict class for which Ypred Value is Maximum.  \n",
    "    Lets say we have 3 different class, $c_{1}$, $c_{2}$, $c_{3}$, whichever gives higher output, we will pich that to be Ypredicted class.  \n",
    "   \n",
    "   \n",
    "**So lets say we have 2 classes, we will have only one unit in output layer.  \n",
    "If we have 3 classes, we will have 3 units in output layer, if we have 4 classes, we will have 4 units and so on for n classes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHZL753f0oWZ"
   },
   "source": [
    "##**MLP Classifier in Sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3y0-4fxE0oTn"
   },
   "source": [
    "The MLP classifier is not a very efficient classifier. It is not advised to use in implementaion of neural networks of large data or an actual product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7fB4M_x1VnT"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5136,
     "status": "ok",
     "timestamp": 1611600021301,
     "user": {
      "displayName": "Gaurav Bhatia",
      "photoUrl": "",
      "userId": "05517600112429710610"
     },
     "user_tz": -330
    },
    "id": "gNQtJuFa1Vk5",
    "outputId": "caea82e7-f5be-446b-ef22-e710648a5aff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf  =  MLPClassifier()  # Creating object \n",
    "iris = datasets.load_iris()  # Loading dataset\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y) # here ytrain is not onehotencoded, classifier will take care of it.\n",
    "clf.fit(xtrain, ytrain)  # Training neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above warning states that the no of iterations has reached to its max value.  \n",
    "We can also change that by changing max_iter value.  \n",
    "It says it runs for 200 times and it has not converged yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5007,
     "status": "ok",
     "timestamp": 1611600021302,
     "user": {
      "displayName": "Gaurav Bhatia",
      "photoUrl": "",
      "userId": "05517600112429710610"
     },
     "user_tz": -330
    },
    "id": "hUMGOWl_1VBL",
    "outputId": "57ca6d35-8dd4-44f9-e5fe-ea1f81ea679b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(xtest,ytest) # Obtaining score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4548,
     "status": "ok",
     "timestamp": 1611600021303,
     "user": {
      "displayName": "Gaurav Bhatia",
      "photoUrl": "",
      "userId": "05517600112429710610"
     },
     "user_tz": -330
    },
    "id": "uh_ESEQ61U-5",
    "outputId": "fe9dcf2d-e3b2-4731-a889-65b8e346c2da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2, 0, 1, 2, 0, 2, 2, 2, 1, 1, 2, 2, 2, 0, 2, 2, 1, 0, 0,\n",
       "       0, 1, 1, 2, 0, 2, 2, 1, 0, 0, 2, 2, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(xtest)   # results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apN3R8d40oQs"
   },
   "source": [
    "###**Important Parameters**  \n",
    "**hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)**\n",
    "The ith element represents the number of neurons in the ith hidden layer.  \n",
    "We can change that. (100, 200) means we need two hidden layers, one with 100 units, second with 200 units.  \n",
    "(100,20,30) means we need three hidden layers one with 100 units, second with 20 units and third with 30 units.  \n",
    "\n",
    "**activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’**\n",
    "Activation function for the hidden layer. It decides which function does the layers use.  \n",
    "\n",
    "‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "\n",
    "‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "\n",
    "‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "\n",
    "‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n",
    "\n",
    "**batch_size : int, default=’auto’**\n",
    "Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples)  \n",
    "  \n",
    "**alpha : default = 0.0001**  \n",
    "Alpha is the Regularization Factor.  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq7e8Pr-0oN_"
   },
   "source": [
    "###**Important Attributes**\n",
    "**coefs_ : list of shape (n_layers - 1,)**\n",
    "The ith element in the list represents the weight matrix corresponding to layer i.\n",
    "\n",
    "**intercepts_ : list of shape (n_layers - 1,)**\n",
    "The ith element in the list represents the bias vector corresponding to layer i + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf  =  MLPClassifier()  # Creating object \n",
    "iris = datasets.load_iris()  # Loading dataset\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y) # here ytrain is not onehotencoded, classifier will take care of it.\n",
    "clf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(xtest,ytest) # Obtaining score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing the hidden layer sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20,), max_iter=3000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf  =  MLPClassifier(hidden_layer_sizes = (20, ), max_iter = 3000)\n",
    "clf.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It didnt gave the warning means *it has converged*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(xtest, ytest)\n",
    "# score has improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we want, we can look at the individual weights on each of the layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-5.16135766e-11, -2.92928568e-03, -8.75015148e-06,\n",
       "          9.35296053e-09, -1.08713292e-01,  2.37676333e-01,\n",
       "          3.79424530e-01, -3.71204104e-23, -4.42015291e-02,\n",
       "          4.34246834e-01, -9.86625363e-03, -4.62814074e-04,\n",
       "          1.82839451e-01, -6.60680161e-11, -6.72082540e-01,\n",
       "          6.07259913e-01,  3.68909660e-01,  3.95995222e-02,\n",
       "          5.18347800e-01, -1.23804011e-01],\n",
       "        [ 3.71739791e-07, -9.87218726e-20, -8.33757358e-04,\n",
       "         -3.45959848e-04, -6.40156652e-03, -2.93585314e-01,\n",
       "         -5.61521595e-02, -1.24305679e-02,  2.70458412e-01,\n",
       "         -6.81375482e-01,  1.48380950e-09, -7.22315157e-08,\n",
       "         -2.38304082e-01, -6.69055568e-03, -6.54957764e-01,\n",
       "         -5.46062589e-01,  4.15171752e-01,  5.50467722e-01,\n",
       "          9.08373849e-01, -3.30173859e-02],\n",
       "        [-1.65612981e-24, -5.81158558e-06,  3.36151269e-08,\n",
       "          1.34097253e-22, -1.79555883e-02,  3.75007087e-01,\n",
       "         -4.28293375e-01, -1.89556690e-04,  3.90786649e-01,\n",
       "          8.07819229e-01, -1.60834739e-03, -6.79508254e-26,\n",
       "          3.39513528e-01, -4.44337564e-05,  1.53264653e+00,\n",
       "          9.32456346e-02, -1.78166260e-01, -3.44717583e-01,\n",
       "         -9.09215950e-01,  3.05685628e-01],\n",
       "        [-5.38355337e-04, -6.13015497e-27,  8.89381379e-10,\n",
       "         -6.98629330e-03, -1.27321982e-03, -1.16822694e-01,\n",
       "         -1.69576079e-01, -3.38113328e-05,  3.98090694e-01,\n",
       "          1.96363906e-01, -7.04305683e-21, -6.09219461e-05,\n",
       "          5.89279213e-01,  5.98712280e-27,  8.19658289e-01,\n",
       "         -1.72836640e-01, -1.70788107e-01,  5.31872945e-01,\n",
       "         -8.72887325e-01, -4.27209783e-01]]),\n",
       " array([[-5.57727724e-22,  3.40115630e-03,  3.96077682e-05],\n",
       "        [-4.85911270e-04,  2.44425376e-05,  7.60549827e-04],\n",
       "        [-6.52802254e-03, -4.30101310e-22,  1.12110141e-03],\n",
       "        [-3.98186018e-03, -2.87951396e-21,  1.21633750e-02],\n",
       "        [-5.84295019e-03, -7.28819996e-07, -1.48241207e-02],\n",
       "        [ 1.56334789e-01,  3.99762717e-01,  7.63666306e-02],\n",
       "        [ 8.67211659e-01,  3.98359346e-01, -2.43558019e-02],\n",
       "        [-1.04995153e-02, -5.57387124e-08,  9.44009110e-11],\n",
       "        [-3.32095083e-01, -9.48146530e-02, -2.73760242e-01],\n",
       "        [-6.46875575e-01,  4.59593643e-01,  2.99962525e-01],\n",
       "        [-5.41426874e-04, -5.71670550e-04,  5.46325084e-04],\n",
       "        [ 1.45975020e-02,  1.06426764e-02,  4.74789788e-03],\n",
       "        [ 6.97289034e-02, -3.52238983e-02,  6.08135793e-01],\n",
       "        [ 2.07819409e-05,  1.49174870e-02,  1.73510423e-03],\n",
       "        [-6.64250815e-02, -1.35833479e+00,  9.55662717e-01],\n",
       "        [ 9.53558930e-03,  6.41756426e-01,  3.04368022e-01],\n",
       "        [ 4.91022511e-01,  7.82723825e-02, -4.02384808e-01],\n",
       "        [ 3.43269927e-01, -5.27356620e-01,  9.98228289e-02],\n",
       "        [ 5.86049189e-01, -2.66322385e-01, -1.34819846e+00],\n",
       "        [-1.17088686e-02,  6.90859050e-04, -3.73491007e-02]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does our network look?  \n",
    "Now we have 4 features and 1 bias in it.  \n",
    "Then we gave 20 hidden layers.  \n",
    "As we have 3 different classes on y, outer layer has 3 nodes.  \n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "This coefs does not include biases. The biases are separate.  \n",
    "So it does not include connections with the bias term.  \n",
    "Lets look at the size of it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clf.coefs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we have one set of coefficient of Hidden layer and one set of coefficient of output layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.16135766e-11, -2.92928568e-03, -8.75015148e-06,\n",
       "         9.35296053e-09, -1.08713292e-01,  2.37676333e-01,\n",
       "         3.79424530e-01, -3.71204104e-23, -4.42015291e-02,\n",
       "         4.34246834e-01, -9.86625363e-03, -4.62814074e-04,\n",
       "         1.82839451e-01, -6.60680161e-11, -6.72082540e-01,\n",
       "         6.07259913e-01,  3.68909660e-01,  3.95995222e-02,\n",
       "         5.18347800e-01, -1.23804011e-01],\n",
       "       [ 3.71739791e-07, -9.87218726e-20, -8.33757358e-04,\n",
       "        -3.45959848e-04, -6.40156652e-03, -2.93585314e-01,\n",
       "        -5.61521595e-02, -1.24305679e-02,  2.70458412e-01,\n",
       "        -6.81375482e-01,  1.48380950e-09, -7.22315157e-08,\n",
       "        -2.38304082e-01, -6.69055568e-03, -6.54957764e-01,\n",
       "        -5.46062589e-01,  4.15171752e-01,  5.50467722e-01,\n",
       "         9.08373849e-01, -3.30173859e-02],\n",
       "       [-1.65612981e-24, -5.81158558e-06,  3.36151269e-08,\n",
       "         1.34097253e-22, -1.79555883e-02,  3.75007087e-01,\n",
       "        -4.28293375e-01, -1.89556690e-04,  3.90786649e-01,\n",
       "         8.07819229e-01, -1.60834739e-03, -6.79508254e-26,\n",
       "         3.39513528e-01, -4.44337564e-05,  1.53264653e+00,\n",
       "         9.32456346e-02, -1.78166260e-01, -3.44717583e-01,\n",
       "        -9.09215950e-01,  3.05685628e-01],\n",
       "       [-5.38355337e-04, -6.13015497e-27,  8.89381379e-10,\n",
       "        -6.98629330e-03, -1.27321982e-03, -1.16822694e-01,\n",
       "        -1.69576079e-01, -3.38113328e-05,  3.98090694e-01,\n",
       "         1.96363906e-01, -7.04305683e-21, -6.09219461e-05,\n",
       "         5.89279213e-01,  5.98712280e-27,  8.19658289e-01,\n",
       "        -1.72836640e-01, -1.70788107e-01,  5.31872945e-01,\n",
       "        -8.72887325e-01, -4.27209783e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a 2d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats what we are hoping for.  \n",
    "We have 4 features and we have 20 hidden layer units, so the no of features we have is 4 * 20.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats exactly what we hoping for.  \n",
    "We have 20 hidden layers and 3 output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can look at our biases which are the intercepts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-3.97568818e-01, -6.31740630e-02,  8.44952772e-04, -4.12808784e-01,\n",
       "         3.46779375e-01, -3.02399438e-01,  4.85932874e-01, -8.81510310e-02,\n",
       "         1.83274898e-01,  3.27191059e-01, -1.45746523e-01, -1.44636188e-01,\n",
       "         1.10599340e-01, -6.37380441e-03, -1.49135358e+00,  1.79455993e-01,\n",
       "        -2.60621616e-01,  5.10211669e-02,  1.10090031e+00, -4.00679157e-01]),\n",
       " array([-0.3366806 ,  0.37872429,  0.08606772])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must have 20 intercepts connecting to hidden layer from the features and 3 intercepts connecting to the output layer connecting from the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the exact shape as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
