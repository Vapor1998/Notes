{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBJzxfQ85wTT"
   },
   "source": [
    "## **Limitations of Linear Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jj8SkGwP54Al"
   },
   "source": [
    "Linear regression, though a very powerful algorithm, has certain disadvantages\n",
    "1. **Main limitation of Linear Regression** is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is almost never linearly separable. The assumption that there is a straight line relationship is usually wrong.\n",
    "\n",
    "2. **Prone to noise and overfitting:** If the number of observations are lesser than the number of features, Linear Regression should not be used, otherwise it may lead to overfit, and the relationship thus formed will be noisy.\n",
    "\n",
    "3. **Prone to outliers:** Linear regression is very sensitive to outliers. An outlier can be considered as an anomaly. It refers to a datapoint which has no clear relationship with any other data point in the data. So, outliers should be analyzed and removed before applying Linear Regression to the dataset, or the linear relationship formed would be highly skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxKroilG54Ux"
   },
   "source": [
    "Let's explore the first limitation in detail. Have a look at the following graph:\n",
    "\n",
    "<img src=\"https://files.codingninjas.in/graph_gd-7051.jpg\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny2szGF354lL"
   },
   "source": [
    "In the above figure, it is quite clear, that the linear line formed will not correctly predict the results of data points(shown in blue).\n",
    "\n",
    "Thus, we need to plot more complex boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUvopCtz542q"
   },
   "source": [
    "## **Plotting More Complex Boundaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecsppjFO55H-"
   },
   "source": [
    "As we have learnt in the previous modules, **Function or Hypothesis** of Linear Regression is represented by -\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + … + m_n.x_n + b$.\n",
    "This is essentially a line of the form $ y = mx + c$.\n",
    "\n",
    "To plot more complex boundaries, we need to have an equation of higher degree. For example:\n",
    "1. $ y = m_1x^2 + m_2x + c $\n",
    "2. $ y = m_1x^3 + m_2x^2 + m_3x + c$\n",
    "\n",
    "So, the basic idea to do this, is to add dummy features in our dataset. Suppose the current data set has three features: $x_1, x_2 $ and $x_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pqcPdx4_sUF"
   },
   "source": [
    "$x_1$ | $x_2$ | $x_3$\n",
    "--- | --- | ---\n",
    "1 | 3 | 5 \n",
    "2 | 4 | 6 \n",
    "- | - | - \n",
    "- | - | - \n",
    "- | - | - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5Xb3R6rAqE9"
   },
   "source": [
    "The equation for the above dataset would be :\n",
    "\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + b$.\n",
    "\n",
    "We can add dummy features to our dataset by enhancing the already existing features. One of the most common method is to create a feature, which is the product of already existing features. Let us create a new feature, $x_{12}$ which is the product of $x_1$ and $x_2$. So, our data set becomes :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YD5w1abBeca"
   },
   "source": [
    "$x_1$ | $x_2$ | $x_3$ | $x_{12}$\n",
    "--- | --- | --- | ---\n",
    "1 | 3 | 5 | $\\;$3\n",
    "2 | 4 | 6 | $\\;$8\n",
    "- | - | - | $\\;$- \n",
    "- | - | - | $\\;$- \n",
    "- | - | - | $\\;$- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYFEXre8CJEZ"
   },
   "source": [
    "The equation for the above dataset would be :\n",
    "\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + m_4.x_{12}+ b$.\n",
    "\n",
    "where the term $x_{12}$ is essentially a quadratic term.\n",
    "\n",
    "We can add more features like $x_{31}$ and $x_{23}$. And we are not just limited to quadratic terms, we may also add cubic terms like $x_{123}$.\n",
    "Other logarithimic and trignometric functions may also be used to plot more complex boundaries.\n",
    "\n",
    "Let's look at an example of how to add a feature into our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfCv-1GWD4Je"
   },
   "source": [
    "### **Example on how to code complex boundaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JvaGqJXEIno"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1H2PYY6DI2W",
    "outputId": "36b8b8f9-f599-4caa-c16d-daae89a0bcdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8.58,   38.38, 1021.03,   84.37],\n",
       "       [  21.79,   58.2 , 1017.21,   66.74],\n",
       "       [  16.64,   48.92, 1011.55,   78.76],\n",
       "       ...,\n",
       "       [  29.8 ,   69.34, 1009.36,   64.74],\n",
       "       [  16.37,   54.3 , 1017.94,   63.63],\n",
       "       [  30.11,   62.04, 1010.69,   47.96]])"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.loadtxt(\"https://files.codingninjas.in/0000000000002419_training_ccpp_x_y_train-7050.csv\", delimiter=\",\")\n",
    "x = train_data[:,:-1]\n",
    "y = train_data[:,-1]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "0IosJ1-bEPyK",
    "outputId": "cc9815e7-de0e-454f-a57f-dd2546d5c5ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.58</td>\n",
       "      <td>38.38</td>\n",
       "      <td>1021.03</td>\n",
       "      <td>84.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.79</td>\n",
       "      <td>58.20</td>\n",
       "      <td>1017.21</td>\n",
       "      <td>66.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.64</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.55</td>\n",
       "      <td>78.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.38</td>\n",
       "      <td>71.32</td>\n",
       "      <td>1009.17</td>\n",
       "      <td>60.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.20</td>\n",
       "      <td>40.03</td>\n",
       "      <td>1017.05</td>\n",
       "      <td>92.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>9.32</td>\n",
       "      <td>37.73</td>\n",
       "      <td>1022.14</td>\n",
       "      <td>79.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>11.20</td>\n",
       "      <td>41.38</td>\n",
       "      <td>1021.65</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7173</th>\n",
       "      <td>29.80</td>\n",
       "      <td>69.34</td>\n",
       "      <td>1009.36</td>\n",
       "      <td>64.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>16.37</td>\n",
       "      <td>54.30</td>\n",
       "      <td>1017.94</td>\n",
       "      <td>63.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>30.11</td>\n",
       "      <td>62.04</td>\n",
       "      <td>1010.69</td>\n",
       "      <td>47.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7176 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1     x2       x3     x4\n",
       "0      8.58  38.38  1021.03  84.37\n",
       "1     21.79  58.20  1017.21  66.74\n",
       "2     16.64  48.92  1011.55  78.76\n",
       "3     31.38  71.32  1009.17  60.42\n",
       "4      9.20  40.03  1017.05  92.46\n",
       "...     ...    ...      ...    ...\n",
       "7171   9.32  37.73  1022.14  79.49\n",
       "7172  11.20  41.38  1021.65  61.89\n",
       "7173  29.80  69.34  1009.36  64.74\n",
       "7174  16.37  54.30  1017.94  63.63\n",
       "7175  30.11  62.04  1010.69  47.96\n",
       "\n",
       "[7176 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = ['x1','x2','x3','x4']\n",
    "df = pd.DataFrame(x)\n",
    "df.columns = li\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "73wPMTRmEPm_",
    "outputId": "f74e4681-6e57-4b0d-c052-bedb86468017"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x1_x1</th>\n",
       "      <th>x1_x2</th>\n",
       "      <th>x1_x3</th>\n",
       "      <th>x1_x4</th>\n",
       "      <th>x2_x2</th>\n",
       "      <th>x2_x3</th>\n",
       "      <th>x2_x4</th>\n",
       "      <th>x3_x3</th>\n",
       "      <th>x3_x4</th>\n",
       "      <th>x4_x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.58</td>\n",
       "      <td>38.38</td>\n",
       "      <td>1021.03</td>\n",
       "      <td>84.37</td>\n",
       "      <td>73.6164</td>\n",
       "      <td>329.3004</td>\n",
       "      <td>8760.4374</td>\n",
       "      <td>723.8946</td>\n",
       "      <td>1473.0244</td>\n",
       "      <td>39187.1314</td>\n",
       "      <td>3238.1206</td>\n",
       "      <td>1.042502e+06</td>\n",
       "      <td>86144.3011</td>\n",
       "      <td>7118.2969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.79</td>\n",
       "      <td>58.20</td>\n",
       "      <td>1017.21</td>\n",
       "      <td>66.74</td>\n",
       "      <td>474.8041</td>\n",
       "      <td>1268.1780</td>\n",
       "      <td>22165.0059</td>\n",
       "      <td>1454.2646</td>\n",
       "      <td>3387.2400</td>\n",
       "      <td>59201.6220</td>\n",
       "      <td>3884.2680</td>\n",
       "      <td>1.034716e+06</td>\n",
       "      <td>67888.5954</td>\n",
       "      <td>4454.2276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.64</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.55</td>\n",
       "      <td>78.76</td>\n",
       "      <td>276.8896</td>\n",
       "      <td>814.0288</td>\n",
       "      <td>16832.1920</td>\n",
       "      <td>1310.5664</td>\n",
       "      <td>2393.1664</td>\n",
       "      <td>49485.0260</td>\n",
       "      <td>3852.9392</td>\n",
       "      <td>1.023233e+06</td>\n",
       "      <td>79669.6780</td>\n",
       "      <td>6203.1376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.38</td>\n",
       "      <td>71.32</td>\n",
       "      <td>1009.17</td>\n",
       "      <td>60.42</td>\n",
       "      <td>984.7044</td>\n",
       "      <td>2238.0216</td>\n",
       "      <td>31667.7546</td>\n",
       "      <td>1895.9796</td>\n",
       "      <td>5086.5424</td>\n",
       "      <td>71974.0044</td>\n",
       "      <td>4309.1544</td>\n",
       "      <td>1.018424e+06</td>\n",
       "      <td>60974.0514</td>\n",
       "      <td>3650.5764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.20</td>\n",
       "      <td>40.03</td>\n",
       "      <td>1017.05</td>\n",
       "      <td>92.46</td>\n",
       "      <td>84.6400</td>\n",
       "      <td>368.2760</td>\n",
       "      <td>9356.8600</td>\n",
       "      <td>850.6320</td>\n",
       "      <td>1602.4009</td>\n",
       "      <td>40712.5115</td>\n",
       "      <td>3701.1738</td>\n",
       "      <td>1.034391e+06</td>\n",
       "      <td>94036.4430</td>\n",
       "      <td>8548.8516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>9.32</td>\n",
       "      <td>37.73</td>\n",
       "      <td>1022.14</td>\n",
       "      <td>79.49</td>\n",
       "      <td>86.8624</td>\n",
       "      <td>351.6436</td>\n",
       "      <td>9526.3448</td>\n",
       "      <td>740.8468</td>\n",
       "      <td>1423.5529</td>\n",
       "      <td>38565.3422</td>\n",
       "      <td>2999.1577</td>\n",
       "      <td>1.044770e+06</td>\n",
       "      <td>81249.9086</td>\n",
       "      <td>6318.6601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>11.20</td>\n",
       "      <td>41.38</td>\n",
       "      <td>1021.65</td>\n",
       "      <td>61.89</td>\n",
       "      <td>125.4400</td>\n",
       "      <td>463.4560</td>\n",
       "      <td>11442.4800</td>\n",
       "      <td>693.1680</td>\n",
       "      <td>1712.3044</td>\n",
       "      <td>42275.8770</td>\n",
       "      <td>2561.0082</td>\n",
       "      <td>1.043769e+06</td>\n",
       "      <td>63229.9185</td>\n",
       "      <td>3830.3721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7173</th>\n",
       "      <td>29.80</td>\n",
       "      <td>69.34</td>\n",
       "      <td>1009.36</td>\n",
       "      <td>64.74</td>\n",
       "      <td>888.0400</td>\n",
       "      <td>2066.3320</td>\n",
       "      <td>30078.9280</td>\n",
       "      <td>1929.2520</td>\n",
       "      <td>4808.0356</td>\n",
       "      <td>69989.0224</td>\n",
       "      <td>4489.0716</td>\n",
       "      <td>1.018808e+06</td>\n",
       "      <td>65345.9664</td>\n",
       "      <td>4191.2676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>16.37</td>\n",
       "      <td>54.30</td>\n",
       "      <td>1017.94</td>\n",
       "      <td>63.63</td>\n",
       "      <td>267.9769</td>\n",
       "      <td>888.8910</td>\n",
       "      <td>16663.6778</td>\n",
       "      <td>1041.6231</td>\n",
       "      <td>2948.4900</td>\n",
       "      <td>55274.1420</td>\n",
       "      <td>3455.1090</td>\n",
       "      <td>1.036202e+06</td>\n",
       "      <td>64771.5222</td>\n",
       "      <td>4048.7769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>30.11</td>\n",
       "      <td>62.04</td>\n",
       "      <td>1010.69</td>\n",
       "      <td>47.96</td>\n",
       "      <td>906.6121</td>\n",
       "      <td>1868.0244</td>\n",
       "      <td>30431.8759</td>\n",
       "      <td>1444.0756</td>\n",
       "      <td>3848.9616</td>\n",
       "      <td>62703.2076</td>\n",
       "      <td>2975.4384</td>\n",
       "      <td>1.021494e+06</td>\n",
       "      <td>48472.6924</td>\n",
       "      <td>2300.1616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7176 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1     x2       x3  ...         x3_x3       x3_x4      x4_x4\n",
       "0      8.58  38.38  1021.03  ...  1.042502e+06  86144.3011  7118.2969\n",
       "1     21.79  58.20  1017.21  ...  1.034716e+06  67888.5954  4454.2276\n",
       "2     16.64  48.92  1011.55  ...  1.023233e+06  79669.6780  6203.1376\n",
       "3     31.38  71.32  1009.17  ...  1.018424e+06  60974.0514  3650.5764\n",
       "4      9.20  40.03  1017.05  ...  1.034391e+06  94036.4430  8548.8516\n",
       "...     ...    ...      ...  ...           ...         ...        ...\n",
       "7171   9.32  37.73  1022.14  ...  1.044770e+06  81249.9086  6318.6601\n",
       "7172  11.20  41.38  1021.65  ...  1.043769e+06  63229.9185  3830.3721\n",
       "7173  29.80  69.34  1009.36  ...  1.018808e+06  65345.9664  4191.2676\n",
       "7174  16.37  54.30  1017.94  ...  1.036202e+06  64771.5222  4048.7769\n",
       "7175  30.11  62.04  1010.69  ...  1.021494e+06  48472.6924  2300.1616\n",
       "\n",
       "[7176 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(i, 4):\n",
    "        ele = li[i] + \"_\" + li[j]\n",
    "        df[ele] = df[li[i]]*df[li[j]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Dzz9gAlElSS"
   },
   "source": [
    "In the above dataset, we had 4 existing features. Using them, we added 10 dummy features who have a quadratic degree. Similar methods can be used to add cubic degree too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8opjAEwpCzkk"
   },
   "source": [
    "**But remember! Don't try to add many extra dummy features, as it leads to overfitting the data, leading to incorrect results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUWTYPHJGSzF"
   },
   "source": [
    "Optimization is a big part of machine learning. Almost every machine learning algorithm has an optimization algorithm at it’s core. Lets look at a very important yet simple optimization algorithm that you can use with any machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Case.  \n",
    "What about n dimensions? $$y = m_1x_1 + m_2x_2 + m_3x_3 .... m_{n+1}x_{n+1} $$ \n",
    "$where \\ c \\ = m_{n+1} \\ and \\ x_{n+1} \\ is \\ column \\ with \\ all \\ 1. $  \n",
    "We cant really differentiate wrt all possible numbers to find the equations.  \n",
    "There is actually formula for this.  \n",
    "So the way we llook at x is lets look at x as it has *m* training datapoints and it has *n* features.\n",
    "\n",
    "So here *we will assume that it has n + 1 features with n original features and additional feature with all 1's.* Why we added that? we want to make it generic.  \n",
    "\n",
    "\n",
    "So if we want to find y, we need to find all the m's. In order to find m, which is coefficient array, there is a formula, $$ coef  = (X^TX)^{-1}*X^T*Y$$  \n",
    "where *X^T = X reversed(matrix transpose)*  \n",
    "SO the dimensions of these are \n",
    "$X = m*(n + 1)$  \n",
    "$X^T = (n+1)*m$    \n",
    "$Y = m*1$\n",
    "\n",
    "But there is a small problem with this.  \n",
    "Lets take time complexity.  \n",
    "$Taking \\ transpose \\ -> O(mn)$,  \n",
    "$Multiplying \\ matrix \\ of \\ (X^TX)^{-1} \\ -> \\ O(mn^2)$  \n",
    "$Taking \\ reverse \\ of X^TX \\ -> O(n^3)$\n",
    "\n",
    "In a nutshell, if n increases, our time increases as well non--linearly. One factor had $m^3$ and one factor had $n^2$  \n",
    "So it will take a lot of time to compute which is not right.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeD_MZanGN0h"
   },
   "source": [
    "## **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6z-ehFwGNyO"
   },
   "source": [
    "Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\n",
    "\n",
    "Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVAR-Q03GNtS"
   },
   "source": [
    "#### **Intuition of Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ein_FM8rGNq4"
   },
   "source": [
    "Think of a large bowl like what you would eat cereal out of or store fruit in. This bowl is a plot of the cost function (f) in 3D space.\n",
    "\n",
    "A random position on the surface of the bowl is the cost of the current values of the coefficients (cost). The bottom of the bowl is the cost of the best set of coefficients, the minimum of the function.\n",
    "\n",
    "The goal is to continue to try different values for the coefficients, evaluate their cost and select new coefficients that have a slightly better (lower) cost.\n",
    "Repeating this process enough times will lead to the bottom of the bowl and you will know the values of the coefficients that result in the minimum cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqdvFpxMGM_I"
   },
   "source": [
    "So our aim is to reach the line \n",
    "$$ y^p = mx + c $$, \n",
    "such that cost function\n",
    "$$ cost = \\sum_i (y_i - (mx_i + c))^2 $$\n",
    "is minimised.\n",
    "Here, m are the coefficents of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFp7yrIGMZXV"
   },
   "source": [
    "The graph of our cost function will look like this :\n",
    "\n",
    "<img src = \"\thttps://files.codingninjas.in/graph_gd2-7053.jpg\" width = \"400\">\n",
    "\n",
    "where, $Cost_{min}$ and $m_{min}$ are the minimum values of $Cost$ and $m$ respectively.\n",
    "\n",
    "The idea is to select $m$ and $cost$ randomly in the beginning. Then, we will find the slope.\n",
    "\n",
    "If the slope is positive, the selected $m$ is to the right of $m_{min}$.\n",
    "\n",
    "If the slope is negative, the selected $m$ is to the left of $m_{min}$.\n",
    "\n",
    "Using the slope, the new optimised value of m ($m'$) can be calculated by :\n",
    "$$ m' = m - \\alpha(slope_m) $$\n",
    "\n",
    "Also, optimised intercept $c$ can be calculated by :\n",
    "$$ c' = c - \\alpha(slope_c) $$\n",
    "\n",
    "where,\n",
    "$\\alpha$ is the learning rate. \n",
    "$$ slope_m = \\frac{\\partial Cost}{\\partial m} $$\n",
    "and \n",
    "$$ slope_c = \\frac{\\partial Cost}{\\partial c} $$\n",
    "\n",
    "It is very easy to find the above two partials.\n",
    "Taking the derivative of $Cost$ wrt $m$ gives us\n",
    "\n",
    "$$ \\frac{\\partial Cost}{\\partial m} = \\frac{-2}{N}\\sum_i(y_i - mx_i - c)x_i $$\n",
    "Taking the derivative of $Cost$ wrt $c$ gives us\n",
    "\n",
    "$$ \\frac{\\partial Cost}{\\partial c} = \\frac{-2}{N}\\sum_i(y_i - mx_i - c) $$\n",
    "\n",
    "\n",
    "Now the question arises, how many times do we optimise $m$ and $c$?\n",
    "\n",
    "For each new value of $m$ and $c$, calculate the $Cost$ too. If all is done correclty, you will notice that with each new optimised value, optimised $Cost$ will keep decreasing.\n",
    "\n",
    "So, we keep on optimising $m$ and $c$ till we reach a point, where the change is $Cost$ (ie, the decrease in cost) is very less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKbB1RKPMZDS"
   },
   "source": [
    "#### **Learning Rate ($\\alpha$) and its Importance**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha9lW4N_MZAx"
   },
   "source": [
    "How big the steps are gradient descent takes into the direction of the local minimum are determined by the learning rate, which figures out how fast or slow we will move towards the optimal weights.\n",
    "\n",
    "For gradient descent to reach the local minimum we must set the learning rate to an appropriate value, which is neither too low nor too high. This is important because if the steps it takes are too big, it may not reach the local minimum because it bounces back and forth between the convex function of gradient descent (see left image below). If we set the learning rate to a very small value, gradient descent will eventually reach the local minimum but that may take a while (see the right image). \n",
    "\n",
    "<img src=\"https://files.codingninjas.in/gradient-descent-learning-rate-7052.png\" width=\"550\">\n",
    "\n",
    "So, the learning rate should never be too high or too low for this reason. You can check if you’re learning rate is doing well by plotting it on a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n76YtelzOXNs"
   },
   "source": [
    "#### **Adaptive Learning Rate**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ihW18ejOXrt"
   },
   "source": [
    "The performance of the model on the training dataset can be monitored by the learning algorithm and the learning rate can be adjusted in response. \n",
    "This is called an adaptive learning rate. \n",
    "Perhaps the simplest implementation is to make the learning rate smaller once the performance of the model plateaus, such as by decreasing the learning rate by a factor of two.\n",
    "\n",
    "An adaptive learning rate method will generally outperform a model with a badly configured learning rate.  \n",
    "  \n",
    "What we do in this is we start with higher value of alpha, so initially we make bigger jumps, but then keep reducing alpha as we go closer to the minima.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7OOJrLIhcn1"
   },
   "source": [
    "\n",
    "### **Let's code Gradient Descent for a Single Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(points, learning_rate, num_iterations):\n",
    "    m = 0\n",
    "    c = 0\n",
    "    for i in range(num_iterations):\n",
    "        m, c = step_gradient(points, learning_rate, m, c)\n",
    "        print(i, 'Cost: ', cost(points, m, c))\n",
    "    return m, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(points, m, c):\n",
    "    total_cost = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        total_cost += (1/M) * ( y - m*x - c)**2\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    data = np.loadtxt(\"https://files.codingninjas.in/data-6984.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    num_iterations = 100\n",
    "    m, c = gd(data, learning_rate, num_iterations)\n",
    "    print(m, c)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "points =  np.array([[1,1], [2,3],[2,2]])\n",
    "for i in range(len(points)):\n",
    "    x = points[i, 0]\n",
    "    y = points[i, 1]\n",
    "    #print(x, end = ' ')\n",
    "    #print(y)\n",
    "print(points[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-db519b80da0a>:8: RuntimeWarning: overflow encountered in double_scalars\n",
      "  m_slope += (-2/M)* (y - m * x - c) * x\n",
      "<ipython-input-10-db519b80da0a>:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  new_m = m - learning_rate*m_slope\n"
     ]
    }
   ],
   "source": [
    "run()\n",
    "# this issue is because of the learning rate. Its gone so high it is out of range.\n",
    "# to fix this, we need to define the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  1484.5865574086486\n",
      "1 Cost:  457.8542575737672\n",
      "2 Cost:  199.5099857255389\n",
      "3 Cost:  134.50591058200533\n",
      "4 Cost:  118.1496934223995\n",
      "5 Cost:  114.0341490603815\n",
      "6 Cost:  112.99857731713657\n",
      "7 Cost:  112.73798187568467\n",
      "8 Cost:  112.6723843590911\n",
      "9 Cost:  112.65585181499745\n",
      "10 Cost:  112.65166489759581\n",
      "11 Cost:  112.6505843615011\n",
      "12 Cost:  112.65028544701502\n",
      "13 Cost:  112.65018320293967\n",
      "14 Cost:  112.650130445072\n",
      "15 Cost:  112.65009013922885\n",
      "16 Cost:  112.6500529669463\n",
      "17 Cost:  112.65001658353178\n",
      "18 Cost:  112.64998039901865\n",
      "19 Cost:  112.64994426496071\n",
      "20 Cost:  112.64990814400622\n",
      "21 Cost:  112.64987202675677\n",
      "22 Cost:  112.64983591084761\n",
      "23 Cost:  112.64979979568368\n",
      "24 Cost:  112.64976368111523\n",
      "25 Cost:  112.64972756710469\n",
      "26 Cost:  112.64969145364236\n",
      "27 Cost:  112.64965534072611\n",
      "28 Cost:  112.64961922835512\n",
      "29 Cost:  112.64958311652944\n",
      "30 Cost:  112.64954700524868\n",
      "31 Cost:  112.64951089451318\n",
      "32 Cost:  112.64947478432279\n",
      "33 Cost:  112.64943867467744\n",
      "34 Cost:  112.64940256557728\n",
      "35 Cost:  112.64936645702221\n",
      "36 Cost:  112.64933034901203\n",
      "37 Cost:  112.64929424154704\n",
      "38 Cost:  112.64925813462712\n",
      "39 Cost:  112.6492220282522\n",
      "40 Cost:  112.64918592242235\n",
      "41 Cost:  112.64914981713754\n",
      "42 Cost:  112.64911371239779\n",
      "43 Cost:  112.64907760820296\n",
      "44 Cost:  112.64904150455324\n",
      "45 Cost:  112.64900540144845\n",
      "46 Cost:  112.64896929888867\n",
      "47 Cost:  112.64893319687388\n",
      "48 Cost:  112.6488970954041\n",
      "49 Cost:  112.64886099447922\n",
      "50 Cost:  112.64882489409929\n",
      "51 Cost:  112.64878879426433\n",
      "52 Cost:  112.64875269497436\n",
      "53 Cost:  112.64871659622933\n",
      "54 Cost:  112.64868049802914\n",
      "55 Cost:  112.648644400374\n",
      "56 Cost:  112.64860830326366\n",
      "57 Cost:  112.64857220669828\n",
      "58 Cost:  112.64853611067772\n",
      "59 Cost:  112.64850001520212\n",
      "60 Cost:  112.64846392027131\n",
      "61 Cost:  112.64842782588545\n",
      "62 Cost:  112.64839173204442\n",
      "63 Cost:  112.6483556387483\n",
      "64 Cost:  112.64831954599697\n",
      "65 Cost:  112.64828345379043\n",
      "66 Cost:  112.64824736212877\n",
      "67 Cost:  112.64821127101193\n",
      "68 Cost:  112.64817518043986\n",
      "69 Cost:  112.64813909041264\n",
      "70 Cost:  112.64810300093015\n",
      "71 Cost:  112.64806691199259\n",
      "72 Cost:  112.64803082359971\n",
      "73 Cost:  112.64799473575155\n",
      "74 Cost:  112.64795864844827\n",
      "75 Cost:  112.64792256168963\n",
      "76 Cost:  112.64788647547579\n",
      "77 Cost:  112.64785038980668\n",
      "78 Cost:  112.64781430468226\n",
      "79 Cost:  112.64777822010265\n",
      "80 Cost:  112.6477421360677\n",
      "81 Cost:  112.64770605257743\n",
      "82 Cost:  112.64766996963193\n",
      "83 Cost:  112.64763388723107\n",
      "84 Cost:  112.64759780537483\n",
      "85 Cost:  112.64756172406335\n",
      "86 Cost:  112.6475256432965\n",
      "87 Cost:  112.64748956307432\n",
      "88 Cost:  112.64745348339677\n",
      "89 Cost:  112.64741740426388\n",
      "90 Cost:  112.6473813256756\n",
      "91 Cost:  112.64734524763193\n",
      "92 Cost:  112.64730917013293\n",
      "93 Cost:  112.6472730931785\n",
      "94 Cost:  112.64723701676861\n",
      "95 Cost:  112.64720094090339\n",
      "96 Cost:  112.64716486558265\n",
      "97 Cost:  112.64712879080662\n",
      "98 Cost:  112.64709271657513\n",
      "99 Cost:  112.64705664288809\n",
      "1.4788027175308358 0.035074970592341756\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqriJytGh2L3"
   },
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YobHmqZNh6nb",
    "outputId": "9217a2ce-905a-4989-f4a8-f955a9cf7257"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt(\"https://files.codingninjas.in/data-6984.csv\", delimiter=\",\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DomnCAj0pAVF",
    "outputId": "0fd4d2a1-999b-40d8-bcdd-502c8dac5502"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = data[:70,:]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjD-2DIppOrJ",
    "outputId": "1235619c-bb90-41ba-b4b5-90628a28f339"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = data[70:,]\n",
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkF9c3OGh2IM"
   },
   "source": [
    "**Now, using gradient descent, we will find the best values of m and c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hlk8sUK_ieFz"
   },
   "outputs": [],
   "source": [
    "# This function finds the new gradient at each step\n",
    "def step_gradient(points, learning_rate, m , c):\n",
    "    m_slope = 0\n",
    "    c_slope = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        m_slope += (-2/M)* (y - m * x - c)*x\n",
    "        c_slope += (-2/M)* (y - m * x - c)\n",
    "    new_m = m - learning_rate * m_slope\n",
    "    new_c = c - learning_rate * c_slope\n",
    "    return new_m, new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-ceqVsJjXIy"
   },
   "outputs": [],
   "source": [
    "# The Gradient Descent Function\n",
    "def gd(points, learning_rate, num_iterations):\n",
    "    m = 0       # Intial random value taken as 0\n",
    "    c = 0       # Intial random value taken as 0\n",
    "    for i in range(num_iterations):\n",
    "        m, c = step_gradient(points, learning_rate, m , c)\n",
    "        print(i, \" Cost: \", cost(points, m, c))\n",
    "    return m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-sIta_SjdeE"
   },
   "outputs": [],
   "source": [
    "# This function finds the new cost after each optimisation.\n",
    "def cost(points, m, c):\n",
    "    total_cost = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        total_cost += (1/M)*((y - m*x - c)**2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1q4EzknjlGW"
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    learning_rate = 0.0001\n",
    "    num_iterations = 100\n",
    "    m, c = gd(training_data, learning_rate, num_iterations)\n",
    "    print(\"Final m :\", m)\n",
    "    print(\"Final c :\", c)\n",
    "    return m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehsYo3P3jyvL",
    "outputId": "f20ed4d2-d741-4287-bc10-bd3f2475b77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Cost:  1461.4044104341087\n",
      "1  Cost:  460.8670268567474\n",
      "2  Cost:  205.4870778024464\n",
      "3  Cost:  140.30318108579826\n",
      "4  Cost:  123.66545280139864\n",
      "5  Cost:  119.41878332450108\n",
      "6  Cost:  118.33484209854512\n",
      "7  Cost:  118.05816441204072\n",
      "8  Cost:  117.98753491264765\n",
      "9  Cost:  117.96949772470519\n",
      "10  Cost:  117.96488434447647\n",
      "11  Cost:  117.96369729432573\n",
      "12  Cost:  117.96338479030575\n",
      "13  Cost:  117.96329550799736\n",
      "14  Cost:  117.9632632015475\n",
      "15  Cost:  117.9632454379033\n",
      "16  Cost:  117.96323138633423\n",
      "17  Cost:  117.96321828237488\n",
      "18  Cost:  117.96320542041524\n",
      "19  Cost:  117.96319262035338\n",
      "20  Cost:  117.9631798362198\n",
      "21  Cost:  117.96316705628094\n",
      "22  Cost:  117.96315427754192\n",
      "23  Cost:  117.96314149923846\n",
      "24  Cost:  117.96312872117527\n",
      "25  Cost:  117.96311594330255\n",
      "26  Cost:  117.9631031656078\n",
      "27  Cost:  117.9630903880875\n",
      "28  Cost:  117.96307761074107\n",
      "29  Cost:  117.96306483356811\n",
      "30  Cost:  117.96305205656859\n",
      "31  Cost:  117.96303927974257\n",
      "32  Cost:  117.96302650309\n",
      "33  Cost:  117.96301372661085\n",
      "34  Cost:  117.96300095030523\n",
      "35  Cost:  117.96298817417302\n",
      "36  Cost:  117.96297539821425\n",
      "37  Cost:  117.96296262242893\n",
      "38  Cost:  117.96294984681701\n",
      "39  Cost:  117.96293707137862\n",
      "40  Cost:  117.96292429611363\n",
      "41  Cost:  117.962911521022\n",
      "42  Cost:  117.96289874610383\n",
      "43  Cost:  117.96288597135916\n",
      "44  Cost:  117.96287319678783\n",
      "45  Cost:  117.96286042238998\n",
      "46  Cost:  117.96284764816549\n",
      "47  Cost:  117.9628348741145\n",
      "48  Cost:  117.96282210023688\n",
      "49  Cost:  117.9628093265326\n",
      "50  Cost:  117.96279655300184\n",
      "51  Cost:  117.96278377964448\n",
      "52  Cost:  117.96277100646049\n",
      "53  Cost:  117.96275823344999\n",
      "54  Cost:  117.96274546061275\n",
      "55  Cost:  117.96273268794901\n",
      "56  Cost:  117.96271991545866\n",
      "57  Cost:  117.96270714314169\n",
      "58  Cost:  117.96269437099812\n",
      "59  Cost:  117.9626815990279\n",
      "60  Cost:  117.96266882723108\n",
      "61  Cost:  117.96265605560774\n",
      "62  Cost:  117.9626432841577\n",
      "63  Cost:  117.96263051288103\n",
      "64  Cost:  117.9626177417778\n",
      "65  Cost:  117.9626049708479\n",
      "66  Cost:  117.96259220009142\n",
      "67  Cost:  117.96257942950821\n",
      "68  Cost:  117.96256665909843\n",
      "69  Cost:  117.96255388886206\n",
      "70  Cost:  117.96254111879902\n",
      "71  Cost:  117.96252834890933\n",
      "72  Cost:  117.962515579193\n",
      "73  Cost:  117.96250280965003\n",
      "74  Cost:  117.96249004028041\n",
      "75  Cost:  117.96247727108418\n",
      "76  Cost:  117.96246450206127\n",
      "77  Cost:  117.96245173321168\n",
      "78  Cost:  117.96243896453551\n",
      "79  Cost:  117.9624261960326\n",
      "80  Cost:  117.96241342770308\n",
      "81  Cost:  117.96240065954687\n",
      "82  Cost:  117.96238789156405\n",
      "83  Cost:  117.96237512375453\n",
      "84  Cost:  117.9623623561184\n",
      "85  Cost:  117.9623495886555\n",
      "86  Cost:  117.96233682136597\n",
      "87  Cost:  117.96232405424978\n",
      "88  Cost:  117.96231128730685\n",
      "89  Cost:  117.96229852053732\n",
      "90  Cost:  117.96228575394112\n",
      "91  Cost:  117.96227298751815\n",
      "92  Cost:  117.96226022126854\n",
      "93  Cost:  117.96224745519224\n",
      "94  Cost:  117.96223468928922\n",
      "95  Cost:  117.96222192355955\n",
      "96  Cost:  117.96220915800318\n",
      "97  Cost:  117.96219639262009\n",
      "98  Cost:  117.96218362741031\n",
      "99  Cost:  117.96217086237382\n",
      "Final m : 1.458255777804894\n",
      "Final c : 0.032397159787702676\n"
     ]
    }
   ],
   "source": [
    "m, c = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt6xE00th1mu"
   },
   "source": [
    "It can be seen, that the cost for the last many iterations remains almost the same, which is 117.962.\n",
    "\n",
    "For this cost, the final m is found to be 1.4582 and final c is found to be 0.0323.\n",
    "\n",
    "These optimised values may then be plugged into our hypothesis funcion to find $y_{pred}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GUpU92HpsAO"
   },
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lAzkZW-prFG"
   },
   "outputs": [],
   "source": [
    "def predict(final_m, final_c, testing_data):\n",
    "    y_pred = []\n",
    "    for i in range(len(testing_data)):\n",
    "        ans = m*testing_data[i][0] + c\n",
    "        y_pred.append(ans)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1q0jeM6qYlN",
    "outputId": "b009df9f-f840-4979-bcf1-a20b00015f4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46.095951282291885,\n",
       " 78.28376167276944,\n",
       " 68.10702680868928,\n",
       " 62.8946250628685,\n",
       " 102.61496837133335,\n",
       " 64.91436131908361,\n",
       " 83.8887150992528,\n",
       " 53.885894749919025,\n",
       " 81.41143026364702,\n",
       " 56.838414234095204,\n",
       " 83.00892226345628,\n",
       " 82.96180012666355,\n",
       " 50.09887462980676,\n",
       " 86.14202346395936,\n",
       " 84.30240868699974,\n",
       " 79.18991662796913,\n",
       " 74.5328181331299,\n",
       " 73.35763378901311,\n",
       " 64.50442501659097,\n",
       " 55.45411963583681,\n",
       " 48.06804235977706,\n",
       " 78.32854078411849,\n",
       " 100.31042647345949,\n",
       " 67.44897116945312,\n",
       " 99.65949980894587,\n",
       " 72.98918795613403,\n",
       " 71.83656946861555,\n",
       " 73.00289789301861,\n",
       " 70.24720708812089,\n",
       " 36.67615508488324]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(m, c, testing_data)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YIbimIksCAd"
   },
   "source": [
    "### **Using the inbuilt Gradient Booster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0nOdHNZss3L",
    "outputId": "0eeaed43-9765-411c-bc09-d1cfd5e26700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "x_train = training_data[:,0].reshape(-1, 1)\n",
    "y_train = training_data[:,1]\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6bboIhOtTUF",
    "outputId": "c3638b67-a73d-4531-aebb-ee99285ba12b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.05621077, 82.80773117, 73.60871863, 62.80258768, 85.242871  ,\n",
       "       62.80258768, 77.29833337, 60.4283958 , 83.40209375, 58.3415076 ,\n",
       "       83.40209375, 83.40209375, 51.3317227 , 77.88979793, 77.29833337,\n",
       "       82.80773117, 75.26029188, 72.23701306, 62.80258768, 76.2986538 ,\n",
       "       51.19104556, 82.80773117, 85.242871  , 73.60871863, 85.242871  ,\n",
       "       78.9358657 , 79.11953733, 78.9358657 , 68.91225498, 33.05621077])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(testing_data[:,0].reshape(-1, 1))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaxOgD6BNFYL"
   },
   "source": [
    "#### **Types of Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZYJ6JdgNMHf"
   },
   "source": [
    "***Batch Gradient Descent***\n",
    "\n",
    "Batch gradient descent, also called vanilla gradient descent, calculates the error for each example within the training dataset, but only after all training examples have been evaluated does the model get updated. This whole process is like a cycle and it's called a training epoch.\n",
    "\n",
    "Some advantages of batch gradient descent are that it's computationally efficient, it produces a stable error gradient and a stable convergence. Some disadvantages are the stable error gradient can sometimes result in a state of convergence that isn’t the best the model can achieve. It also requires the entire training dataset be in memory and available to the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pqc-83HZNVFj"
   },
   "source": [
    "***Stochastic Gradient Descent***\n",
    "\n",
    "By contrast, stochastic gradient descent (SGD) does this for each training example within the dataset, meaning it updates the parameters for each training example one by one. Depending on the problem, this can make SGD faster than batch gradient descent. One advantage is the frequent updates allow us to have a pretty detailed rate of improvement.\n",
    "\n",
    "The frequent updates, however, are more computationally expensive than the batch gradient descent approach. Additionally, the frequency of those updates can result in noisy gradients, which may cause the error rate to jump around instead of slowly decreasing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKDTcq-ENU6c"
   },
   "source": [
    "***Mini-Batch Gradient Descent***\n",
    "\n",
    "Mini-batch gradient descent is the go-to method since it’s a combination of the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent.\n",
    "\n",
    "Common mini-batch sizes range between 50 and 256, but like any other machine learning technique, there is no clear rule because it varies for different applications. This is the go-to algorithm when training a neural network and it is the most common type of gradient descent within deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEWQhyBjNm0Y"
   },
   "source": [
    "#### **Some Useful Tips for Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ9xlvRxNr5s"
   },
   "source": [
    "**Plot Cost versus Time:** Collect and plot the cost values calculated by the algorithm each iteration. The expectation for a well performing gradient descent run is a decrease in cost each iteration. If it does not decrease, try reducing your learning rate.\n",
    "\n",
    "**Choose correct Learning Rate:** The learning rate value is a small real value such as 0.1, 0.001 or 0.0001. Try different values for your problem and see which works best.\n",
    "\n",
    "**Rescale Inputs:** The algorithm will reach the minimum cost faster if the shape of the cost function is not skewed and distorted. You can achieved this by rescaling all of the input variables (X) to the same range, such as [0, 1] or [-1, 1].\n",
    "\n",
    "**Few Passes:** Stochastic gradient descent often does not need more than 1-10 passes through the training dataset to converge on good or good enough coefficients.\n",
    "\n",
    "**Plot Mean Cost:** The updates for each training dataset instance can result in a noisy plot of cost over time when using stochastic gradient descent. Taking the average over 10, 100, or 1000 updates can give you a better idea of the learning trend for the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5HyDcjRPeJG"
   },
   "source": [
    "#### **Your Next Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq4rvP-qPiyg"
   },
   "source": [
    "You have learnt how to code Gradient Descent for a single featured dataset. Try to code a more Generic Gradient Descent. Let us consider that the $i^{th}$ feature for the first row is $x_1^i$. Similarily for the $j^th$ row, the $i^{th}$ feature will be $x_j^i$.So, your cost function would look something like :\n",
    "\n",
    "$$ cost = \\frac{1}{M}\\sum_i^M (y_i - (m_ix_i^1 + m_ix_i^2 + m_ix_i^3 + ...... + m_{n + 1}x_{n + 1} ))^2 $$\n",
    "\n",
    "Here $m_{n + 1}x_{n + 1}$ is actually 'c', constant value. (We usually take them to be 1)\n",
    "\n",
    "Also, to find the next m (m'), our equation becomes :\n",
    "$$ m_j' = m_j - \\alpha\\frac{\\partial cost}{\\partial m_j} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\frac{\\partial cost}{\\partial m_i} = \\frac{1}{M}\\sum_i^M 2(y_i - (m_ix_i^1 + m_ix_i^2 + m_ix_i^3 + ...... + m_{n + 1}x_{n + 1} ))x_i^j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^{i}_j$ means ith column(feature) and jth row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So of we say what is the 4th datapoint of 5th feature? It is $x^{5}_4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO what is our cost function? SO the cost function is $$\\frac{1}{M}\\sum_{i=1}^m (y_i - m_1x_i^{1} + m_2x_i^{2} + m_3x_i^{3} ... m_{n+1}x_{n+1})^2 $$  \n",
    "  \n",
    "And we have also added extra column with all 1's, so instead of c we will call it $m_{n+1}x_{n+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our task is to find all these $m's$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we were saying for 1d case.  \n",
    "$m' = m - \\alpha \\frac{\\partial Cost}{\\partial m}$. Its exactly the same for generic. The only thing changing is   \n",
    "$m'_j = m_j - \\alpha \\frac{\\partial Cost}{\\partial m_j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So everything will become easy if we find $\\frac{\\partial Cost}{\\partial m_j}$   \n",
    "To find this, we take the derivative   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial Cost}{\\partial m_j} = \\frac{1}{M} \\sum_{i = 1}^M 2(y_i - (m_1x_i^1 + m_2x_i^2 .... )$$  \n",
    "If we differentiate the internal part, w.r.t $m_j$ , the only thing we get here will be $x_i^j$. Means for $m_1$, we get $x_i^1$ (We get only this coz everything is independent of $m_1$ so we dont need it to be differentiated.), For $m_2$ we get $x_i^2$, and so on.  \n",
    "So we get  $$\\frac{\\partial Cost}{\\partial m_j} = \\frac{1}{M} \\sum_{i = 1}^M 2(y_i - (m_1x_i^1 + m_2x_i^2 .... )x_i^j$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can calculate what is the value of slope will be for a particular $m_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Generic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes are we pass m as array with last element as c in it.\n",
    "# We will not have m_slope or c_slope as well, we will have another array whoch will start with all 0's and we will update \n",
    "# these values within this array.  \n",
    "\n",
    "def step_gradient(X, Y, learning_rate, m):\n",
    "    \n",
    "    # m_slope = 0\n",
    "    # c_slope = 0\n",
    "    \n",
    "    features = X.shape[1]\n",
    "    mslope  = np.array([0.0 for i in range(features)])   # make sure we take numpy array here or else it will give typeerror.\n",
    "    M = len(X)\n",
    "    for i in range(M): # going through all data\n",
    "        x = X[i, :]\n",
    "        y = Y[i]\n",
    "        \n",
    "        # calculating m*x for a row here\n",
    "        # this will return an array of mx.\n",
    "        mx = m*x\n",
    "        for j in range(features):\n",
    "            mslope[j] +=   (-2/M)* (y - mx.sum(axis = 0) ) * x[j]  \n",
    "    new_m = m - learning_rate*mslope\n",
    "    m = new_m\n",
    "    return m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(X, Y, learning_rate, num_iterations):\n",
    "    m = [0 for i in range(X.shape[1])]\n",
    "    for i in range(num_iterations):\n",
    "        m= step_gradient(X,Y, learning_rate, m)\n",
    "        print(i, 'Cost: ', cost(X, Y, m))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, Y, m):\n",
    "    total_cost = 0\n",
    "    M = len(X)\n",
    "    \n",
    "    for i in range(M):\n",
    "        x = X[i, :]\n",
    "        y = Y[i]\n",
    "        total_cost += (1/M) * ( y - (m*x).sum() )**2\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    data = np.loadtxt(\"https://files.codingninjas.in/data-6984.csv\", delimiter=\",\")\n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    num_iterations = 100\n",
    "    \n",
    "    # here we adding columns with all 1's for Generic Case.\n",
    "    X = data[:, :-1]\n",
    "    Y = data[:, -1]\n",
    "    for i in X:\n",
    "        i = np.append(i, 1.0)\n",
    "\n",
    "    m = gd(X, Y, learning_rate, num_iterations)\n",
    "    print(m, m[-1])# As last element of m is c.\n",
    "    return m\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  1485.652621595884\n",
      "1 Cost:  458.4008018325014\n",
      "2 Cost:  199.7274628352812\n",
      "3 Cost:  134.590662860195\n",
      "4 Cost:  118.18849815531217\n",
      "5 Cost:  114.05825216366071\n",
      "6 Cost:  113.01821066557271\n",
      "7 Cost:  112.7563167445008\n",
      "8 Cost:  112.69036896643561\n",
      "9 Cost:  112.67376258850246\n",
      "10 Cost:  112.66958091936738\n",
      "11 Cost:  112.66852792895457\n",
      "12 Cost:  112.66826277435395\n",
      "13 Cost:  112.66819600550082\n",
      "14 Cost:  112.66817919236713\n",
      "15 Cost:  112.66817495863467\n",
      "16 Cost:  112.66817389253406\n",
      "17 Cost:  112.66817362407818\n",
      "18 Cost:  112.66817355647805\n",
      "19 Cost:  112.66817353945555\n",
      "20 Cost:  112.66817353516912\n",
      "21 Cost:  112.6681735340897\n",
      "22 Cost:  112.66817353381796\n",
      "23 Cost:  112.66817353374945\n",
      "24 Cost:  112.6681735337323\n",
      "25 Cost:  112.66817353372797\n",
      "26 Cost:  112.66817353372683\n",
      "27 Cost:  112.66817353372657\n",
      "28 Cost:  112.66817353372653\n",
      "29 Cost:  112.66817353372656\n",
      "30 Cost:  112.66817353372646\n",
      "31 Cost:  112.6681735337265\n",
      "32 Cost:  112.66817353372646\n",
      "33 Cost:  112.6681735337265\n",
      "34 Cost:  112.6681735337265\n",
      "35 Cost:  112.66817353372652\n",
      "36 Cost:  112.6681735337265\n",
      "37 Cost:  112.66817353372649\n",
      "38 Cost:  112.66817353372657\n",
      "39 Cost:  112.66817353372647\n",
      "40 Cost:  112.66817353372649\n",
      "41 Cost:  112.66817353372653\n",
      "42 Cost:  112.66817353372649\n",
      "43 Cost:  112.6681735337265\n",
      "44 Cost:  112.66817353372649\n",
      "45 Cost:  112.66817353372646\n",
      "46 Cost:  112.66817353372652\n",
      "47 Cost:  112.6681735337265\n",
      "48 Cost:  112.66817353372649\n",
      "49 Cost:  112.6681735337265\n",
      "50 Cost:  112.66817353372652\n",
      "51 Cost:  112.66817353372647\n",
      "52 Cost:  112.66817353372647\n",
      "53 Cost:  112.66817353372647\n",
      "54 Cost:  112.66817353372647\n",
      "55 Cost:  112.66817353372647\n",
      "56 Cost:  112.66817353372647\n",
      "57 Cost:  112.66817353372647\n",
      "58 Cost:  112.66817353372647\n",
      "59 Cost:  112.66817353372647\n",
      "60 Cost:  112.66817353372647\n",
      "61 Cost:  112.66817353372647\n",
      "62 Cost:  112.66817353372647\n",
      "63 Cost:  112.66817353372647\n",
      "64 Cost:  112.66817353372647\n",
      "65 Cost:  112.66817353372647\n",
      "66 Cost:  112.66817353372647\n",
      "67 Cost:  112.66817353372647\n",
      "68 Cost:  112.66817353372647\n",
      "69 Cost:  112.66817353372647\n",
      "70 Cost:  112.66817353372647\n",
      "71 Cost:  112.66817353372647\n",
      "72 Cost:  112.66817353372647\n",
      "73 Cost:  112.66817353372647\n",
      "74 Cost:  112.66817353372647\n",
      "75 Cost:  112.66817353372647\n",
      "76 Cost:  112.66817353372647\n",
      "77 Cost:  112.66817353372647\n",
      "78 Cost:  112.66817353372647\n",
      "79 Cost:  112.66817353372647\n",
      "80 Cost:  112.66817353372647\n",
      "81 Cost:  112.66817353372647\n",
      "82 Cost:  112.66817353372647\n",
      "83 Cost:  112.66817353372647\n",
      "84 Cost:  112.66817353372647\n",
      "85 Cost:  112.66817353372647\n",
      "86 Cost:  112.66817353372647\n",
      "87 Cost:  112.66817353372647\n",
      "88 Cost:  112.66817353372647\n",
      "89 Cost:  112.66817353372647\n",
      "90 Cost:  112.66817353372647\n",
      "91 Cost:  112.66817353372647\n",
      "92 Cost:  112.66817353372647\n",
      "93 Cost:  112.66817353372647\n",
      "94 Cost:  112.66817353372647\n",
      "95 Cost:  112.66817353372647\n",
      "96 Cost:  112.66817353372647\n",
      "97 Cost:  112.66817353372647\n",
      "98 Cost:  112.66817353372647\n",
      "99 Cost:  112.66817353372647\n",
      "[1.47948973] 1.4794897258742805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.47948973])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = run()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, m, c):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros(N)\n",
    "    \n",
    "    m=m.reshape(-1, 1)\n",
    "    X = X.reshape(-1,1)\n",
    "    for i in range(N):\n",
    "        x = X[i, :]\n",
    "        y[i] = (m*x).sum() + c \n",
    "    return y\n",
    "\n",
    "#Ypred= predict(testing_data,m1, c) \n",
    "#Ypred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4f971ce569a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'list'"
     ]
    }
   ],
   "source": [
    "\n",
    "a = [1, 2, 3 ]\n",
    "n = [3, 4, 5]\n",
    "c = a * n\n",
    "c  # this type of error will occur(not this one) if we dont take numpy array in step_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Generic Gradient Descent for Boston Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We first load dataset which is inside the module\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "type(boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After checking, we saw that data is the sample points while target is the test of the datapoints, and features names as column names\n",
    "## So we take it into the variable.\n",
    "\n",
    "X = boston.data\n",
    "Y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting Data into Train-Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying Feature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41182307, -0.48521388, -0.71840425, ..., -0.50639585,\n",
       "         0.24321193, -0.23529519],\n",
       "       [ 1.85924131, -0.48521388,  1.05388146, ...,  0.79920978,\n",
       "         0.44879453,  0.12351472],\n",
       "       [-0.23867409, -0.48521388,  1.27430019, ..., -1.76537271,\n",
       "        -1.2784054 , -0.05869344],\n",
       "       ...,\n",
       "       [-0.44838411,  3.73411364, -1.24264339, ..., -1.76537271,\n",
       "         0.38400795, -1.35657613],\n",
       "       [ 7.55045832, -0.48521388,  1.05388146, ...,  0.79920978,\n",
       "         0.32707739,  1.46064225],\n",
       "       [-0.4064831 , -0.48521388, -0.06459465, ...,  0.05314942,\n",
       "         0.4318582 ,  1.60080236]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# creating object\n",
    "SS = StandardScaler()\n",
    "X_train[:,:]=SS.fit_transform(X_train[:,:])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.32399292, -0.49794465,  2.24595056, ...,  0.78393171,\n",
       "         0.33848736,  0.7208233 ],\n",
       "       [-0.33018589, -0.49794465, -0.8677052 , ...,  0.82910382,\n",
       "         0.33239083, -1.018244  ],\n",
       "       [-0.33139494,  2.64534062, -1.37445094, ..., -2.60397644,\n",
       "         0.44197606, -1.2428209 ],\n",
       "       ...,\n",
       "       [-0.32985994, -0.49794465,  0.33771453, ..., -1.06812474,\n",
       "         0.38024864, -0.77822745],\n",
       "       [-0.32017405, -0.49794465, -0.40238281, ...,  1.14530858,\n",
       "         0.37781002, -0.0848463 ],\n",
       "       [-0.31915223, -0.49794465, -0.40238281, ...,  1.14530858,\n",
       "         0.4093596 , -0.49329552]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling testing data\n",
    "X_test[:,:]=SS.fit_transform(X_test[:,:])\n",
    "X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes are we pass m as array with last element as c in it.\n",
    "# We will not have m_slope or c_slope as well, we will have another array whoch will start with all 0's and we will update \n",
    "# these values within this array.  \n",
    "\n",
    "def step_gradient(X, Y, learning_rate, m):\n",
    "    \n",
    "    # m_slope = 0\n",
    "    # c_slope = 0\n",
    "    \n",
    "    features = X.shape[1]\n",
    "    # make sure to take 0.0 in mslope or it will not change the cost(the cost will remain same and slope will remain 0 after every iteration).\n",
    "    mslope  = np.array([0.0 for i in range(features)])   # make sure we take numpy array here or else it will give typeerror.\n",
    "    M = len(X)\n",
    "    for i in range(M): # going through all data\n",
    "        x = X[i, :]\n",
    "        y = Y[i]\n",
    "        \n",
    "        # calculating m*x for a row here\n",
    "        # this will return an array of mx.\n",
    "        mx = m*x\n",
    "        for j in range(features):\n",
    "            mslope[j] +=   (-2/M)* (y - mx.sum(axis = 0) ) * x[j]  \n",
    "    new_m = m - learning_rate*mslope\n",
    "    m = new_m\n",
    "    return m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(X, Y, learning_rate, num_iterations):\n",
    "    m = [0 for i in range(X.shape[1])]\n",
    "    for i in range(num_iterations):\n",
    "        m= step_gradient(X,Y, learning_rate, m)\n",
    "        print(i, 'Cost: ', cost(X, Y, m))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, Y, m):\n",
    "    total_cost = 0\n",
    "    M = len(X)\n",
    "    \n",
    "    for i in range(M):\n",
    "        x = X[i, :]\n",
    "        y = Y[i]\n",
    "        total_cost += (1/M) * ( y - (m*x).sum() )**2\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appending 1's as the last column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.c_[X_train, np.ones(X_train.shape[0])]\n",
    "X_test = np.c_[X_test, np.ones(X_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    #data = np.loadtxt(\"https://files.codingninjas.in/data-6984.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 200\n",
    "    \n",
    "    # here we adding columns with all 1's for Generic Case.\n",
    "    X = X_train\n",
    "    Y = Y_train\n",
    "    #for i in X:\n",
    "    #    i = np.append(i, 1.0)\n",
    "\n",
    "    m = gd(X, Y, learning_rate, num_iterations)\n",
    "    print(m, m[-1])# As last element of m is c.\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  378.35572672642104\n",
      "1 Cost:  249.3339853074587\n",
      "2 Cost:  168.18064602568006\n",
      "3 Cost:  116.51108705938672\n",
      "4 Cost:  83.5268938668361\n",
      "5 Cost:  62.43178781146188\n",
      "6 Cost:  48.91321230850231\n",
      "7 Cost:  40.229121251895535\n",
      "8 Cost:  34.633779076997804\n",
      "9 Cost:  31.01450116362529\n",
      "10 Cost:  28.66134900347289\n",
      "11 Cost:  27.12089258698708\n",
      "12 Cost:  26.103232708850495\n",
      "13 Cost:  25.422819523928396\n",
      "14 Cost:  24.96074578534836\n",
      "15 Cost:  24.640701375000724\n",
      "16 Cost:  24.413627364370846\n",
      "17 Cost:  24.24791485511667\n",
      "18 Cost:  24.123140652437204\n",
      "19 Cost:  24.026060576714713\n",
      "20 Cost:  23.94804482906812\n",
      "21 Cost:  23.883435043216533\n",
      "22 Cost:  23.828490799671766\n",
      "23 Cost:  23.780713370769405\n",
      "24 Cost:  23.738411042253286\n",
      "25 Cost:  23.700419255983824\n",
      "26 Cost:  23.665920059827002\n",
      "27 Cost:  23.634325321129598\n",
      "28 Cost:  23.605200932045\n",
      "29 Cost:  23.57821740714736\n",
      "30 Cost:  23.553117505483016\n",
      "31 Cost:  23.52969486031229\n",
      "32 Cost:  23.507779747603287\n",
      "33 Cost:  23.487229501868775\n",
      "34 Cost:  23.46792197214639\n",
      "35 Cost:  23.449750979014667\n",
      "36 Cost:  23.432623098950792\n",
      "37 Cost:  23.416455337706097\n",
      "38 Cost:  23.40117340624745\n",
      "39 Cost:  23.386710411014707\n",
      "40 Cost:  23.373005833916185\n",
      "41 Cost:  23.360004718903998\n",
      "42 Cost:  23.347657009028712\n",
      "43 Cost:  23.335916995640282\n",
      "44 Cost:  23.324742853147733\n",
      "45 Cost:  23.31409624057173\n",
      "46 Cost:  23.303941956388968\n",
      "47 Cost:  23.294247636748878\n",
      "48 Cost:  23.284983489614735\n",
      "49 Cost:  23.276122059113046\n",
      "50 Cost:  23.26763801561182\n",
      "51 Cost:  23.259507967946405\n",
      "52 Cost:  23.251710294878546\n",
      "53 Cost:  23.24422499337928\n",
      "54 Cost:  23.23703354171708\n",
      "55 Cost:  23.230118775639674\n",
      "56 Cost:  23.223464776184567\n",
      "57 Cost:  23.217056767853993\n",
      "58 Cost:  23.210881026056285\n",
      "59 Cost:  23.20492479285378\n",
      "60 Cost:  23.19917620017376\n",
      "61 Cost:  23.19362419974082\n",
      "62 Cost:  23.188258499071217\n",
      "63 Cost:  23.183069502947006\n",
      "64 Cost:  23.178048259849124\n",
      "65 Cost:  23.173186412886174\n",
      "66 Cost:  23.16847615480375\n",
      "67 Cost:  23.163910186702427\n",
      "68 Cost:  23.159481680130384\n",
      "69 Cost:  23.155184242250016\n",
      "70 Cost:  23.151011883807747\n",
      "71 Cost:  23.146958989662508\n",
      "72 Cost:  23.14302029165198\n",
      "73 Cost:  23.139190843596058\n",
      "74 Cost:  23.135465998257114\n",
      "75 Cost:  23.131841386091434\n",
      "76 Cost:  23.128312895643212\n",
      "77 Cost:  23.124876655444574\n",
      "78 Cost:  23.121529017297963\n",
      "79 Cost:  23.118266540828447\n",
      "80 Cost:  23.115085979202387\n",
      "81 Cost:  23.111984265918913\n",
      "82 Cost:  23.108958502588493\n",
      "83 Cost:  23.1060059476197\n",
      "84 Cost:  23.103124005742472\n",
      "85 Cost:  23.1003102183025\n",
      "86 Cost:  23.097562254265693\n",
      "87 Cost:  23.094877901878206\n",
      "88 Cost:  23.092255060930604\n",
      "89 Cost:  23.089691735580423\n",
      "90 Cost:  23.087186027689334\n",
      "91 Cost:  23.08473613063678\n",
      "92 Cost:  23.0823403235728\n",
      "93 Cost:  23.07999696607766\n",
      "94 Cost:  23.07770449319716\n",
      "95 Cost:  23.075461410825906\n",
      "96 Cost:  23.073266291411855\n",
      "97 Cost:  23.071117769959205\n",
      "98 Cost:  23.069014540306682\n",
      "99 Cost:  23.066955351661438\n",
      "100 Cost:  23.064939005369588\n",
      "101 Cost:  23.062964351905965\n",
      "102 Cost:  23.06103028806752\n",
      "103 Cost:  23.059135754355\n",
      "104 Cost:  23.057279732529885\n",
      "105 Cost:  23.055461243333422\n",
      "106 Cost:  23.053679344356613\n",
      "107 Cost:  23.05193312804981\n",
      "108 Cost:  23.050221719862588\n",
      "109 Cost:  23.048544276504135\n",
      "110 Cost:  23.046899984315946\n",
      "111 Cost:  23.045288057748646\n",
      "112 Cost:  23.0437077379362\n",
      "113 Cost:  23.042158291359733\n",
      "114 Cost:  23.0406390085957\n",
      "115 Cost:  23.039149203142063\n",
      "116 Cost:  23.037688210317075\n",
      "117 Cost:  23.03625538622583\n",
      "118 Cost:  23.034850106789904\n",
      "119 Cost:  23.033471766835685\n",
      "120 Cost:  23.032119779237398\n",
      "121 Cost:  23.03079357411156\n",
      "122 Cost:  23.02949259805845\n",
      "123 Cost:  23.02821631344856\n",
      "124 Cost:  23.02696419775012\n",
      "125 Cost:  23.025735742895513\n",
      "126 Cost:  23.024530454683763\n",
      "127 Cost:  23.02334785221673\n",
      "128 Cost:  23.022187467366965\n",
      "129 Cost:  23.021048844274898\n",
      "130 Cost:  23.0199315388736\n",
      "131 Cost:  23.01883511843952\n",
      "132 Cost:  23.01775916116707\n",
      "133 Cost:  23.016703255766\n",
      "134 Cost:  23.01566700107975\n",
      "135 Cost:  23.014650005723823\n",
      "136 Cost:  23.013651887742327\n",
      "137 Cost:  23.012672274282455\n",
      "138 Cost:  23.011710801284913\n",
      "139 Cost:  23.010767113189562\n",
      "140 Cost:  23.009840862655796\n",
      "141 Cost:  23.0089317102962\n",
      "142 Cost:  23.0080393244227\n",
      "143 Cost:  23.007163380805107\n",
      "144 Cost:  23.006303562440255\n",
      "145 Cost:  23.005459559332262\n",
      "146 Cost:  23.004631068282137\n",
      "147 Cost:  23.00381779268734\n",
      "148 Cost:  23.003019442349395\n",
      "149 Cost:  23.002235733290437\n",
      "150 Cost:  23.001466387577278\n",
      "151 Cost:  23.000711133152826\n",
      "152 Cost:  22.999969703674466\n",
      "153 Cost:  22.999241838359083\n",
      "154 Cost:  22.99852728183435\n",
      "155 Cost:  22.997825783995506\n",
      "156 Cost:  22.997137099868233\n",
      "157 Cost:  22.99646098947639\n",
      "158 Cost:  22.995797217714724\n",
      "159 Cost:  22.995145554226514\n",
      "160 Cost:  22.99450577328559\n",
      "161 Cost:  22.993877653682418\n",
      "162 Cost:  22.993260978614458\n",
      "163 Cost:  22.992655535580074\n",
      "164 Cost:  22.992061116276403\n",
      "165 Cost:  22.99147751650034\n",
      "166 Cost:  22.990904536052955\n",
      "167 Cost:  22.99034197864715\n",
      "168 Cost:  22.989789651817958\n",
      "169 Cost:  22.989247366836103\n",
      "170 Cost:  22.988714938623907\n",
      "171 Cost:  22.988192185673817\n",
      "172 Cost:  22.987678929969835\n",
      "173 Cost:  22.987174996910603\n",
      "174 Cost:  22.986680215235346\n",
      "175 Cost:  22.98619441695148\n",
      "176 Cost:  22.985717437264817\n",
      "177 Cost:  22.98524911451124\n",
      "178 Cost:  22.984789290090543\n",
      "179 Cost:  22.98433780840204\n",
      "180 Cost:  22.983894516781987\n",
      "181 Cost:  22.983459265442395\n",
      "182 Cost:  22.98303190741186\n",
      "183 Cost:  22.98261229847767\n",
      "184 Cost:  22.982200297129395\n",
      "185 Cost:  22.9817957645041\n",
      "186 Cost:  22.981398564332792\n",
      "187 Cost:  22.981008562888082\n",
      "188 Cost:  22.980625628933495\n",
      "189 Cost:  22.98024963367371\n",
      "190 Cost:  22.97988045070594\n",
      "191 Cost:  22.979517955972923\n",
      "192 Cost:  22.979162027716256\n",
      "193 Cost:  22.978812546431808\n",
      "194 Cost:  22.978469394825115\n",
      "195 Cost:  22.97813245776875\n",
      "196 Cost:  22.977801622259822\n",
      "197 Cost:  22.97747677737913\n",
      "198 Cost:  22.977157814250823\n",
      "199 Cost:  22.97684462600292\n",
      "[-0.67227416  1.15654561 -0.30096127  0.76868634 -1.38569184  2.24423026\n",
      "  0.26346439 -3.20143792  2.64240392 -1.90299965 -2.07091435  1.11916978\n",
      " -4.4860789  22.78970976] 22.789709762533008\n"
     ]
    }
   ],
   "source": [
    "m = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=np.array((m*X_test).sum(axis=1))\n",
    "# as we took last feature as 1, (assumed as c), we dont have to add c at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(Y_test, Y_pred, c='crimson')\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "\n",
    "p1 = max(max(Y_pred), max(Y_test))\n",
    "p2 = min(min(Y_pred), min(Y_test))\n",
    "plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "plt.xlabel('True Values', fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# The blue line made by p1 and p2 is a line that represents perfect regression. \n",
    "# The closer the points are to the line, the more accurate the model is. It can also be used to understand if the model is under or over predicting. \n",
    "# If points are above the blue line, the model is over predicting while if the points are below the blue line, the model is under predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting errors\n",
    "g=plt.plot(Y_test - Y_pred,marker='o',linestyle='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Score using **Coefficient of Determination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7688680487427053"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = r2_score(Y_test, Y_pred)\n",
    "r2\n",
    "# Seems like a good score but can be improved by changing lr and iterations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of MultiVariable Regression and Gradient Descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
