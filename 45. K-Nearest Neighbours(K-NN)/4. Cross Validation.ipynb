{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By cross validation,\n",
    "what we mean is lets say there is an algorithm, and we have this classifier, which we want to train.  \n",
    "And lets say there is some parameter p that we need to adjust.  \n",
    "We need to choose the value for p.  \n",
    "So how do we decide p?  \n",
    "So we can look at the score of trainng data itself, passing the training dataand we can see  how well this classifier is working and choose our p accordingly.  \n",
    "The problem with that is it might lead to overfitting because we might choose a value of p for which the training error is very less.  \n",
    "But thats not what we want.  \n",
    "The other option is why dont we pass the testing data to see what is the best value of p.  \n",
    "The problem with that approach is if we are going to use our testing data to decide the value of p, that basically means that you are using your testing data to actually fit your model to dcide the parameters of your model. And thats again something that we dont want to do we want to make sure that your testing data is just for testing.  \n",
    "So we are stuck with the fact that using testing kind of violates the basic rules of whatever we haeve learned that we want to test on the test data to figure out how good or bad we are performing, and the problem with doing it on training data is putting that p to a value which will basically lead to a low training error, but high testing error.  \n",
    "So what we do is we use CROSS VALIDATION.  \n",
    "It means we have our training data, lets split the training data into k parts, lets train our classifier using k-1 parts of the data, and test on the remaining part. In this we have split the training data, in this we have not used the test data anywhere. So first we do it on k-1 and then test it on last element and then we do this for all k subsets.  \n",
    "So we run this k times,where we are using k-1 to train, and the rest 1 which is left for testing and then we are doing this k times. And then we can find out overall average score by doing this operation.   \n",
    "So we can try doing this again and again, for different values of p, and the overall score will be, lets say p = 1, and we run cross validation 10 times if our k is 10, so if we run it 10 times, we will get 10 scores, and we will take the average of these scores.  \n",
    "Similarly we can try it for p = 2, p = 3, and this way we can figure it out the best value of p at which we are getting the best score.  \n",
    "\n",
    "Now how it is done in sklearn, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn  import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(iris.data, iris.target, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## so we are using cross_val_score, \n",
    "## estimator = is the classifier that we want to fit on variuos types.\n",
    "## group = if we by ourself pass what group each data belongs to for eg, we want to make 3 groups and training data is x, \n",
    "##         so lets say the 0th row goes to group a, 2nd to b, 3rd to a, 4th to c, and so on. Then we can pass array of size of \n",
    "##         number of samplesand pass which datapoint belongs to which group.  \n",
    "##         We will not use this generally, we want it to be a randomshufffle but in case we want to do it, we willdecide how to \n",
    "##         make this k subsets.  \n",
    "## cv = cv is how exactly we make that split how exactly we generate the cross validation subsets.  \n",
    "##      Most standard one is k fold we will use that.\n",
    "## njobs = It means if we want to do some parallel processing, we can do it if our estimators fit function requiressome extra \n",
    "##         parameters.\n",
    "## fit_parms = If we want to pass some extra parameters apart from x and y we use this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use linear regression as fit for now.\n",
    "clf = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89427614, 0.87888815, 0.92393884, 0.95307551, 0.92710904])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we now pass training data and testing data. \n",
    "cross_val_score(clf, xtrain, ytrain)\n",
    "\n",
    "## this is the score. It used 5 folds.It splitted data into 5 parts and we take avg of these score to see how good our \n",
    "## classifier is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.85124923, 0.        , 0.76155439, 0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  one thing we can do is we can pass cv parameter in cross_va_score and pass the kfold object and we can do what we want.\n",
    "## we can check the list of things we can do with kfold documentation.\n",
    "## insttead of passing the training data, we can pass whole iris data and see what happens\n",
    "cross_val_score(clf, iris.data, iris.target)\n",
    "\n",
    "## we can see the score is zero in 3 places. \n",
    "## this is happening coz iris has 150 datasets the forst 50 belongs to class 0, second 50 belongs to class 1 and last 50 to \n",
    "## class 2.\n",
    "## And that is how the data is split.  \n",
    "## If we use 3 folds with shuffle = False, that means the first time we trained, we trained first 100 and tested on last 50.\n",
    "## So obviously we get the score to be 0. \n",
    "## In our training data we dont have any object for class 2, but we have class 2 on training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.90322728, 0.92527008, 0.94214064])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## in this case we use cv object. \n",
    "from sklearn.model_selection import KFold\n",
    "cross_val_score(clf, iris.data, iris.target, cv = KFold(3, True, 0 ))\n",
    "## Here we put the no of splits to 3, shuffle is True and random_state = 0.\n",
    "\n",
    "## Now we get the good score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
