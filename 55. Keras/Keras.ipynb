{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF-T-YCPBpaN"
   },
   "source": [
    "#**Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rapt (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ltk (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rapt (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ltk (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs9RlcetCZqW"
   },
   "source": [
    "**What will you learn?**\n",
    "1. **Principles of Keras**\n",
    "2. **Why to use Keras**\n",
    "3. **Keras vs TensorFlow**\n",
    "4. **Flow of Code**\n",
    "5. **Types of Model in Keras** : Sequential, Functional API\n",
    "6. **Implementation** : Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rONhqMDoBpfI"
   },
   "source": [
    "Keras is a powerful and easy-to-use free open source Python library for developing and evaluating deep learning models.\n",
    "\n",
    "It wraps the efficient numerical computation libraries Theano and TensorFlow and allows you to define and train neural network models in just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY0HrW03Rb2g"
   },
   "source": [
    "##**Principles of Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6jQksB2RYyN"
   },
   "source": [
    "Keras was created to be user friendly, modular, easy to extend, and to work with Python. The API was “designed for human beings, not machines,” and “follows best practices for reducing cognitive load.”\n",
    "\n",
    "Neural layers, cost functions, optimizers, initialization schemes, activation functions, and regularization schemes are all standalone modules that you can combine to create new models. New modules are simple to add, as new classes and functions. Models are defined in Python code, not separate model configuration files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDvyYL6eRjUd"
   },
   "source": [
    "##**Why do we use Keras?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY-YHqQDRl9b"
   },
   "source": [
    "The biggest reasons to use Keras stem from its guiding principles, primarily the one about being user friendly. Beyond ease of learning and ease of model building, Keras offers the advantages of broad adoption, support for a wide range of production deployment options, integration with at least five back-end engines (TensorFlow, CNTK, Theano, MXNet, and PlaidML), and strong support for multiple GPUs and distributed training. Plus, Keras is backed by Google, Microsoft, Amazon, Apple, Nvidia, Uber, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz1yurAZRxjo"
   },
   "source": [
    "##**Keras vs Tensorflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUcDJQgNR70v"
   },
   "source": [
    "Parameters | Keras | TensorFlow \n",
    ":---|:---|:---\n",
    "Type|High-Level API Wrapper|Low-Level API\n",
    "Complexity|Easy to use if you Python language|You need to learn the syntax of using some of Tensorflow function\n",
    "Purpose|Rapid deployment for making model with standard layers|Allows you to make an arbitrary computational graph or model layers\n",
    "Tools|Uses other API debug tool such as TFDBG|You can use Tensorboard visualization tools\n",
    "Community|Large active communities|Large active communities and widely shared resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ1Ed7p8SeOo"
   },
   "source": [
    "# 2. Flow of Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIs7Y_p-SeMI"
   },
   "source": [
    "1. Creating a model.  $->$ Like creating a Neural Network.\n",
    "2. Defining Architecture. $->$ Defining how many layers, no of units in each layer, activation function etc.\n",
    "3. Compile the model. $->$ This will create all the weights that is required once we defined the architecture. We will provide info abt optimizer in this layer.\n",
    "4. Fit the model. $->$ calls the fit function. may take batch_size in this. \n",
    "5. Evaluaute the model. $->$ Calls the score function to find accuracy.\n",
    "\n",
    "At different levels, we will be providing different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YkbXyjZSeJj"
   },
   "source": [
    "# 3. Types of Keras Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LExm2NJFSeHJ"
   },
   "source": [
    "There are three ways to create Keras models:\n",
    "\n",
    "**The Sequential model**, which is very straightforward (a simple list of layers), but is limited to single-input, single-output stacks of layers (as the name gives away).\n",
    "\n",
    "**The Functional API**, which is an easy-to-use, fully-featured API that supports arbitrary model architectures. For most people and most use cases, this is what you should be using. This is the Keras \"industry strength\" model.\n",
    "\n",
    "**Model subclassing**, where you implement everything from scratch on your own. Use this if you have complex, out-of-the-box research use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# thats it it will create a model for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using example of Breast Cancer Dataset.  \n",
    "In case of Breast Cancer, we have 30 inputs comming into the system.  \n",
    "So each datapoint have 30 inputs.  \n",
    "Lets have one hidden layer of 32 units, and hidden layer 2 will be of 32 units activation function will be relu in both.  \n",
    "And we will have one output unit. Activation function will be sigmoid here.  \n",
    "While adding layer,we obviously need to define the units its going to have.  \n",
    "So the dimension between h1 and h2 will be 32 x 16, and between h2 and output will be 16 x 1 and between input and h1, if we dont know what is the shape of input coming in, we cannot tell what is the dimension of weight we using here, so just for the input layer, we need to supply something called 'input_shape'.  \n",
    "So we need to provide units, we need to provide activation function, and 'input_shape'( just for the first layer) and there are so much argument we need to supply if we want.  \n",
    "So we need to define if we want the bias unit or not using use_bias.  \n",
    "Then we can define if we want to use any regularization or not.  \n",
    "We can also define if we want to initialize any weights or not.  \n",
    "We can also supply initializers if we want.  \n",
    "We can aslo supply constraints if we want for eg, my biases cant be negative, weights cant be negative etc.  \n",
    "Default Nerual network layer is called as dense layer.  \n",
    "We will see more layers soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "# we need to import dense layer\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dense layer in the model\n",
    "# for that we need to define no of units, activation function and dimensions.\n",
    "layer1 = Dense(units = 32, activation = 'relu', input_dim = 30) \n",
    "model.add(layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also add the layers without creating variable of it.\n",
    "model.add(Dense(units =16, activation= 'relu')) # layer2  \n",
    "model.add(Dense(units =1, activation= 'sigmoid')) #output layer\n",
    "\n",
    "# we added only one unit coz cancer dataset is binary classificaton, classes were only 2, in the previous mnist case, classes\n",
    "# were 10 so we added units as 10 in it.\n",
    "# One thing to play around is with activation function and units we want to use and no of layers we want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compiling the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we added the layers, we left with three simple tasks, compiling, which is about telling the Model which optimizer to use, what cost function, what loss function to use and which metrics to compute to any kind of evaluation.  \n",
    "Once we done with compiling, we just need to fit the data, and we need to evaluate the data.  \n",
    "While compiling, we need to provide three main things,  \n",
    "1. Which optimizer to use.  \n",
    "2. Which loss function to use.  \n",
    "3. Metrics. It can print some metrics while running the data, lets say cost function or accuracy. For eg, in case of breast cancer, we will be using accuracy, or if it is Regression Problem, we can be loooking at sum of squares differeces, to figure out how far we are from actual truth. So we can provide list to it and say what metrics we want to print when it is fitting the data as well as it is evaluating the data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) # loss function is binary in this case, \n",
    "                                                                                        #in other cases, check docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only thing remaining is Load the data and call the fit function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fitting Training Data Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are familiar with the fit function as it takes X and Y.  \n",
    "Along with the x and y, we can pass Epochs, its not really required, but we will send this.  \n",
    "1. **Epochs** : are actually no of times we want to run the iterations the no of times we want to run the forward propogation and the backward propogation.  \n",
    "By default, epochs is 1.  \n",
    "We will provide some epochs whatever no of iterations we want.  \n",
    "2. **Batch size** :  if we want to update the weights in mini batches, in that case, we can use the batch size. Default is 32. Lets say if we take batch_size to be 40 and we have 1000 datapoints and if we have epochs 40, it will go all of your data 40 times but its not going to train your network during each iterations not all 1000, means in first iteration, its going to go through complete data in a batch of 50, so its going to pick first 50, updates the weights, next 50, update the weights,  ans so  on thats how it will go through all the data and then it will go to the next iteration it will do the same thing again.  \n",
    "3. **Validation_data** : Another argument we can use is validation_data,it is if we provide our testing data, we can provide our testing data as well and after every iteration, it will tell you what is the accuracy looking like validation data. Its not going to use validation data to train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before fit, we need to import the breast cancer data.  \n",
    "\n",
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# one more thing we need to do is we need to scale the data.  \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "# now we can use this data to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 997us/step - loss: 0.4464 - accuracy: 0.8791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e07ea576d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "# if we run this, it will run for only one epoch.  \n",
    "\n",
    "# and it is giving us pretty bad accuracy because we run for only one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "15/15 [==============================] - 0s 999us/step - loss: 0.3208 - accuracy: 0.9297\n",
      "Epoch 2/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2456 - accuracy: 0.9429\n",
      "Epoch 3/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9538\n",
      "Epoch 4/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9582\n",
      "Epoch 5/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9582\n",
      "Epoch 6/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9648\n",
      "Epoch 7/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9692\n",
      "Epoch 8/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9714\n",
      "Epoch 9/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9758\n",
      "Epoch 10/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9780\n",
      "Epoch 11/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9802\n",
      "Epoch 12/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9824\n",
      "Epoch 13/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9802\n",
      "Epoch 14/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9802\n",
      "Epoch 15/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9802\n",
      "Epoch 16/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.9802\n",
      "Epoch 17/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9824\n",
      "Epoch 18/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9846\n",
      "Epoch 19/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.9868\n",
      "Epoch 20/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 0.9868\n",
      "Epoch 21/40\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0536 - accuracy: 0.9868\n",
      "Epoch 22/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0508 - accuracy: 0.9868\n",
      "Epoch 23/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9846\n",
      "Epoch 24/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9846\n",
      "Epoch 25/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.9868\n",
      "Epoch 26/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9890\n",
      "Epoch 27/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0441 - accuracy: 0.9890\n",
      "Epoch 28/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9912\n",
      "Epoch 29/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9890\n",
      "Epoch 30/40\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0405 - accuracy: 0.9912\n",
      "Epoch 31/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0398 - accuracy: 0.9912\n",
      "Epoch 32/40\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.9912\n",
      "Epoch 33/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9912\n",
      "Epoch 34/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0361 - accuracy: 0.9912\n",
      "Epoch 35/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9912\n",
      "Epoch 36/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9912\n",
      "Epoch 37/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0334 - accuracy: 0.9912\n",
      "Epoch 38/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0325 - accuracy: 0.9912\n",
      "Epoch 39/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0317 - accuracy: 0.9912\n",
      "Epoch 40/40\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0307 - accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e07fb022b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets run for 40 epochs\n",
    "model.fit(x_train, y_train, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.9912\n",
      "Epoch 2/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0293 - accuracy: 0.9912\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0287 - accuracy: 0.9912\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9912\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0276 - accuracy: 0.9912\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0271 - accuracy: 0.9912\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9912\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0259 - accuracy: 0.9912\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0254 - accuracy: 0.9912\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0246 - accuracy: 0.9912\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0242 - accuracy: 0.9912\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 0.9912\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9912\n",
      "Epoch 14/40\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.9912\n",
      "Epoch 15/40\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.9912\n",
      "Epoch 16/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.9912\n",
      "Epoch 17/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9912\n",
      "Epoch 18/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.9912\n",
      "Epoch 19/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9912\n",
      "Epoch 20/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9934\n",
      "Epoch 21/40\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.9934\n",
      "Epoch 22/40\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.9934\n",
      "Epoch 23/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0187 - accuracy: 0.9934\n",
      "Epoch 24/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0184 - accuracy: 0.9934\n",
      "Epoch 25/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0184 - accuracy: 0.9956\n",
      "Epoch 26/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9956\n",
      "Epoch 27/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9956\n",
      "Epoch 28/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0171 - accuracy: 0.9956\n",
      "Epoch 29/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0164 - accuracy: 0.9956\n",
      "Epoch 30/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0160 - accuracy: 0.9956\n",
      "Epoch 31/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 0.9956\n",
      "Epoch 32/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 0.9956\n",
      "Epoch 33/40\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9956\n",
      "Epoch 34/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0145 - accuracy: 0.9956\n",
      "Epoch 35/40\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9956\n",
      "Epoch 36/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 37/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 38/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0134 - accuracy: 0.9956\n",
      "Epoch 39/40\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9956\n",
      "Epoch 40/40\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0128 - accuracy: 0.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e07fb2ea00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another argument is batch size, lets take batch size to be 40\n",
    "# to run this, it is recommended to run all above again. \n",
    "# because the weights are random, we optimized it, weights have reached w'. \n",
    "# So if we run it again, we will improve further on  w'. \n",
    "# SO if we run again on w'', it will be much better. \n",
    "model.fit(x_train, y_train, epochs = 40, batch_size = 50)  \n",
    "\n",
    "# its getting better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.1333 - val_accuracy: 0.9561\n",
      "Epoch 2/40\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.1327 - val_accuracy: 0.9649\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9956 - val_loss: 0.1335 - val_accuracy: 0.9561\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9956 - val_loss: 0.1345 - val_accuracy: 0.9561\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9956 - val_loss: 0.1358 - val_accuracy: 0.9561\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9956 - val_loss: 0.1355 - val_accuracy: 0.9561\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.9956 - val_loss: 0.1369 - val_accuracy: 0.9561\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9956 - val_loss: 0.1367 - val_accuracy: 0.9561\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9956 - val_loss: 0.1370 - val_accuracy: 0.9561\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9978 - val_loss: 0.1392 - val_accuracy: 0.9561\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9978 - val_loss: 0.1396 - val_accuracy: 0.9561\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9978 - val_loss: 0.1397 - val_accuracy: 0.9561\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9978 - val_loss: 0.1391 - val_accuracy: 0.9561\n",
      "Epoch 14/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9978 - val_loss: 0.1427 - val_accuracy: 0.9561\n",
      "Epoch 15/40\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 0.9978 - val_loss: 0.1491 - val_accuracy: 0.9561\n",
      "Epoch 16/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9978 - val_loss: 0.1496 - val_accuracy: 0.9561\n",
      "Epoch 17/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 0.1475 - val_accuracy: 0.9561\n",
      "Epoch 18/40\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.1477 - val_accuracy: 0.9561\n",
      "Epoch 19/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9978 - val_loss: 0.1452 - val_accuracy: 0.9561\n",
      "Epoch 20/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9978 - val_loss: 0.1444 - val_accuracy: 0.9561\n",
      "Epoch 21/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9978 - val_loss: 0.1432 - val_accuracy: 0.9561\n",
      "Epoch 22/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.1445 - val_accuracy: 0.9561\n",
      "Epoch 23/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.1453 - val_accuracy: 0.9561\n",
      "Epoch 24/40\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.1436 - val_accuracy: 0.9561\n",
      "Epoch 25/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.1424 - val_accuracy: 0.9561\n",
      "Epoch 26/40\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.1415 - val_accuracy: 0.9561\n",
      "Epoch 27/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.1413 - val_accuracy: 0.9561\n",
      "Epoch 28/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.9561\n",
      "Epoch 29/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.9561\n",
      "Epoch 30/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.9561\n",
      "Epoch 31/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.9561\n",
      "Epoch 32/40\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.1393 - val_accuracy: 0.9561\n",
      "Epoch 33/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9978 - val_loss: 0.1321 - val_accuracy: 0.9561\n",
      "Epoch 34/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9978 - val_loss: 0.1332 - val_accuracy: 0.9561\n",
      "Epoch 35/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.1357 - val_accuracy: 0.9561\n",
      "Epoch 36/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.1389 - val_accuracy: 0.9561\n",
      "Epoch 37/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.1414 - val_accuracy: 0.9561\n",
      "Epoch 38/40\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0055 - accuracy: 0.9978 - val_loss: 0.1416 - val_accuracy: 0.9561\n",
      "Epoch 39/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.1424 - val_accuracy: 0.9561\n",
      "Epoch 40/40\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e07fb4fd90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last argument we need to see is validation_data\n",
    "# now, if we see by definition, it gives loss and accuracy. \n",
    "# lets add test data in validation\n",
    "\n",
    "model.fit(x_train, y_train, epochs = 40, batch_size = 50, validation_data=(x_test, y_test))  \n",
    "\n",
    "# now if we can see, it is giving accuracy and loss on training and testing as well.  \n",
    "# we can experiment on epochs, and if we think we are performing training and testing data as well, we need to do some \n",
    "# regularization as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluations & Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at two functions\n",
    "1. Predict\n",
    "2. Evaluate $->$ We will provide test data and it will give me the score.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions  = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.7434914e-09],\n",
       "       [9.5958257e-01],\n",
       "       [9.9996531e-01],\n",
       "       [9.9367452e-01],\n",
       "       [9.9999964e-01],\n",
       "       [9.9999356e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9999011e-01],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [8.9750123e-01],\n",
       "       [9.8825741e-01],\n",
       "       [1.0000000e+00],\n",
       "       [1.2719193e-01],\n",
       "       [1.4198068e-01],\n",
       "       [3.1158396e-09],\n",
       "       [1.0000000e+00],\n",
       "       [7.7914292e-15],\n",
       "       [1.5112675e-11],\n",
       "       [1.1094950e-22],\n",
       "       [3.2591185e-09],\n",
       "       [3.1567275e-08],\n",
       "       [9.9987900e-01],\n",
       "       [9.9996483e-01],\n",
       "       [3.0894592e-12],\n",
       "       [9.9998802e-01],\n",
       "       [9.9999928e-01],\n",
       "       [7.9507663e-05],\n",
       "       [9.9994421e-01],\n",
       "       [5.4122600e-16],\n",
       "       [9.9999833e-01],\n",
       "       [1.6846581e-11],\n",
       "       [9.9626100e-01],\n",
       "       [1.2133746e-07],\n",
       "       [1.0000000e+00],\n",
       "       [6.0351432e-08],\n",
       "       [9.9632049e-01],\n",
       "       [9.3715080e-09],\n",
       "       [9.9983096e-01],\n",
       "       [9.1489536e-09],\n",
       "       [7.1719289e-04],\n",
       "       [1.0000000e+00],\n",
       "       [8.6790323e-04],\n",
       "       [9.9999952e-01],\n",
       "       [9.9983007e-01],\n",
       "       [5.4858948e-20],\n",
       "       [1.0000000e+00],\n",
       "       [9.9577832e-01],\n",
       "       [9.9999982e-01],\n",
       "       [7.8298880e-13],\n",
       "       [5.8718569e-10],\n",
       "       [2.3834109e-03],\n",
       "       [6.8728283e-12],\n",
       "       [9.9999213e-01],\n",
       "       [9.9999833e-01],\n",
       "       [9.9999863e-01],\n",
       "       [9.9995124e-01],\n",
       "       [9.9976140e-01],\n",
       "       [9.8521543e-01],\n",
       "       [4.7415963e-22],\n",
       "       [2.2073446e-07],\n",
       "       [1.8143724e-07],\n",
       "       [1.0000000e+00],\n",
       "       [9.9998558e-01],\n",
       "       [1.8009678e-10],\n",
       "       [9.8471260e-01],\n",
       "       [4.6476203e-35],\n",
       "       [9.0575828e-14],\n",
       "       [4.1194905e-15],\n",
       "       [9.9999881e-01],\n",
       "       [3.2466039e-01],\n",
       "       [8.3137309e-15],\n",
       "       [9.9996793e-01],\n",
       "       [3.5877526e-03],\n",
       "       [3.4244477e-18],\n",
       "       [9.8107970e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9996543e-01],\n",
       "       [9.9999082e-01],\n",
       "       [1.0000000e+00],\n",
       "       [1.4105439e-04],\n",
       "       [2.7332247e-29],\n",
       "       [5.9027635e-16],\n",
       "       [1.0000000e+00],\n",
       "       [3.8985308e-07],\n",
       "       [9.9999857e-01],\n",
       "       [9.9999964e-01],\n",
       "       [1.0000000e+00],\n",
       "       [3.6518604e-11],\n",
       "       [2.0795817e-29],\n",
       "       [1.0000000e+00],\n",
       "       [8.9577138e-03],\n",
       "       [2.2257343e-01],\n",
       "       [9.7837096e-17],\n",
       "       [9.9999583e-01],\n",
       "       [9.9999613e-01],\n",
       "       [6.3558774e-16],\n",
       "       [9.9995309e-01],\n",
       "       [9.9988407e-01],\n",
       "       [9.9999464e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9948120e-01],\n",
       "       [9.9984169e-01],\n",
       "       [9.9957097e-01],\n",
       "       [4.2902446e-13],\n",
       "       [1.0000000e+00],\n",
       "       [4.6552678e-20],\n",
       "       [9.8494309e-01],\n",
       "       [1.1141908e-01],\n",
       "       [9.9989533e-01],\n",
       "       [9.9960458e-01],\n",
       "       [3.9208245e-16],\n",
       "       [9.4291189e-12],\n",
       "       [9.1480732e-01]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n",
    "# this is the output of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1428 - accuracy: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1427806317806244, 0.9561403393745422]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can find the score by using model.evaluate\n",
    "score = model.evaluate(x_test, y_test)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whatever metrics we provide on compile, evaluate will give you that.\n",
    "Evaluate is providing us loss as well as all the metrics that we will provide while compiling our model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZloCIbfQSeDl"
   },
   "source": [
    "##**Lets use Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FtC0694Sd2R"
   },
   "source": [
    "**Step 1** : Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ISDQJUNVJFV"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HixGaogXVdIp"
   },
   "source": [
    "**Step 2** : Define Keras Model\n",
    "\n",
    "Currently we will use Dense layers. While defining the model, we need to know the number of units we want to keep in each layer, their activation function, and input dimensions for the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZS05ArJZVcRz"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlihJJmBVjDl"
   },
   "outputs": [],
   "source": [
    "# Creating a Model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VddLs3tzVi7_"
   },
   "outputs": [],
   "source": [
    "layer1 = Dense(units=32, activation = 'relu', input_dim = 30)\n",
    "model.add(layer1)\n",
    "model.add(Dense(units=16, activation = 'relu'))\n",
    "model.add(Dense(units=1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPAHlTOzV33U"
   },
   "source": [
    "**Step 3** : Compile the Model\n",
    "\n",
    "Here, we define the optimizer we want to use, along with the loss funtion and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT4AjOciVi5x"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgEs5Z-NWJe2"
   },
   "source": [
    "**Step 4** : Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3837,
     "status": "ok",
     "timestamp": 1611608423774,
     "user": {
      "displayName": "Gaurav Bhatia",
      "photoUrl": "",
      "userId": "05517600112429710610"
     },
     "user_tz": -330
    },
    "id": "oenTrhh3WGUA",
    "outputId": "121b46f0-2491-430e-d708-b6ff4acfd704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.6454 - accuracy: 0.6503 - val_loss: 0.5250 - val_accuracy: 0.8509\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.5133 - accuracy: 0.8480 - val_loss: 0.4218 - val_accuracy: 0.9035\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.4052 - accuracy: 0.9337 - val_loss: 0.3519 - val_accuracy: 0.9035\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3410 - accuracy: 0.9419 - val_loss: 0.2990 - val_accuracy: 0.9123\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.2850 - accuracy: 0.9436 - val_loss: 0.2575 - val_accuracy: 0.9211\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2423 - accuracy: 0.9511 - val_loss: 0.2246 - val_accuracy: 0.9211\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2027 - accuracy: 0.9516 - val_loss: 0.1994 - val_accuracy: 0.9211\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1752 - accuracy: 0.9595 - val_loss: 0.1787 - val_accuracy: 0.9211\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1683 - accuracy: 0.9607 - val_loss: 0.1619 - val_accuracy: 0.9211\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1493 - accuracy: 0.9568 - val_loss: 0.1483 - val_accuracy: 0.9386\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1370 - accuracy: 0.9592 - val_loss: 0.1371 - val_accuracy: 0.9386\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1027 - accuracy: 0.9753 - val_loss: 0.1286 - val_accuracy: 0.9474\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1065 - accuracy: 0.9649 - val_loss: 0.1204 - val_accuracy: 0.9561\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0898 - accuracy: 0.9752 - val_loss: 0.1129 - val_accuracy: 0.9561\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0945 - accuracy: 0.9784 - val_loss: 0.1062 - val_accuracy: 0.9561\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0816 - accuracy: 0.9814 - val_loss: 0.1015 - val_accuracy: 0.9561\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0920 - accuracy: 0.9787 - val_loss: 0.0973 - val_accuracy: 0.9561\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0693 - accuracy: 0.9826 - val_loss: 0.0920 - val_accuracy: 0.9737\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0708 - accuracy: 0.9835 - val_loss: 0.0881 - val_accuracy: 0.9737\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0735 - accuracy: 0.9804 - val_loss: 0.0861 - val_accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f942827c550>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size = 50, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYHUnYPHWWQx"
   },
   "source": [
    "**Step 5** : Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1390,
     "status": "ok",
     "timestamp": 1611608472029,
     "user": {
      "displayName": "Gaurav Bhatia",
      "photoUrl": "",
      "userId": "05517600112429710610"
     },
     "user_tz": -330
    },
    "id": "h-yonZl0WO4n",
    "outputId": "e9ee17db-a8d6-4f53-e0dd-7272b433ce31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0861 - accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08612696081399918, 0.9736841917037964]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "score = model.evaluate(x_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "den2MI9SWups"
   },
   "source": [
    "##**Advantages of Keras**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SsI-JbDWuaO"
   },
   "source": [
    "**Fast Deployment and Easy to understand** : Keras is very quick to make a network model. If you want to make a simple network model with a few lines, Keras can help you with that.\n",
    "\n",
    "**Large Community Support** : There are lots of AI communities that use Keras for their Deep Learning framework. Many of them publish their codes as well tutorial to the general public.\n",
    "\n",
    "**Have Multiple Backends** : You can choose Tensorflow, CNTK, and Theano as your backend with Keras. You can choose a different backend for different projects depending on your needs. Each backend has its own unique advantage.\n",
    "\n",
    "**Cross-Platform and Easy Model Deployment** : With a variety of supported devices and platforms, you can deploy Keras on any device like\n",
    "\n",
    "1. iOS with CoreML\n",
    "2. Android with Tensorflow Android,\n",
    "3. Web browser with .js support\n",
    "4. Cloud engine\n",
    "5. Raspberry Pi\n",
    "\n",
    "**Multi GPUs Support** : You can train Keras with on a single GPU or use multiple GPUs at once. Because Keras has a built-in support for data parallelism so it can process large volumes of data and speed up the time needed to train it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
