{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the probblem that we want to solve now is basically finding out $$P( X = x / y = a_{i})$$, \n",
    "\n",
    "That in itself is not an easy problem to solve,  \n",
    "So X = x, what does this mean? so our input is 2 dimensional, it has few features, lets assume n features.  \n",
    "This actually means the probablity $ P((f_{1}, f_{2},f_{3}...f_{n}) = (x_{1}, x_{2}, x_{3} ... x_{n}) / y =a_{i}) $.\n",
    "Now lets come to the part where this classifier is callled $Naive$ $Bayes$ Classifier.  \n",
    "It makes a very strong assumption(Naive Assumption) that lets assume that all the features are independent of each other.  \n",
    "And for Independent events, for eg what is the probablity that A happens and B happens $P(A \\cap B)$ if A and B are independent of each other, this probablity will be $P(A)* P(B)$.  \n",
    "Thats like saying what is the probablity that I tossed a coin, get a head, and the next time I tossed a coin I get tails.  \n",
    "Both these events are independent of each other.  \n",
    "The probablity that first time I get a head and the second time I get a tail, what you can do is given these two  events are independent of each other is $P(1st = Head) * P(2nd = tail)$ = 1/2 * 1/2 which is 0.25 .  \n",
    "Similarly here if we assume that all the features are independent of each other, the probablity $P((f_{1}, f_{2},f_{3}...f_{n})$  is equal to $$ P(f_{1} = x_{1}/ y = a_{i}) * P(f_{2} = x_{2}/ y = a_{i}) * P(f_{3} = x_{3}/ y = a_{i}) ...... P(f_{n} =  x_{n}/ y = a_{i}) $$.  \n",
    "So we are assuming all the features are independent of each other and given the fact they are independent of each other, we can just multiply these probablities without worrying about if one happening tells me more about the other has happened or not.  \n",
    "So $Naive$ $Bayes$ is called $Naive$ because in reality, thats not going to be the case.  \n",
    "These features are going to be depend upon each other, most of dataset wont have property of Independence of the features that we have.  \n",
    "Thats the reason we call it Naive Bayes.  \n",
    "So, $$P( X = x / y = a_{i})  = \\prod_{j = 1}^n P(X^{j} = x^{j} / y = a_{i}) $$  \n",
    "So for $ P( X = x / y = a_{i}) *P(y=a_{i})$, \n",
    "$$P( X = x / y = a_{i}) *P(y=a_{i})  = \\prod_{j = 1}^n P(X^{j} = x^{j} / y = a_{i})*P(y=a_{i})$$  .\n",
    "So now we dont have to worry about $P( X = x / y = a_{i})$, we have to worry about Individual features to find out the final no that we want to calculate instead of all features combined.  \n",
    "We will find out abt these individual features next.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
