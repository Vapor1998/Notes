{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking to build decision tree given some data, what we want to do is we  want to build a classifier which will give us predictions.  \n",
    "The idea of Random Forests is that instead of having one we will build a huge classifier within in which we will have small classifier which are DT and we will take the majority of it to predict our final prediction.  \n",
    "How are we going to build these decision trees?  \n",
    "We use something call BAGGING.  \n",
    "Bagging is shortcut for Bootstrap aggregation Algorithm.  \n",
    "What bagging says if you have m datapoints, lets select k out of them, but we will select these k with replacement. \n",
    "If we have datapoints d1, d2, d3 and lets say we want to select 3 datapoints, in this case it is possible that we end up in something like d1, d2, d2. So in bagging we use data with replacement.  \n",
    "So in Random forest, we use bagging, we have m datapoints and we will choose m points out of these. But this is with replacement. That means some datapoint might come twice and if any datapoint is comming more than once, that means there are some datapoints which is not coming at all like in prev example.  \n",
    "We can actually built one decision tree and sometimes we call them bagged trees.  \n",
    "But in random forest what we are going to do is we are going to build a lot of trees and everytime we are going to get different data.  \n",
    "So along with bagging there is also different step that is feature selection.  \n",
    "So in FEATURE SELECTION what weare going to do is we are not going to train with all features.  \n",
    "The first DT will be trained on some feature chosen randomly from all features.  \n",
    "So this means we are adding randomness on data which are on the rows from the datapoint but also adding randomness on the features also for which column to choose. SO we choose k features out of n features from datapoint m*n.  \n",
    "So if we are training k features on one random trees, we will choose another random k features on next decision tree.  \n",
    "This is not with replacement(for features.).  \n",
    "Replacement is only for datas.  \n",
    "So for any standard number to choose for selecting number of features, it is $\\sqrt{n}$.  \n",
    "For DT, it will be a big number, generally we take 10-12 trees for Random Forest.(Depends on Problem). \n",
    "Good part abt Feature selection is lets say we have a feature which causing overfitting, so there will be a tree which might not have that feature which causes overfitting and that might reduce the overfitting.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
