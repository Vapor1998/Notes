{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we can use DT and RF for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression problems are when you have to predict a value out of continous range.  \n",
    "For eg. we have a diabetes dataset where we predict HbA1C which we take as y.  \n",
    "Now we are given some data and on the basis of that we have to predict the values of y.  \n",
    "The DT that we did is based completely on classification.  \n",
    "If we want to use it for regression, we want to make some changes.  \n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -- - - - - - \n",
    "Basic Algo remains the same.  \n",
    "In case of DT, we will have a root node, which will have all the datapoints, earlier what we were doing is we were predicting the class by majority. We were looking how many datapoints were there and how many training datapoints were there, and we were going by the majority of training data at that point.  \n",
    "SO lets say if we have 50 samples of a, 40 samples of class b,  we will predict a.  \n",
    "Now what we have is we have continous range of values. eg. 2.5 ,3.0, 3.1, 2.4 etc.  \n",
    "What we will do is we will predict the mean of these.  \n",
    "So lets say if the data is on node, we will have to predict the mean instead of the majority class value.  \n",
    "Obviously thats going to be bad, so what we do is we split the data in two parts and get it into next level of nodes.  \n",
    "How do we split?  \n",
    "Initially what we were doing in classification is we were going to each feature and checking how it is effecting accuracy or other parameters etc.  \n",
    "Here what we do is we find a feature on which if we split, there are many metric in which one of those is meansquareerror, \n",
    "so that mean square error gets minimum.  \n",
    "Where mean square error is $\\frac{1}{N}\\sum_{i=1}^N(y_{i actual} - y_{mean})^2$\n",
    "So we are hoping is after split, the mse gets reduced.  \n",
    "What we do is we pick a feature which will lead to maximum decrease in error.  \n",
    "And eventually leaf nodes will either be just 1 datapoint or they will have multiple datapoints depending upon whether you are doing pruning or you are stopping to split earlier or whatever you are doing the way we did in classification.  \n",
    "So our leaf might have one or more no of datapoints and we will find the mean to predict vale at that particular leaf.  \n",
    "So everything says exactly the same the ONLY THING THAT CHANGES IS BASICALLY HOW  YOU ARE GOING TO PREDICT FOR A PARTICULAR NODE THERE WE WERE GOING BY MAJORITY OF DATA HERE WE ARE GOING BY MEAN VALUE AND SECOND THING THAT IS CHANGING IS HOW TO DECIDE WHICH FEATURE TO SPLIT UPON INSTEAD OF USING ANY OF GINI INDEX OR OTHER DT METRIC, WE WILL USE MEAN SQUARE ERROR(THERE ARE OTHER METRICS ALSO).  \n",
    "So lets say there are three levels, MSE at level 0 will be greater than level 1 2 and 3. and MSE of level 1 will be greater than 2 and 3 and so on.  \n",
    "In a nutshell, MSE will decrease after every split.  \n",
    "What we find is we find a feature that will give less MSE on split.  \n",
    "And in case of RF, we will build a lot of decision trees like these, WE WILL TAKE MEAN OF ALL DT TO PREDICT OUR FINAL VALUE.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
