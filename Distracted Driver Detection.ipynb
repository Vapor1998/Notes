{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout,BatchNormalization,MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = {}\n",
    "with open('imgs/driver_imgs_list.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    \n",
    "    # first row in csv has column headers, we dont want column headers. \n",
    "    next(reader)\n",
    "    \n",
    "    # now getting our key-value pair\n",
    "    for row in reader:\n",
    "        key = row[1]\n",
    "        if key in data:\n",
    "            data[key].append(row[2])\n",
    "        else:\n",
    "            data[key] = [row[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "distraction_list = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we need to make master_data directory.\n",
    "# inside it, we create two subdirectories training and testing.\n",
    "\n",
    "import os\n",
    "os.mkdir('master_data')\n",
    "os.mkdir('master_data/training')\n",
    "os.mkdir('master_data/testing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inside the training and testing, we need to make 10 new subdirectories related to these distractions.\n",
    "# so we iterate over each distractions.\n",
    "for distraction in distraction_list:\n",
    "    os.mkdir(os.path.join('master_data/training/', distraction))\n",
    "    os.mkdir(os.path.join('master_data/testing/', distraction))\n",
    "# we have created subdirectiories inside training and testing but the directories are empty so we need to fill them\n",
    "# with source directory images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil as sh\n",
    "split_size = 0.8\n",
    "\n",
    "for dis, images in data.items():\n",
    "    train_size = int(split_size*len(images))\n",
    "    train_images = images[:train_size]\n",
    "    test_images = images[train_size:]\n",
    "    for image in train_images:\n",
    "        source = os.path.join('imgs/train',  dis,image)\n",
    "        dest = os.path.join('master_data/training',   dis)\n",
    "        sh.copy(source, dest)\n",
    "    for image in test_images:\n",
    "        source = os.path.join('imgs/train', dis, image)\n",
    "        dest = os.path.join('master_data/testing',  dis)\n",
    "        sh.copy(source, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17934 images belonging to 10 classes.\n",
      "Found 4490 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'master_data/training'\n",
    "test_dir = 'master_data/testing'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                train_dir,   # directory from which we want to generate our data.\n",
    "                                target_size = (100, 100),  # telling what is the size of the class\n",
    "                                class_mode =  'categorical',  # we need to tell what is the mode of classification, eg 'binary' \n",
    "                                batch_size = 128\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                                test_dir,   # directory from which we want to generate our data.\n",
    "                                target_size = (100, 100),  # telling what is the size of the class\n",
    "                                class_mode =  'categorical',  # we need to tell what is the mode of classification, eg 'binary' \n",
    "                                batch_size = 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'loss', patience = 2, min_delta=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 98, 98, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 49, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 47, 47, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 21, 21, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              6554624   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 6,592,554\n",
      "Trainable params: 6,590,506\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # layer 1\n",
    "    Conv2D(16, (3,3), activation = 'relu', input_shape = (100, 100, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    # layer 2\n",
    "    Conv2D(32, (3,3), activation = 'relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    #layer 3\n",
    "    Conv2D(64, (3,3), activation = 'relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    # Dense layer\n",
    "    Flatten(),\n",
    "    Dense(1024, activation ='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Last Layer is Important coz we have to decide here mow many classes are there in the images\n",
    "    Dense(10, activation ='softmax')\n",
    "\n",
    "\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 884s 6s/step - loss: 0.1952 - accuracy: 0.9435 - val_loss: 2.2563 - val_accuracy: 0.1900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cacf5e2bb0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, epochs = 1, verbose = 1, validation_data = test_generator, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
