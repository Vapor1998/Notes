{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef65de0",
   "metadata": {},
   "source": [
    "# [Naive Bayes](https://aman.ai/coursera-nlp/naive-bayes/)\n",
    "\n",
    "#### In Naive Bayes, Bayes comes from Bayes Theorem from Probablity. \n",
    "  \n",
    "Bayes Theorem states that,\n",
    "\n",
    "$$ P(\\frac{A}{B}) = \\frac{P(\\frac{B}{A}) * P(A)}{P(B)} $$\n",
    "\n",
    "where,  \n",
    "$P(A|B)$ – the probability of event A occurring, given event B has occurred  \n",
    "$P(B|A)$ – the probability of event B occurring, given event A has occurred  \n",
    "$P(A)$ – the probability of event A  \n",
    "$P(B)$ – the probability of event B   \n",
    "\n",
    "  \n",
    " \n",
    "So what is the problem that is generally given?  \n",
    "In a classification problem, we are generally given an x, lets say one test data, we are supposed to predict the value of y.  \n",
    "Lets say the value of Y can be a or b. For Naive Bayes we can do as many classes as possible. Its not really a Binary classification. It can handle as many classes as we have.  \n",
    "So we have to find out whether the class of this x is a or b.  \n",
    "We can never be sure what class is going to be. Its basically based on the training data whether we believe the class is goign to be a or b.  \n",
    "Lets say the x is the input we recieved.  \n",
    "And we want to predict y values out of these classes.\n",
    "\n",
    "In another way we can say we want to find out what is the probablity that y is a given that input is x which can be written as  $P(y=a/x = x)$.    \n",
    "Along with these, we can find out these values $P(y=b/X = x)$, along with lets say another class $P(y=c/X = x)$, what these values mean is given this input is already decided, what is the probablity that the output class is the y = a, y=b or y = c. \n",
    "\n",
    "\n",
    "And sum of these three should be 1. Coz these three are the only possible outputs.  \n",
    "Lets say if we find the probablities of these three terms, we can find the out the predictions that should be the maximum of these three.  \n",
    "So in other ways lets say the classes are $Y = a_{1}, a_{2}, a_{3\n",
    "} ..... a_{k}$ say we have k possible classes.  \n",
    "What we really waant to find is $P(y = a_{i}/ X = x)$ and then maximize this with $i = 1-to-n$. So the answer will be the maximum $a_{i}$ means maximum a will be the answer.  \n",
    "SO lets just focus on one particular value.  \n",
    "Lets say we want to find out what is the probablity $P(y = a_{i}/ X = x)$ seems like this might be really hard to find out given the training data as well.  \n",
    "What we are going to try is we will try the Bayes theorem and look at the other side of it.  \n",
    "Applying Bayes theorem our equation looks like $$P(y = a_{i}/ X = x) =  \\frac {P( X = x / y = a_{i}) * P(y = a_{i})}{P(X = x)}$$   \n",
    "\n",
    "If we put this in $P(y = a_{i}/ X = x)$, and we try to maximize over $i$, we will realize the denominator is same for all terms. So, $$P(y = a_{i}/ X = x) = \\frac {P( X = x / y = a_{i}) * P(y = a_{i})}{P(X = x)} $$  \n",
    "Lets say for $a_{1}$,  \n",
    "$$P(y = a_{1}/ X = x) = \\frac {P( X = x / y = a_{1}) * P(y = a_{1})}{P(X = x)} $$.  \n",
    "Now if we do it for class $a_{2}$, the numerator will be different because they depend upon class, but the denominator is going to be exactly the same for any class.  \n",
    "So what we are going to do is we are going to get rid of denominator.  \n",
    "As soon as we get rid of denominator, THE SUM IS NOT GOING TO BE 1, ITS NOT GOING TO BE PROBABLITY ANYMORE.  \n",
    "But the point is we want to maximize the $P(y = a_{i}/ X = x)$, we want to find that particular i which is max, removing the denominator in $\\frac {P( X = x / y = a_{1}) * P(y = a_{1})}{P(X = x)} $ will not change the maximum it will not change the $i$ for which this term $P( X = x / y = a_{1}) * P(y = a_{1})$ going to be maximized.It is going to be the exactly the same $i$.  \n",
    "So we can very easily get rid of the denominator,  the problem that we are now going to solve is we want ot maximize $ (P(y = a_{i}/ X = x))$. This is basically same as $ Find-i-such-that $  $$max_{i = 1}^k  (P(y = a_{i}/ X = x))$$ This is basically same as $$  max_{i = 1}^k  P( X = x / y = a_{i}) * P(y = a_{i})$$ because the denominator is exactly same in all the terms.  \n",
    "So we are going to maximize $P( X = x / y = a_{i}) * P(y = a_{i})$.  \n",
    "It is not too hard to find these values, lets say we have 3 classes, a1, a2, a3. and a1 = 40, a2 = 30, a3 = 30.  \n",
    "So $P(y = a_{i})$  of $a_{1} = 0.4$, $a_{2} = 0.3$ and $a_{3} = 0.3$,  \n",
    "Finding $P( X = x / y = a_{i})$ will be interesting.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2d090",
   "metadata": {},
   "source": [
    "So the problem that we want to solve now is basically finding out $$P( X = x / y = a_{i})$$, \n",
    "\n",
    "That in itself is not an easy problem to solve,  \n",
    "So X = x, what does this mean? so our input is 2 dimensional, it has few features, lets assume n features.  \n",
    "This actually means the probablity $ P((f_{1}, f_{2},f_{3}...f_{n}) = (x_{1}, x_{2}, x_{3} ... x_{n}) / y =a{i}) $.\n",
    "Now lets come to the part where this classifier is callled $Naive$ $Bayes$ Classifier.  \n",
    "  \n",
    "  \n",
    "It makes a very strong assumption(Naive Assumption) that lets assume that all the features are independent of each other.  \n",
    "And for Independent events, for eg what is the probablity that A happens and B happens $P(A \\cap B)$ if A and B are independent of each other, this probablity will be $P(A)* P(B)$.  \n",
    "Thats like saying what is the probablity that I tossed a coin, get a head, and the next time I tossed a coin I get tails.  \n",
    "Both these events are independent of each other.  \n",
    "```\n",
    "Features Independent of each other   \n",
    "  \n",
    "When we say that all features are independent of each other, we mean that the presence or absence of one particular feature does not influence the presence or absence of any other feature in the dataset, given the class label. In other words, the features are conditionally independent of each other given the class label.\n",
    "  \n",
    "This assumption simplifies the computation of probabilities in the algorithm. Instead of estimating the joint probability distribution of all features together, Naive Bayes estimates the probability of each feature independently given the class label and multiplies them together. This is why the likelihood term in the Naive Bayes formula is factored as a product of individual feature probabilities, given the class label.\n",
    "```\n",
    "  \n",
    "The probablity that first time I get a head and the second time I get a tail, what you can do is given these two  events are independent of each other is $P(1st = Head) * P(2nd = tail)$ = 1/2 * 1/2 which is 0.25 .  \n",
    "Similarly here if we assume that all the features are independent of each other, the probablity $P((f_{1}, f_{2},f_{3}...f_{n})$  is equal to $$ P(f_{1} = x_{1}/ y = a_{i}) * P(f_{2} = x_{2}/ y = a_{i}) * P(f_{3} = x_{3}/ y = a_{i}) ...... P(f_{n} =  x_{n}/ y = a_{i}) $$.  \n",
    "So we are assuming all the features are independent of each other and given the fact they are independent of each other, we can just multiply these probablities without worrying about if one happening tells me more about the other has happened or not.  \n",
    "So $Naive$ $Bayes$ is called $Naive$ because in reality, thats not going to be the case.  \n",
    "These features are going to be depend upon each other, most of dataset wont have property of Independence of the features that we have.  \n",
    "Thats the reason we call it Naive Bayes.  \n",
    "So, $$P( X = x / y = a_{i})  = \\prod_{j = 1}^n P(x^{j} = x^{j} / y = a_{i}) $$  \n",
    "So for $ P( X = x / y = a_{i}) *P(y=a_{i})$, \n",
    "$$P( X = x / y = a_{i}) *P(y=a_{i})  = \\prod_{j = 1}^n P(x^{j} = x^{j} / y = a_{i})*P(y=a_{i})$$  .\n",
    "So now we dont have to worry about $P( X = x / y = a_{i})$, we have to worry about Individual features to find out the final no that we want to calculate instead of all features combined.  \n",
    "We will find out abt these individual features next.  \n",
    "\n",
    "----\n",
    "\n",
    "##### We have been able to break our problem into smaller problem.  \n",
    "We started with the problem of finding which class the data belongs to then we decided lets find probablity of each class.  \n",
    "Then we decided lets use bayes theorem because finding probablity of each class will be higher lets try to find probablity of the input data given the class.   \n",
    "And we have to multiply it with the class probablity as well.  \n",
    "And then using Naive assumption of naive Bayes, what we figured out is that to find out \n",
    "$$P( X = x / y = a_{i}) *P(y=a_{i})  = \\prod_{j = 1}^n P(x^{j} = x^{j} / y = a_{i})*P(y=a_{i})$$ .  \n",
    "Now we are going  to find out $P(x^j = x^j)/(y=a_{i})$ . what should we do to calculate these values.  \n",
    "For this, we assume all our data, all our features are labelled features(Discrete Valued Features).  \n",
    "For eg. lets say we have features, $f_{1}$, $f_{2}$, $f_{3}$.  \n",
    "Lets say $f_{1}$ might be taking three different values, a,b,a,c,b,a,c.  \n",
    "Similarly $f_{2}$ might be taking three different values, $k_{1}$, $k_{2}$, $k_{1}$, $k_{2}$, $k_{3}$.   \n",
    "Similarly $f_{3}$ might be taking four different values,$l_{1}$, $l_{2}$, $l_{1}$, $l_{3}$, $l_{4}$.  \n",
    "lets say the $f_{2}$ is salary which is marked as low, medium and high.  \n",
    "Lets say $f_{3}$ could be repaying loan which can be marked as Yes or No.  \n",
    "So data is some fixed labels. This is for Discrete Data.  \n",
    "So what is the probablity that given $x^j$ class is $a_{i}$.  \n",
    "First of all the class is given as $a_{i}$, so out of all training data we have, lets focus on $a_{1}$.  \n",
    "Whatever we do for $a_{1}$ will be applied ofr all other classes.  \n",
    "SO $$ P(x^{j} = x^{j} / y = a_{i}) = \\frac{count of training data such that(X^j = x^j and y = a_{1})That means the jth feature value is x^j and only those rows where y is a_{1} I want to find how many of these have this particular label}{count of training data(y = a_{1})}. $$  \n",
    "Lets say one of these has desired value that we are looking for, so answer will be 1/3. Because we have 3 possible rowswith class $a_{1}$, only one of them has the entry of $x^j$, so the value is going to be 1/3.  \n",
    "So, $$ P(x^{j} = x^{j} / y = a_{i})  =  \\frac{Count-in-training-data(X^j = x^j and y = a_{1})}{count-in-training-data(y = a_{1})}$$ .  \n",
    "Lets take an example\n",
    "\n",
    "\n",
    "\n",
    "| salary | Loan_approved       |\n",
    "|--------|-------------------  |\n",
    "|   High    | Yes               |\n",
    "|   High   |  Yes                |\n",
    "|   Mid    |  Yes              |\n",
    "|   High   |  Yes                |\n",
    "|   Mid    |  Yes               |\n",
    "|   Low     |    Yes                 |\n",
    "|   Low  |  No           |\n",
    "|   Mid  |  No                 |\n",
    "|   Low  |  No               |\n",
    "|   High  |   No                  |\n",
    "\n",
    "Above table is for the Loan Approval.   \n",
    "Lets say what is the Probablity of Salary is High given Loan Approved is No. = $ P(salary = High/ y = No)$    \n",
    "To do so, we will look at only applications which has y = No.  \n",
    "So we have 4 values in table and fot salary is High, we have only 1 entry of it.  \n",
    "So the Probablity is 1/4.  \n",
    "First we need to count the values of y and then we need to find the x within that rows of y.  \n",
    "Similarly if we want to find the $P(salary = High/ y = Yes)$.  \n",
    "First we need to find the the no of y which has Yes and that is 6, now from those 6 rows, we need to check how much salary is High? It is 3 so the answer is 3/6.   \n",
    "\n",
    "Now lets Just talk about given the training Data, how are we going to apply this how are we going to write code to get to the final probablities.   \n",
    "Lets say we have training data, one way to structure this data is because everything is given in $y = a_{i}$,  what we are going to do is we want to create a dictionary, this dictionary on the first level will have $dict[a_{i}]$, of, which class this basically belongs to? So our top level dictionary will have all possible different classes, so the dictionary will have the keys $dict[a_{1}]$, $dict[a_{2}]$, $dict[a_{3}]$ .... and so on. And within each of these dictionaries we will have another dictionaries, which will have features.Lets say the features are $x^1$, $x^2$, $x^3$,... and so on. SO the dictionary inside dictionary dict will be $d[x^1]$, $d[x^2]$, $d[x^3]$, .... and so on.  \n",
    "Within these feature dictionary we need to know what are the possible values of these features lets say $x^1$.  \n",
    "Lets say it has High, Mid, low.  \n",
    "So within $x^1$, we will have another dictionaries, which will have keys as values of this feature($x^1$), (High, mid, low) and these keys will store corrosponding counts of these values of feature.  \n",
    "Also the $dict[a_{i}]$ will store the count of the class $a_{i}$ as value also.  \n",
    "And now if we want to find out the probablity $P(X^j = x^j/ y=a_{i}) $, what we simply do is we go to dictionary $dict[a_{i}]$, wthin that I know I have to look for feature $[X^j]$, within that I know I want to particular label $[x^j]$, which looks like $dict[a_{i}][X^j][x^j]$, this will give us the count of training data where class is $a_{i}$, and the feature $X^j$ value is $x^j$.  We need to divide this by total number of trainng data points where $y = a_{i}$. \n",
    "What we can do is along with the features, lets store a $[total-count]$ key as well and this will basically store what is the total count of datapoints with particular class $a_{i}$. So within $a_{i}$, we will have another dict which will have totalcount which now looks like $$ \\frac{dict[a_{i}][X^j][x^j]}{dict[a_{i}][total-count]}$$.  So whole equation looks like  \n",
    "\n",
    "$$ P(X^j = x^j/ y=a_{i})  = \\frac{dict[a_{i}][X^j][x^j]}{dict[a_{i}][total-count]}$$\n",
    "\n",
    "-----\n",
    "##### Before Implementing, we have to look at one more thing which mmight cause us lot of problem.  \n",
    "Lets say the data that we have the same table.  \n",
    "\n",
    "| salary | Loan_approved       |\n",
    "|--------|-------------------  |\n",
    "|   High    | Yes               |\n",
    "|   High   |  Yes                |\n",
    "|   Mid    |  Yes              |\n",
    "|   Low  |  Yes                |\n",
    "|   Mid    |  Yes               |\n",
    "|   High    |    Yes                 |\n",
    "|   Low  |  No           |\n",
    "|   Low  |  No                 |\n",
    "|   Low  |  No               |\n",
    "|   Mid|   No                  |\n",
    "|   Low |   No              |\n",
    "\n",
    "So what is the probablity that $P(salary = Low / approved = Yes)$,  \n",
    "So we look at only those which have approved yes.  \n",
    "Then within that, how many have salary as Low?, so 1/6. Seems like we are getting the probablity.  \n",
    "Now how about $P(salary = High / approved = No)$  \n",
    "Seems like none of the No entry have the salary High.  \n",
    "So we will end up writing the probablity as 0/5.  \n",
    "This means that if the approval is No, the salary cannot be high.  \n",
    "So if the example comes up, which has high salary, thhere is no way you can predict that it can get approved.  \n",
    "That sounds like a Bad classification.  \n",
    "So it is possible that a high salary person has done fraud thats why hes not getting approval.  \n",
    "In a nutshell, we do not want our probablities to be zero.  \n",
    "What we generally do is we correct the values. What we are gonna do is salary has three different values.  \n",
    "So we will add a little data to each of these. That means we will look at main formula $$ P(X^j = x^j / y = a_{i}) = \\frac{count of(y = a_{i} and X^j = x^j)}{count of(y = a_i)} $$ .  \n",
    "Now we are saying that the numerator count can be zero.   \n",
    "What we want is we want Probablity never to be zero.  \n",
    "So lets add +1 in numerator.  \n",
    " $$ P(X^j = x^j / y = a_{i}) = \\frac{count of(y = a_{i} and X^j = x^j) + 1}{count of(y = a_i)} $$, \n",
    " \n",
    "Adding a +1 creates a problem that it does not represents probablity anymore.  \n",
    "The sum of all such problem will not be 1 anymore.  \n",
    "Lets say for $ P(X^j = x^j / y = a_{i})$ , $X^j$ has two possible values lets say h and l, then the probablity equation wil be like,  $P(X^j = h / y = a_i) + P(X^j = l / y = a_i) = 1$, because $X^j$ can either tale the value h or l.  \n",
    "So what we will do is we should add to denominator as well.  \n",
    "We should add possible no of values $X^j$ can take. \n",
    " $$ P(X^j = x^j / y = a_{i}) = \\frac{count of(y = a_{i} and X^j = x^j) + 1}{count of(y = a_i) + Possible- values-of-|X^j|} $$\n",
    "\n",
    "So for eg, lets take the above table, values in $X^j$ is 3, high, mid and low, so $|X^j|$ will be 3.   \n",
    "This correction is called $Laplace-Correction$\n",
    "\n",
    "$Laplace-Correction$ says that to avoid zero probablity, whatever probablity you are calculating, just add +1 in numerator, just assume the hypothetical training datapoint 1 we are adding. Now as we add 1, and there are lets say 5 different possible values of $X^j$, and we are adding 1 on each of them, we need to add 5 to the denominator as well.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61636624",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
