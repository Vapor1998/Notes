{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef65de0",
   "metadata": {},
   "source": [
    "# [Naive Bayes](https://aman.ai/coursera-nlp/naive-bayes/)\n",
    "\n",
    "#### In Naive Bayes, Bayes comes from Bayes Theorem from Probablity. \n",
    "  \n",
    "Bayes Theorem states that,\n",
    "\n",
    "$$ P(\\frac{A}{B}) = \\frac{P(\\frac{B}{A}) * P(A)}{P(B)} $$\n",
    "\n",
    "where,  \n",
    "$P(A|B)$ – the probability of event A occurring, given event B has occurred  \n",
    "$P(B|A)$ – the probability of event B occurring, given event A has occurred  \n",
    "$P(A)$ – the probability of event A  \n",
    "$P(B)$ – the probability of event B   \n",
    "\n",
    "  \n",
    " \n",
    "So what is the problem that is generally given?  \n",
    "In a classification problem, we are generally given an x, lets say one test data, we are supposed to predict the value of y.  \n",
    "Lets say the value of Y can be a or b. For Naive Bayes we can do as many classes as possible. Its not really a Binary classification. It can handle as many classes as we have.  \n",
    "So we have to find out whether the class of this x is a or b.  \n",
    "We can never be sure what class is going to be. Its basically based on the training data whether we believe the class is goign to be a or b.  \n",
    "Lets say the x is the input we recieved.  \n",
    "And we want to predict y values out of these classes.\n",
    "\n",
    "In another way we can say we want to find out what is the probablity that y is a given that input is x which can be written as  $P(y=a/x = x)$.    \n",
    "Along with these, we can find out these values $P(y=b/X = x)$, along with lets say another class $P(y=c/X = x)$, what these values mean is given this input is already decided, what is the probablity that the output class is the y = a, y=b or y = c. \n",
    "\n",
    "\n",
    "And sum of these three should be 1. Coz these three are the only possible outputs.  \n",
    "Lets say if we find the probablities of these three terms, we can find the out the predictions that should be the maximum of these three.  \n",
    "So in other ways lets say the classes are $Y = a_{1}, a_{2}, a_{3\n",
    "} ..... a_{k}$ say we have k possible classes.  \n",
    "What we really waant to find is $P(y = a_{i}/ X = x)$ and then maximize this with $i = 1-to-n$. So the answer will be the maximum $a_{i}$ means maximum a will be the answer.  \n",
    "SO lets just focus on one particular value.  \n",
    "Lets say we want to find out what is the probablity $P(y = a_{i}/ X = x)$ seems like this might be really hard to find out given the training data as well.  \n",
    "What we are going to try is we will try the Bayes theorem and look at the other side of it.  \n",
    "Applying Bayes theorem our equation looks like $$P(y = a_{i}/ X = x) =  \\frac {P( X = x / y = a_{i}) * P(y = a_{i})}{P(X = x)}$$   \n",
    "\n",
    "If we put this in $P(y = a_{i}/ X = x)$, and we try to maximize over $i$, we will realize the denominator is same for all terms. So, $$P(y = a_{i}/ X = x) = \\frac {P( X = x / y = a_{i}) * P(y = a_{i})}{P(X = x)} $$  \n",
    "Lets say for $a_{1}$,  \n",
    "$$P(y = a_{1}/ X = x) = \\frac {P( X = x / y = a_{1}) * P(y = a_{1})}{P(X = x)} $$.  \n",
    "Now if we do it for class $a_{2}$, the numerator will be different because they depend upon class, but the denominator is going to be exactly the same for any class.  \n",
    "So what we are going to do is we are going to get rid of denominator.  \n",
    "As soon as we get rid of denominator, THE SUM IS NOT GOING TO BE 1, ITS NOT GOING TO BE PROBABLITY ANYMORE.  \n",
    "But the point is we want to maximize the $P(y = a_{i}/ X = x)$, we want to find that particular i which is max, removing the denominator in $\\frac {P( X = x / y = a_{1}) * P(y = a_{1})}{P(X = x)} $ will not change the maximum it will not change the $i$ for which this term $P( X = x / y = a_{1}) * P(y = a_{1})$ going to be maximized.It is going to be the exactly the same $i$.  \n",
    "So we can very easily get rid of the denominator,  the problem that we are now going to solve is we want ot maximize $ (P(y = a_{i}/ X = x))$. This is basically same as $ Find-i-such-that $  $$max_{i = 1}^k  (P(y = a_{i}/ X = x))$$ This is basically same as $$  max_{i = 1}^k  P( X = x / y = a_{i}) * P(y = a_{i})$$ because the denominator is exactly same in all the terms.  \n",
    "So we are going to maximize $P( X = x / y = a_{i}) * P(y = a_{i})$.  \n",
    "It is not too hard to find these values, lets say we have 3 classes, a1, a2, a3. and a1 = 40, a2 = 30, a3 = 30.  \n",
    "So $P(y = a_{i})$  of $a_{1} = 0.4$, $a_{2} = 0.3$ and $a_{3} = 0.3$,  \n",
    "Finding $P( X = x / y = a_{i})$ will be interesting.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2d090",
   "metadata": {},
   "source": [
    "So the problem that we want to solve now is basically finding out $$P( X = x / y = a_{i})$$, \n",
    "\n",
    "That in itself is not an easy problem to solve,  \n",
    "So X = x, what does this mean? so our input is 2 dimensional, it has few features, lets assume n features.  \n",
    "This actually means the probablity $ P((f_{1}, f_{2},f_{3}...f_{n}) = (x_{1}, x_{2}, x_{3} ... x_{n}) / y =a{i}) $.\n",
    "Now lets come to the part where this classifier is callled $Naive$ $Bayes$ Classifier.  \n",
    "  \n",
    "  \n",
    "It makes a very strong assumption(Naive Assumption) that lets assume that all the features are independent of each other.  \n",
    "And for Independent events, for eg what is the probablity that A happens and B happens $P(A \\cap B)$ if A and B are independent of each other, this probablity will be $P(A)* P(B)$.  \n",
    "Thats like saying what is the probablity that I tossed a coin, get a head, and the next time I tossed a coin I get tails.  \n",
    "Both these events are independent of each other.  \n",
    "```\n",
    "Features Independent of each other   \n",
    "  \n",
    "When we say that all features are independent of each other, we mean that the presence or absence of one particular feature does not influence the presence or absence of any other feature in the dataset, given the class label. In other words, the features are conditionally independent of each other given the class label.\n",
    "  \n",
    "This assumption simplifies the computation of probabilities in the algorithm. Instead of estimating the joint probability distribution of all features together, Naive Bayes estimates the probability of each feature independently given the class label and multiplies them together. This is why the likelihood term in the Naive Bayes formula is factored as a product of individual feature probabilities, given the class label.\n",
    "```\n",
    "  \n",
    "The probablity that first time I get a head and the second time I get a tail, what you can do is given these two  events are independent of each other is $P(1st = Head) * P(2nd = tail)$ = 1/2 * 1/2 which is 0.25 .  \n",
    "Similarly here if we assume that all the features are independent of each other, the probablity $P((f_{1}, f_{2},f_{3}...f_{n})$  is equal to $$ P(f_{1} = x_{1}/ y = a_{i}) * P(f_{2} = x_{2}/ y = a_{i}) * P(f_{3} = x_{3}/ y = a_{i}) ...... P(f_{n} =  x_{n}/ y = a_{i}) $$.  \n",
    "So we are assuming all the features are independent of each other and given the fact they are independent of each other, we can just multiply these probablities without worrying about if one happening tells me more about the other has happened or not.  \n",
    "So $Naive$ $Bayes$ is called $Naive$ because in reality, thats not going to be the case.  \n",
    "These features are going to be depend upon each other, most of dataset wont have property of Independence of the features that we have.  \n",
    "Thats the reason we call it Naive Bayes.  \n",
    "So, $$P( X = x / y = a_{i})  = \\prod_{j = 1}^n P(x^{j} = x^{j} / y = a_{i}) $$  \n",
    "So for $ P( X = x / y = a_{i}) *P(y=a_{i})$, \n",
    "$$P( X = x / y = a_{i}) *P(y=a_{i})  = \\prod_{j = 1}^n P(x^{j} = x^{j} / y = a_{i})*P(y=a_{i})$$  .\n",
    "So now we dont have to worry about $P( X = x / y = a_{i})$, we have to worry about Individual features to find out the final no that we want to calculate instead of all features combined.  \n",
    "We will find out abt these individual features next.  \n",
    "\n",
    "----\n",
    "\n",
    "##### We have been able to break our problem into smaller problem.  \n",
    "We started with the problem of finding which class the data belongs to then we decided lets find probablity of each class.  \n",
    "Then we decided lets use bayes theorem because finding probablity of each class will be higher lets try to find probablity of the input data given the class.   \n",
    "And we have to multiply it with the class probablity as well.  \n",
    "And then using Naive assumption of naive Bayes, what we figured out is that to find out \n",
    "$$P( X = x / y = a_{i}) *P(y=a_{i})  = \\prod_{j = 1}^n P(x^{j} = x^{j} / y = a_{i})*P(y=a_{i})$$ .  \n",
    "Now we are going  to find out $P(x^j = x^j)/(y=a_{i})$ . what should we do to calculate these values.  \n",
    "For this, we assume all our data, all our features are labelled features(Discrete Valued Features).  \n",
    "For eg. lets say we have features, $f_{1}$, $f_{2}$, $f_{3}$.  \n",
    "Lets say $f_{1}$ might be taking three different values, a,b,a,c,b,a,c.  \n",
    "Similarly $f_{2}$ might be taking three different values, $k_{1}$, $k_{2}$, $k_{1}$, $k_{2}$, $k_{3}$.   \n",
    "Similarly $f_{3}$ might be taking four different values,$l_{1}$, $l_{2}$, $l_{1}$, $l_{3}$, $l_{4}$.  \n",
    "lets say the $f_{2}$ is salary which is marked as low, medium and high.  \n",
    "Lets say $f_{3}$ could be repaying loan which can be marked as Yes or No.  \n",
    "So data is some fixed labels. This is for Discrete Data.  \n",
    "So what is the probablity that given $x^j$ class is $a_{i}$.  \n",
    "First of all the class is given as $a_{i}$, so out of all training data we have, lets focus on $a_{1}$.  \n",
    "Whatever we do for $a_{1}$ will be applied ofr all other classes.  \n",
    "SO $$ P(x^{j} = x^{j} / y = a_{i}) = \\frac{count of training data such that(X^j = x^j and y = a_{1})That means the jth feature value is x^j and only those rows where y is a_{1} I want to find how many of these have this particular label}{count of training data(y = a_{1})}. $$  \n",
    "Lets say one of these has desired value that we are looking for, so answer will be 1/3. Because we have 3 possible rowswith class $a_{1}$, only one of them has the entry of $x^j$, so the value is going to be 1/3.  \n",
    "So, $$ P(x^{j} = x^{j} / y = a_{i})  =  \\frac{Count-in-training-data(X^j = x^j and y = a_{1})}{count-in-training-data(y = a_{1})}$$ .  \n",
    "Lets take an example\n",
    "\n",
    "\n",
    "\n",
    "| salary | Loan_approved       |\n",
    "|--------|-------------------  |\n",
    "|   High    | Yes               |\n",
    "|   High   |  Yes                |\n",
    "|   Mid    |  Yes              |\n",
    "|   High   |  Yes                |\n",
    "|   Mid    |  Yes               |\n",
    "|   Low     |    Yes                 |\n",
    "|   Low  |  No           |\n",
    "|   Mid  |  No                 |\n",
    "|   Low  |  No               |\n",
    "|   High  |   No                  |\n",
    "\n",
    "Above table is for the Loan Approval.   \n",
    "Lets say what is the Probablity of Salary is High given Loan Approved is No. = $ P(salary = High/ y = No)$    \n",
    "To do so, we will look at only applications which has y = No.  \n",
    "So we have 4 values in table and fot salary is High, we have only 1 entry of it.  \n",
    "So the Probablity is 1/4.  \n",
    "First we need to count the values of y and then we need to find the x within that rows of y.  \n",
    "Similarly if we want to find the $P(salary = High/ y = Yes)$.  \n",
    "First we need to find the the no of y which has Yes and that is 6, now from those 6 rows, we need to check how much salary is High? It is 3 so the answer is 3/6.   \n",
    "\n",
    "Now lets Just talk about given the training Data, how are we going to apply this how are we going to write code to get to the final probablities.   \n",
    "Lets say we have training data, one way to structure this data is because everything is given in $y = a_{i}$,  what we are going to do is we want to create a dictionary, this dictionary on the first level will have $dict[a_{i}]$, of, which class this basically belongs to? So our top level dictionary will have all possible different classes, so the dictionary will have the keys $dict[a_{1}]$, $dict[a_{2}]$, $dict[a_{3}]$ .... and so on. And within each of these dictionaries we will have another dictionaries, which will have features.Lets say the features are $x^1$, $x^2$, $x^3$,... and so on. SO the dictionary inside dictionary dict will be $d[x^1]$, $d[x^2]$, $d[x^3]$, .... and so on.  \n",
    "Within these feature dictionary we need to know what are the possible values of these features lets say $x^1$.  \n",
    "Lets say it has High, Mid, low.  \n",
    "So within $x^1$, we will have another dictionaries, which will have keys as values of this feature($x^1$), (High, mid, low) and these keys will store corrosponding counts of these values of feature.  \n",
    "Also the $dict[a_{i}]$ will store the count of the class $a_{i}$ as value also.  \n",
    "And now if we want to find out the probablity $P(X^j = x^j/ y=a_{i}) $, what we simply do is we go to dictionary $dict[a_{i}]$, wthin that I know I have to look for feature $[X^j]$, within that I know I want to particular label $[x^j]$, which looks like $dict[a_{i}][X^j][x^j]$, this will give us the count of training data where class is $a_{i}$, and the feature $X^j$ value is $x^j$.  We need to divide this by total number of trainng data points where $y = a_{i}$. \n",
    "What we can do is along with the features, lets store a $[total-count]$ key as well and this will basically store what is the total count of datapoints with particular class $a_{i}$. So within $a_{i}$, we will have another dict which will have totalcount which now looks like $$ \\frac{dict[a_{i}][X^j][x^j]}{dict[a_{i}][total-count]}$$.  So whole equation looks like  \n",
    "\n",
    "$$ P(X^j = x^j/ y=a_{i})  = \\frac{dict[a_{i}][X^j][x^j]}{dict[a_{i}][total-count]}$$\n",
    "\n",
    "-----\n",
    "##### Before Implementing, we have to look at one more thing which mmight cause us lot of problem.  \n",
    "Lets say the data that we have the same table.  \n",
    "\n",
    "| salary | Loan_approved       |\n",
    "|--------|-------------------  |\n",
    "|   High    | Yes               |\n",
    "|   High   |  Yes                |\n",
    "|   Mid    |  Yes              |\n",
    "|   Low  |  Yes                |\n",
    "|   Mid    |  Yes               |\n",
    "|   High    |    Yes                 |\n",
    "|   Low  |  No           |\n",
    "|   Low  |  No                 |\n",
    "|   Low  |  No               |\n",
    "|   Mid|   No                  |\n",
    "|   Low |   No              |\n",
    "\n",
    "So what is the probablity that $P(salary = Low / approved = Yes)$,  \n",
    "So we look at only those which have approved yes.  \n",
    "Then within that, how many have salary as Low?, so 1/6. Seems like we are getting the probablity.  \n",
    "Now how about $P(salary = High / approved = No)$  \n",
    "Seems like none of the No entry have the salary High.  \n",
    "So we will end up writing the probablity as 0/5.  \n",
    "This means that if the approval is No, the salary cannot be high.  \n",
    "So if the example comes up, which has high salary, thhere is no way you can predict that it can get approved.  \n",
    "That sounds like a Bad classification.  \n",
    "So it is possible that a high salary person has done fraud thats why hes not getting approval.  \n",
    "In a nutshell, we do not want our probablities to be zero.  \n",
    "What we generally do is we correct the values. What we are gonna do is salary has three different values.  \n",
    "So we will add a little data to each of these. That means we will look at main formula $$ P(X^j = x^j / y = a_{i}) = \\frac{count of(y = a_{i} and X^j = x^j)}{count of(y = a_i)} $$ .  \n",
    "Now we are saying that the numerator count can be zero.   \n",
    "What we want is we want Probablity never to be zero.  \n",
    "So lets add +1 in numerator.  \n",
    " $$ P(X^j = x^j / y = a_{i}) = \\frac{count of(y = a_{i} and X^j = x^j) + 1}{count of(y = a_i)} $$, \n",
    " \n",
    "Adding a +1 creates a problem that it does not represents probablity anymore.  \n",
    "The sum of all such problem will not be 1 anymore.  \n",
    "Lets say for $ P(X^j = x^j / y = a_{i})$ , $X^j$ has two possible values lets say h and l, then the probablity equation wil be like,  $P(X^j = h / y = a_i) + P(X^j = l / y = a_i) = 1$, because $X^j$ can either tale the value h or l.  \n",
    "So what we will do is we should add to denominator as well.  \n",
    "We should add possible no of values $X^j$ can take. \n",
    " $$ P(X^j = x^j / y = a_{i}) = \\frac{count of(y = a_{i} and X^j = x^j) + 1}{count of(y = a_i) + Possible- values-of-|X^j|} $$\n",
    "\n",
    "So for eg, lets take the above table, values in $X^j$ is 3, high, mid and low, so $|X^j|$ will be 3.   \n",
    "This correction is called $Laplace-Correction$\n",
    "\n",
    "$Laplace-Correction$ says that to avoid zero probablity, whatever probablity you are calculating, just add +1 in numerator, just assume the hypothetical training datapoint 1 we are adding. Now as we add 1, and there are lets say 5 different possible values of $X^j$, and we are adding 1 on each of them, we need to add 5 to the denominator as well.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e526e",
   "metadata": {},
   "source": [
    "# `Multinomial Naive Bayes`  \n",
    "Multinomial Naive Bayes is an extension of the Naive Bayes algorithm that is suitable for classification tasks with discrete features. It is commonly used for text classification problems. \n",
    "\n",
    "**Its is used when we have discrete data (e.g. movie ratings ranging 1 and 5 as each rating will have certain frequency to represent). In text learning we have the count of each word to predict the class or label.**\n",
    "\n",
    "Let's consider a classification problem with a feature vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ and a target class variable $y$ that can take on one of $C$ possible classes.\n",
    "\n",
    "The goal is to determine the posterior probability $P(y | \\mathbf{x})$, which represents the probability of class $y$ given the observed features $\\mathbf{x}$.\n",
    "\n",
    "According to Bayes' theorem, the posterior probability can be calculated as:\n",
    "\n",
    "$$P(y | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y) \\cdot P(y)}{P(\\mathbf{x})}$$\n",
    "\n",
    "where:\n",
    "- $P(y | \\mathbf{x})$ is the posterior probability (probability of class $y$ given the features $\\mathbf{x}$).\n",
    "- $P(\\mathbf{x} | y)$ is the likelihood (probability of observing features $\\mathbf{x}$ given class $y$).\n",
    "- $P(y)$ is the prior probability (probability of class $y$ occurring).\n",
    "- $P(\\mathbf{x})$ is the evidence probability (probability of observing the features $\\mathbf{x}$).\n",
    "\n",
    "In Multinomial Naive Bayes, the feature vector $\\mathbf{x}$ represents the frequencies or counts of discrete features. The assumption is that the feature counts follow a multinomial distribution.\n",
    "\n",
    "The likelihood $P(\\mathbf{x} | y)$ can be modeled using the multinomial distribution, and it is calculated as:\n",
    "\n",
    "$$P(\\mathbf{x} | y) = \\prod_{i=1}^{n} P(x_i | y)^{x_i}$$\n",
    "\n",
    "where $P(x_i | y)$ is the probability of feature $x_i$ occurring given class $y$.\n",
    "\n",
    "The prior probability $P(y)$ can be estimated from the training data by calculating the relative frequencies of each class.\n",
    "\n",
    "To classify a new instance $\\mathbf{x}'$, we calculate the posterior probability for each class and select the class with the highest probability.\n",
    "\n",
    "In practice, to avoid issues with zero probabilities, techniques like Laplace smoothing or add-one smoothing are often employed to estimate the probabilities. Laplace smoothing adds a small constant to the counts to ensure non-zero probabilities.\n",
    "\n",
    "Multinomial Naive Bayes is widely used for text classification tasks, where the feature vectors represent word frequencies or term frequencies in documents. It assumes independence between features but can still perform well in practice, especially when the feature vectors are sparse and high-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef6462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'clf__alpha': 0.1, 'vect__max_features': 3000, 'vect__ngram_range': (1, 1)}\n",
      "Best Score:  0.9539234346486667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "# Create a pipeline for text preprocessing and Multinomial Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'vect__max_features': [1000, 2000, 3000],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__alpha': [0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "# Create a KFold object for k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=kf)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(data_train.data, data_train.target)\n",
    "\n",
    "# Predict the test data\n",
    "predicted = grid_search.predict(data_test.data)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba186e4",
   "metadata": {},
   "source": [
    "# `Gaussian Naive Bayes`  \n",
    "Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes the features follow a Gaussian (normal) distribution. It is suitable for continuous or numeric features.    \n",
    "**Because of the assumption of the normal distribution, Gaussian Naive Bayes is used in cases when all our features are continuous. For example in Iris dataset features are sepal width, petal width, sepal length, petal length. So its features can have different values in data set as width and length can vary. We can’t represent features in terms of their occurrences. This means data is continuous. Hence we use Gaussian Naive Bayes here.**\n",
    "\n",
    "Let's consider a classification problem with a feature vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ and a target class variable $y$ that can take on one of $C$ possible classes.\n",
    "\n",
    "The goal is to determine the posterior probability $P(y | \\mathbf{x})$, which represents the probability of class $y$ given the observed features \\(\\mathbf{x}\\).\n",
    "\n",
    "According to Bayes' theorem, the posterior probability can be calculated as:\n",
    "\n",
    "$P(y | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y) \\cdot P(y)}{P(\\mathbf{x})}$\n",
    "\n",
    "where:\n",
    "- $P(y | \\mathbf{x})$ is the posterior probability (probability of class $y$ given the features $\\mathbf{x}$).\n",
    "- $P(\\mathbf{x} | y)$ is the likelihood (probability of observing features $\\mathbf{x}$ given class $y$). \n",
    "- $P(y)$ is the prior probability (probability of class $y$ occurring).\n",
    "- $P(\\mathbf{x})$ is the evidence probability (probability of observing the features $\\mathbf{x}$).\n",
    "\n",
    "In Gaussian Naive Bayes, the assumption is that the feature values given each class follow a Gaussian distribution. Mathematically, it can be written as:\n",
    "\n",
    "$$P(x_i | y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right)$$\n",
    "\n",
    "where $P(x_i | y)$ is the probability of feature $x_i$ occurring given class $y$, $\\mu_{y,i}$ is the mean of feature $x_i$ for class $y$, and $\\sigma_{y,i}^2$ is the variance of feature $x_i$ for class $y$.\n",
    "\n",
    "To classify a new instance $\\mathbf{x}'$, we calculate the posterior probability for each class and select the class with the highest probability.\n",
    "\n",
    "In practice, the prior probabilities $P(y)$, means $\\mu_{y,i}$, and variances $\\sigma_{y,i}^2$ can be estimated from the training data using maximum likelihood estimation or other techniques.\n",
    "\n",
    "Gaussian Naive Bayes is computationally efficient and can handle continuous or numeric features. However, it assumes that the features are independent given the class, which may not always hold in real-world scenarios.   \n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "In Gaussian Naive Bayes, we assume that the feature values for each class follow a Gaussian (normal) distribution. This assumption allows us to model the likelihood $P(x_i | y)$ as a Gaussian distribution for each feature $x_i$ given class $y$.\n",
    "\n",
    "The Gaussian probability density function (PDF) is given by:\n",
    "\n",
    "$$P(x_i | y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right)$$\n",
    "\n",
    "where:\n",
    "- $P(x_i | y)$ is the probability of feature $x_i$ occurring given class \\(y\\).\n",
    "- $\\mu_{y,i}$ is the mean of feature \\(x_i\\) for class \\(y\\).\n",
    "- $\\sigma_{y,i}^2$ is the variance of feature \\(x_i\\) for class \\(y\\).\n",
    "\n",
    "To calculate the posterior probability $P(y | \\mathbf{x})$ using Bayes' theorem, we need to compute three components:\n",
    "\n",
    "1. Prior Probability $P(y)$: This represents the probability of class $y$ occurring in the dataset. It can be estimated by calculating the relative frequencies of each class in the training data.\n",
    "\n",
    "2. Likelihood $P(\\mathbf{x} | y)$: This represents the probability of observing the feature vector $\\mathbf{x}$ given class $y$. In Gaussian Naive Bayes, we assume that the features are independent, given the class. Therefore, the likelihood is calculated as the product of the individual feature probabilities:\n",
    "\n",
    "   $$P(\\mathbf{x} | y) = P(x_1 | y) \\cdot P(x_2 | y) \\cdot \\ldots \\cdot P(x_n | y)$$\n",
    "\n",
    "   We can substitute the Gaussian PDF for each feature $P(x_i | y)$ to compute the likelihood.\n",
    "\n",
    "3. Evidence Probability $P(\\mathbf{x})$: This represents the probability of observing the feature vector $\\mathbf{x}$ in the dataset. It acts as a normalization factor and ensures that the posterior probabilities sum up to 1. However, in the context of classification, this term is not necessary because we only need to compare the posterior probabilities for different classes.\n",
    "\n",
    "To classify a new instance $\\mathbf{x}'$, we calculate the posterior probability $P(y | \\mathbf{x}')$ for each class $y$. We select the class with the highest posterior probability as the predicted class for $\\mathbf{x}'$.\n",
    "\n",
    "It's worth noting that Gaussian Naive Bayes assumes independence between features, which means that the presence or absence of one feature does not affect the presence or absence of other features. This assumption simplifies the computation and can work well when the features are not strongly correlated. However, if the independence assumption is violated, it may lead to suboptimal results.\n",
    "\n",
    "In practice, to apply Gaussian Naive Bayes, you can use libraries such as scikit-learn in Python, which provide ready-to-use implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c57316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Perform prediction on the test data\n",
    "y_pred = naive_bayes.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee33f1d",
   "metadata": {},
   "source": [
    "# `Bernoulli Naive Bayes`   \n",
    "Bernoulli Naive Bayes is a variant of the Naive Bayes algorithm that assumes binary features (i.e., features that take only two possible values, typically 0 and 1). \n",
    "**It assumes that all our features are binary such that they take only two values. Means 0s can represent “word does not occur in the document” and 1s as \"word occurs in the document**\n",
    "  \n",
    "Bernoulli Naive Bayes is a variant of the Naive Bayes algorithm that is specifically designed for binary (two-class) classification problems. It assumes that the features (input variables) are binary, meaning they can take on only two values, typically represented as 0 and 1.\n",
    "\n",
    "Bernoulli Naive Bayes is commonly used in text classification tasks, such as sentiment analysis, spam detection, and document categorization. It works well with text data represented as binary feature vectors, where each feature represents the presence or absence of a specific word or term in a document.\n",
    "\n",
    "Here are a few scenarios where Bernoulli Naive Bayes can be particularly useful:\n",
    "\n",
    "1. **Text Classification**: When classifying text data, Bernoulli Naive Bayes can be effective. It assumes that the presence or absence of certain words or features is indicative of the class label. For example, in sentiment analysis, the presence of positive words in a document might indicate a positive sentiment, while the absence of those words might indicate a negative sentiment.\n",
    "\n",
    "2. **Binary Feature Data**: Bernoulli Naive Bayes is well-suited for problems where the features are binary, representing yes/no, true/false, or presence/absence. For instance, in document classification, each feature may represent whether a specific word occurs in a document or not.\n",
    "\n",
    "3. **Sparse Feature Representation**: If the dataset has a large number of features and most of them are zero (sparse representation), Bernoulli Naive Bayes can handle this efficiently. It leverages the sparsity of the data to make predictions effectively.\n",
    "\n",
    "It's important to note that Bernoulli Naive Bayes assumes independence between the features given the class, which is a simplifying assumption. In situations where the features have dependencies, such as the order of words in a sentence or the co-occurrence of certain words, other variants of Naive Bayes (e.g., Multinomial Naive Bayes or Gaussian Naive Bayes) may be more appropriate.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is specifically used for binary classification problems where the features are binary and is commonly applied to text classification tasks.\n",
    "Let's consider a classification problem with a feature vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ where each $x_i$ represents a binary feature, and a target class variable $y$ that can take on one of $C$ possible classes.\n",
    "\n",
    "The goal is to determine the posterior probability $P(y | \\mathbf{x})$, which represents the probability of class $y$ given the observed features $\\mathbf{x}$.\n",
    "\n",
    "According to Bayes' theorem, the posterior probability can be calculated as:\n",
    "\n",
    "$$P(y | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y) \\cdot P(y)}{P(\\mathbf{x})}$$\n",
    "\n",
    "where:\n",
    "- $P(y | \\mathbf{x})$ is the posterior probability (probability of class $y$ given the features $\\mathbf{x}$).\n",
    "- $P(\\mathbf{x} | y)$ is the likelihood (probability of observing features \\(\\mathbf{x}\\) given class \\(y\\)).\n",
    "- $P(y)$ is the prior probability (probability of class $y$ occurring).\n",
    "- $P(\\mathbf{x})$ is the evidence probability (probability of observing the features $\\mathbf{x}$).\n",
    "\n",
    "In Bernoulli Naive Bayes, we assume that the features follow a Bernoulli distribution, which means they are binary (0 or 1). Mathematically, it can be written as:\n",
    "\n",
    "$$P(x_i | y) = \\theta_{y,i}^{x_i} \\cdot (1 - \\theta_{y,i})^{(1 - x_i)}$$\n",
    "\n",
    "where:\n",
    "- $P(x_i | y)$ is the probability of feature $x_i$ occurring given class $y$.\n",
    "- $\\theta_{y,i}$ is the parameter representing the probability of feature $x_i$ being 1 given class $y$.\n",
    "\n",
    "To classify a new instance $\\mathbf{x}'$, we calculate the posterior probability for each class and select the class with the highest probability.\n",
    "\n",
    "In practice, the prior probabilities $P(y)$ and the parameters $\\theta_{y,i}$ can be estimated from the training data using maximum likelihood estimation or other techniques.\n",
    "\n",
    "Bernoulli Naive Bayes is particularly suitable for text classification tasks, where the presence or absence of words (binary features) in a document can be used as features.\n",
    "\n",
    "Note that Bernoulli Naive Bayes assumes independence between features, similar to other Naive Bayes variants. This assumption simplifies the computation and can work well when the features are relatively independent. However, it may not be suitable for features that exhibit strong dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38be6571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7906186989281545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fetch the 20newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
    "\n",
    "# Extract the features and labels from the dataset\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "# Create a CountVectorizer for feature extraction\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Create a Bernoulli Naive Bayes classifier\n",
    "naive_bayes = BernoulliNB()\n",
    "\n",
    "# Fit the classifier to the data\n",
    "naive_bayes.fit(X, y)\n",
    "\n",
    "# Perform prediction on the training data\n",
    "y_pred = naive_bayes.predict(X)\n",
    "\n",
    "# Calculate accuracy on the training data\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61636624",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
