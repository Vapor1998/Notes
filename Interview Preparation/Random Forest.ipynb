{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26202ab",
   "metadata": {},
   "source": [
    "### `Random Forest `  \n",
    "Random Forest is an ensemble learning method that combines multiple decision trees to create a more robust and accurate model. It is widely used for both classification and regression tasks. Here's an explanation of Random Forest:\n",
    "\n",
    "1. Ensemble Learning:\n",
    "Ensemble learning is a technique that combines multiple individual models to make predictions. Random Forest is an ensemble learning method that combines the predictions of multiple decision trees to make a final prediction. This combination of models helps to reduce overfitting and improve the overall performance and generalization of the model.\n",
    "\n",
    "2. Decision Trees:\n",
    "Random Forest is built upon the concept of decision trees. A decision tree is a flowchart-like model that makes predictions by recursively splitting the feature space based on different features and their values. Each internal node represents a feature test, and each leaf node represents a prediction or outcome.\n",
    "\n",
    "3. Randomness in Random Forest:\n",
    "Random Forest introduces randomness in two main aspects:\n",
    "\n",
    "   a. Random Subset of Features: At each split in a decision tree, only a random subset of features is considered for splitting. This random feature selection helps to reduce the correlation between trees and encourages diversity among them.\n",
    "\n",
    "   b. Bootstrap Aggregation (Bagging): Random Forest uses a technique called bootstrap aggregating or bagging. It involves creating multiple subsets of the original training data by randomly sampling with replacement. Each subset is then used to train a separate decision tree in the forest. This helps to introduce variability in the training data and improve the robustness of the model.\n",
    "\n",
    "4. Prediction in Random Forest:\n",
    "To make a prediction using a Random Forest, each decision tree in the forest independently predicts the outcome based on its own set of rules. For classification tasks, the final prediction is made by majority voting, where the class that receives the most votes across all the trees is selected as the final prediction. For regression tasks, the final prediction is the average or mean of the predictions from all the trees.\n",
    "\n",
    "5. Advantages of Random Forest:\n",
    "   - Random Forest is robust against overfitting and tends to generalize well to unseen data.\n",
    "   - It can handle a large number of features and is not sensitive to outliers.\n",
    "   - Random Forest provides a measure of feature importance, which can be useful for feature selection.\n",
    "   - It can be parallelized and easily scaled to large datasets.\n",
    "\n",
    "Random Forest is a powerful and versatile algorithm that has shown excellent performance in a wide range of machine learning tasks. It is known for its simplicity, interpretability, and ability to handle complex data relationships.   \n",
    "\n",
    "  ---\n",
    "  Random Forest is an ensemble learning algorithm that combines multiple decision trees to make predictions. Here are the steps involved in executing the Random Forest algorithm:\n",
    "\n",
    "1. **Data Preparation**: Random Forest requires a labeled dataset to train the model. The dataset is typically split into features (input variables) and labels (output variable).\n",
    "\n",
    "2. **Bootstrap Sampling**: Random Forest employs a technique called bootstrap sampling, where multiple random subsets of the original dataset are created by sampling with replacement. Each subset, known as a bootstrap sample, has the same size as the original dataset but may contain duplicate instances.\n",
    "\n",
    "3. **Tree Construction**: For each bootstrap sample, a decision tree is constructed. Each tree is trained on a subset of features randomly selected from the total feature set. This random subset of features helps introduce diversity among the trees.\n",
    "\n",
    "4. **Decision Tree Training**: The decision tree is trained using a recursive process called recursive binary splitting. At each node of the tree, the algorithm selects the best split among a subset of features based on a splitting criterion (e.g., Gini impurity or information gain). This process continues recursively until a stopping criterion is met, such as reaching a maximum tree depth or the minimum number of samples required to split a node.\n",
    "\n",
    "5. **Ensemble Aggregation**: Once all the decision trees are constructed, predictions are made by aggregating the individual predictions of each tree. For classification tasks, the class with the majority vote among the trees is selected as the final prediction. For regression tasks, the average of the predicted values from all the trees is taken.\n",
    "\n",
    "6. **Out-of-Bag Evaluation**: Random Forest has a built-in evaluation mechanism called out-of-bag (OOB) evaluation. Since each decision tree is trained on a different bootstrap sample, the instances that were not included in the bootstrap sample (out-of-bag instances) can be used to evaluate the performance of the model without the need for a separate validation set.\n",
    "\n",
    "7. **Feature Importance**: Random Forest can provide an estimate of feature importance. During the tree construction process, the algorithm keeps track of how much each feature contributes to reducing impurity or error. This information can be used to rank the features based on their importance.\n",
    "\n",
    "8. **Prediction**: Once the Random Forest model is trained, it can be used to make predictions on new, unseen data by passing the data through each individual decision tree and aggregating the results.\n",
    "\n",
    "The steps mentioned above outline the general execution process of the Random Forest algorithm. The number of decision trees, the maximum tree depth, and other hyperparameters can be specified to customize the Random Forest model for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6625a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Score:  0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Create a KFold object for k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=kf)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
