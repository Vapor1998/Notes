{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135e08a8",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting) is a popular machine learning algorithm used for supervised learning tasks like regression and classification. It is based on the gradient boosting framework and is known for its efficiency and effectiveness in handling structured data. In this explanation, I'll provide an overview of XGBoost along with key steps and formulas, presented in LaTeX format.\n",
    "\n",
    "### Step 1: Objective Function\n",
    "\n",
    "The objective of XGBoost is to minimize a regularized objective function, which is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{Objective Function} = \\sum_{i=1}^{n} \\left[ L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k) \\right]\n",
    "$$\n",
    "\n",
    "- $n$ is the number of training examples.\n",
    "- $L(y_i, \\hat{y}_i)$ is the loss function measuring the difference between the true label $y_i$ and the predicted label $\\hat{y}_i$.\n",
    "- $K$ is the number of trees (weak learners) in the ensemble.\n",
    "- $\\Omega(f_k)$ is the regularization term for the $k$-th tree.\n",
    "\n",
    "### Step 2: Initialize the Model\n",
    "\n",
    "Initialize the model with a constant prediction, often the mean of the target variable:\n",
    "\n",
    "$$\n",
    "f_0(x) = \\text{argmin}_\\gamma \\sum_{i=1}^{n} L(y_i, \\gamma)\n",
    "$$\n",
    "\n",
    "### Step 3: For each tree in the ensemble:\n",
    "\n",
    "#### a. Compute Residuals\n",
    "\n",
    "Calculate the negative gradient of the loss function with respect to the predicted values $\\hat{y}_i$:\n",
    "\n",
    "$$\n",
    "r_{ik} = -\\left[\\frac{\\partial L(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}\\right]_{\\hat{y}_i=f_{k-1}(x_i)}\n",
    "$$\n",
    "\n",
    "#### b. Fit a Weak Learner (Regression Tree)\n",
    "\n",
    "Train a regression tree $h_k(x)$ to predict the residuals $r_{ik}$. This tree captures the error that the previous trees have made.\n",
    "\n",
    "#### c. Update the Ensemble\n",
    "\n",
    "Update the ensemble model with the new tree:\n",
    "\n",
    "$$\n",
    "f_k(x) = f_{k-1}(x) + \\eta h_k(x)\n",
    "$$\n",
    "\n",
    "Where $\\eta$ is the learning rate, a hyperparameter controlling the step size when adding the new tree.\n",
    "\n",
    "#### d. Regularization Term\n",
    "\n",
    "Add a regularization term to the objective function to prevent overfitting:\n",
    "\n",
    "$$\n",
    "\\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $T$ is the number of leaves in the tree $h_k(x)$.\n",
    "- $w_j$ is the score associated with leaf $j$.\n",
    "- $\\gamma$ and $\\lambda$ are hyperparameters controlling regularization.\n",
    "\n",
    "### Step 4: Model Prediction\n",
    "\n",
    "To make a final prediction, use the ensemble of trees:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = f_K(x) = \\sum_{k=1}^{K} \\eta h_k(x)\n",
    "$$\n",
    "\n",
    "### Step 5: Regularization and Hyperparameter Tuning\n",
    "\n",
    "Regularization parameters like $\\eta$, $\\gamma$, and $\\lambda$ can be tuned using techniques such as cross-validation to find the optimal values for your specific dataset.\n",
    "\n",
    "### Step 6: Model Evaluation\n",
    "\n",
    "Evaluate the model's performance using appropriate evaluation metrics (e.g., RMSE for regression or accuracy for classification) on a validation or test dataset.\n",
    "\n",
    "XGBoost iteratively adds trees to the ensemble, with each tree aiming to correct the errors of the previous ones, making it a powerful and robust algorithm for various machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a04d87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc744135400469189b8fe2c94c07c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:02<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HASHIM RAZI\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HASHIM RAZI\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdc12b1bbb34cb89dcf313068a96e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79c6557c8ce45ddbed86210209b8ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0ad17ffe1e4508b9e8702ffcfcf5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Apple, Label: I-ORG, Score: 0.9995648264884949\n",
      "Entity: Inc, Label: I-ORG, Score: 0.9994916915893555\n",
      "Entity: American, Label: I-MISC, Score: 0.9970537424087524\n",
      "Entity: Cup, Label: I-LOC, Score: 0.9946129322052002\n",
      "Entity: ##ert, Label: I-LOC, Score: 0.8585067987442017\n",
      "Entity: ##ino, Label: I-LOC, Score: 0.9911662936210632\n",
      "Entity: California, Label: I-LOC, Score: 0.9981574416160583\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Define a NER pipeline\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d063796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Parties, Label: I-MISC, Score: 0.6970688700675964\n",
      "Entity: Parties, Label: I-MISC, Score: 0.8338868021965027\n"
     ]
    }
   ],
   "source": [
    "# Perform NER on text\n",
    "text = 'Article 4 : The Parties will consult together whenever, in the opinion of any of them, the territorial integrity, political independence or security of any of the Parties is threatened.'\n",
    "entities = nlp_ner(text)\n",
    "\n",
    "# Print the extracted entities\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49939dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b42ada455a74aaabc843d3400a22bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd10bcbdf094889acd536004512c5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78870b2fe3e14f1ab3f9e9b26700987d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87506ceb74494fde92d693de9f5b28da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a nice puppet'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc1d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
