{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3f1593",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a type of supervised machine learning algorithm used for binary classification tasks, where the dependent variable can take only two possible outcomes (e.g., yes/no, true/false). It aims to predict the probability of an event occurring based on input features. Unlike linear regression, which predicts continuous values, logistic regression outputs probabilities between 0 and 1, representing the likelihood of the positive class.\n",
    "\n",
    "The algorithm uses the logistic function (sigmoid) to transform the linear combination of input features and their corresponding coefficients into probabilities. The sigmoid function maps any real number to a value in the range (0, 1), making it ideal for probability estimation.\n",
    "\n",
    "The key assumption of logistic regression is that the relationship between the independent variables and the log odds of the outcome is linear. This assumption enables the algorithm to model the probability of the event occurring.\n",
    "\n",
    "To train the logistic regression model, a cost function, typically the log-loss (cross-entropy), is minimized. The log-loss quantifies the difference between predicted probabilities and the actual binary outcomes, driving the model to make accurate predictions.\n",
    "\n",
    "The coefficients obtained during model training represent the log odds of the outcome associated with each predictor variable. A positive coefficient indicates a positive relationship with the log odds of the event occurring, while a negative coefficient suggests a negative relationship.\n",
    "\n",
    "Interpreting the coefficients allows us to understand the impact of each predictor on the outcome. Higher positive coefficients imply stronger positive influence, while higher negative coefficients imply stronger negative influence.\n",
    "\n",
    "Logistic regression assumes that the observations are independent and that there is no perfect multicollinearity among the predictor variables. It also requires a sufficiently large sample size for stable estimates.\n",
    "\n",
    "In cases where multicollinearity exists, regularization techniques like L1 (Lasso) and L2 (Ridge) can be applied to control the model's complexity and reduce overfitting.\n",
    "\n",
    "Logistic regression can be extended to handle multi-class classification problems using techniques like one-vs-rest (OvR) or softmax regression (multinomial logistic regression).\n",
    "\n",
    "Model evaluation is typically done using metrics such as accuracy, precision, recall, specificity, and the F1-score, which help assess the performance of the classifier.\n",
    "\n",
    "The receiver operating characteristic (ROC) curve and the area under the curve (AUC) are also used to visualize and compare different models' performance.\n",
    "\n",
    "Logistic regression can handle high-dimensional datasets and is particularly effective when the classes are linearly separable.\n",
    "\n",
    "However, it may not perform well on datasets with complex non-linear relationships, and its performance may be limited if the assumptions of the model are violated.\n",
    "\n",
    "Logistic regression finds applications in various fields, such as medicine (e.g., disease prediction), marketing (e.g., customer churn prediction), finance (e.g., credit risk assessment), and more.\n",
    "\n",
    "It is relatively easy to implement and computationally efficient, making it a popular choice for many binary classification problems.\n",
    "\n",
    "The choice of features and data preprocessing play a crucial role in the performance of logistic regression models.\n",
    "\n",
    "Outliers can have a significant impact on the model's performance and may require appropriate handling.\n",
    "\n",
    "Addressing class imbalance is essential for improving the model's ability to correctly predict the minority class.\n",
    "\n",
    "Regularization is a useful technique to prevent overfitting and enhance the model's generalization ability to unseen data.\n",
    "\n",
    "In summary, logistic regression is a fundamental and widely used algorithm in machine learning for binary classification tasks. It models the probability of an event occurring based on input features, allowing us to make informed decisions and predictions in various real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd2291",
   "metadata": {},
   "source": [
    "## Sigmoid Function    \n",
    "\n",
    "It looks like \n",
    "![Sigmoid Function](https://miro.medium.com/v2/resize:fit:640/format:webp/1*Xu7B5y9gp0iL5ooBj7LtWw.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c574dfb",
   "metadata": {},
   "source": [
    "As we can see, peak tries to reach 1 and bottom tries to reach 0.  \n",
    "- As z goes towards ->   ${-\\infty}$, $e^{-z}$ goes towards -> ${\\infty}$,  \n",
    "   and S will go towards 0.\n",
    "- As z goes towards -> ${\\infty}$, $e^{-z}$ which is $e^{-\\infty}$ -> 0,   \n",
    "   so S goes towards 1/1 which is 1\n",
    "- When z = 0, it is at origin which is 0.5\n",
    "\n",
    "So we need to exploit sigmoid function in logistic regression.  \n",
    "\n",
    "###### In Logistic Regression, our output will actually be \n",
    "\n",
    "\\begin{equation*}\n",
    "h(x) = \\frac{1}{1 + e^{-g(x)}}\n",
    "\\end{equation*}\n",
    "\n",
    "where $g(x) = mx + c$ Which is linear regression line.  \n",
    "Lets say we have features x1 x2 and x3, so it will be like m1x1, m2x2 and m3x3.  \n",
    "And lets say we going to add extra $x_{n+1}$ which is basically 1.  \n",
    "So we can say that above equation is $m^{T}x$ where T is Transpose,  \n",
    "where m is effectively, [$m_{1}$, $m_{2}$, ... $m_{n}$] and $m_{n}$ (last element of m)is c,  \n",
    "and x is [$x_{1}$, $x_{2}$, 1] where last is extra 1.  \n",
    "\n",
    "  \n",
    "  \n",
    "So our predictions will be,  \n",
    "\n",
    "\\begin{equation*}\n",
    "h(x) = \\frac{1}{1 + e^{-m^{T}x}}\n",
    "\\end{equation*}  \n",
    "\n",
    "lets say if **h(x)  >  0.5, ypred = 1, and h(x) <= 0.5, ypred = 0**.  \n",
    "\n",
    "Now what does $\\frac{1}{1 + e^{-z}}$ = 0.5 mean?\n",
    "we have seen if z > 0 -> s(z) > 0.5,  \n",
    "    if z <= 0, s(z) <= 0.  \n",
    "\n",
    "If $m^{T}x$  > 0, h(x) > 0.5, ypred -> 1.  \n",
    "If $m^{T}x$  <= 0, h(x) <= 0.5, ypred -> 0.    \n",
    "\n",
    "---\n",
    "\n",
    "Now we have sigmoid function and we need to find the parameters.Parameters are..\n",
    "\n",
    "lets say the hypothesis is  $$ h(x) = s(m^{T}x)  \\frac{1}{1 + e^{-g(x)}}$$ .  \n",
    "What we need is we need to find the m.The best fit m such that error is minimized.  \n",
    "  \n",
    "    \n",
    "      \n",
    "How do we define the error function?\n",
    "The error function depends upon the output, and the actual Y(prediction) = E(h(x), y).  \n",
    "\n",
    "So the error fuction in linear regression looks like  $$\\sum_{i = 1}^{m} (y^{i} - h(x))^2$$  \n",
    "where **h(x) = $m^{T}x$.** \n",
    "  \n",
    "But now, **h(x)** is $$\\frac{1}{1 + e^{-g(x)}}$$ which is complicated.  \n",
    "And for this **h(x)** if we use the same error function(linear regression one) it creates problems.  \n",
    "The problem it creates is there is many local minimas.  \n",
    "Instead of using this error function, we need to use the error function so that for $$\\frac{1}{1 + e^{-g(x)}}$$  it should be convex.  \n",
    "Convex function means there will be only one local minima.  \n",
    "So instead of using error function as $$\\sum_{i = 1}^{m} (y^{i} - h(x))^2$$ we will use a different one.  \n",
    "So our function will be splitted into two parts.  \n",
    "\n",
    "```python\n",
    "    if y == 1:  \n",
    "        E(h(x), y) =  -log(h(x))\n",
    "    else:  \n",
    "        E(h(x), y) =  -log(1 - h(x))\n",
    "```\n",
    "\n",
    "  \n",
    "    \n",
    "For a particular data,  \n",
    "\n",
    "if y == 1:    \n",
    "\\   $E(h(x^i), y^i) ->  -log(h(x^i))$  \n",
    "else:    \n",
    "\\   $E(h(x^i), y^i) ->  -log(1 - h(x^i))$  \n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "In a simplified form, we can combine the both equations in the form,  \n",
    "$$E(h(x^i), y^i) = -y^ilog(h(x^i) - (1-y^i)log(1-h(x^i))$$ \n",
    "\n",
    "This is the error function of ith training data.  \n",
    "\n",
    "So overall error on whole x and y data willbe  \n",
    "$$E(x, y) = \\frac{1}{m} \\sum_{i=1}^m -y^ilog(h(x^i) - (1-y^i)log(1-h(x^i))$$   \n",
    "We did a small change by adding $\\frac{1}{m}$ because training data is huge so that our error must not become huge we are kind of finding average error per training data point.  \n",
    "This error function is convex.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Now that we have found our error function to be  \n",
    "\n",
    "$$E(x, y) = \\frac{1}{m} \\sum_{i=1}^m -y^ilog(h(x^i) - (1-y^i)log(1-h(x^i))$$   \n",
    "\n",
    "What we want to do is we want to find the optimal m values. To do so, we are planning to use gradient descent as we have done earlier.  j\n",
    "The idea of gradient descent is lets start with particular value of m, lets update m.To update particular m we use  \n",
    "$$m_{j} = m_{j} - \\alpha \\frac{\\partial E}{\\partial m_{j}}$$.  \n",
    "\n",
    "Now to find  $\\frac{\\partial E}{\\partial m_{j}}$, we need to calculate $\\frac{\\partial E(x, y, m)}{\\partial m_{j}}$\n",
    "where this is also dependent upon x.\n",
    "What we are supposed to do is we are supposed to take derivative of $$E(x, y) = \\frac{1}{m} \\sum_{i=1}^m -y^ilog(h(x^i) - (1-y^i)log(1-h(x^i))$$ w.r.t particular parameter which is $ \\frac{1}{1+e^{m^T_x}}$\n",
    "\n",
    "----\n",
    "\n",
    "So our error function was  \n",
    "$$E(m) = \\frac{1}{m} \\sum_{i=1}^m -y^ilog(h(x^i) - (1-y^i)log(1-h(x^i))$$   \n",
    "\n",
    "\n",
    "Before we try to differentiate this, lets check what is $log(h(x^i))$  \n",
    "  \n",
    "   \n",
    "$$log(h(x^i)) = \\frac{1}{1 + e^{-m^{T}x^{i}}}$$\n",
    "\n",
    "Now using **log(a/b)** property, we get =  \n",
    "$$ log(1) - log(1 + e^{-m^{T}x^{i}}) \\quad = \\quad -log(1 + e^{-m^{T}x^{i}})   \\quad               .......... (1)  $$  \n",
    "\n",
    "$\\quad$\n",
    "  \n",
    "    \n",
    "Now what is $log(1 - h(x^i))$   \n",
    "  \n",
    "$$log(1 - h(x^i)) =  log(1 - \\frac{1}{1 + e^{-m^{T}x^{i}}})$$  \n",
    "\n",
    "$log(1 - \\frac{1}{1 + e^{-m^{T}x^{i}}})$  can be simplified into  \n",
    "\n",
    "$$ log(e^{-m^{T}x^{i}}) - log(1+ e^{-m^{T}x^{i}})\\quad..........(2)$$ \n",
    "\n",
    "\n",
    "Now, lets put (1) and (2) in error function, we get, \n",
    "For now, we are not assuming $\\frac{1}{m}$, we are focusing on internal part  \n",
    "\n",
    "$$E(m) = -y^i(-log(1+ e^{-m^{T}x^{i}})) - (1-y^i)log(e^{-m^{T}x^{i}}) - log(1+ e^{-m^{T}x^{i}})$$    \n",
    "solving the above equation,  \n",
    "$$ E(m) = y^ilog(e^{-m^{T}x^{i}}) - log(e^{-m^{T}x^{i}}) + log(1+ e^{-m^{T}x^{i}}) + y^ilog(e^{-m^{T}x^{i}}) - y^ilog(1+e^{-m^{T}x^{i}})$$   \n",
    "$$ = {m^{T}x^{i}} + y^i(m^{T}x^{i}) + log(1+e^{-m^{T}x^{i}})  \\quad ........ since \\ loge^z = z$$\n",
    "  \n",
    "$\\quad$   \n",
    "      \n",
    "        \n",
    "\n",
    "$$ E^i(m)= {m^{T}x^{i}} + y^i(m^{T}x^{i}) + log(1+e^{-m^{T}x^{i}}) $$ \n",
    "\n",
    "\n",
    "  ----\n",
    "  \n",
    "  $$E^i(m) = {m^{T}x^{i}} - y^i(m^{T}x^{i}) + log(1+e^{-m^{T}x^{i}}) $$ \n",
    "  \n",
    "  $\\qquad$\n",
    "The above is the equation we derived for ith value , so lets simplify further.  \n",
    "$$ = log{m^{T}x^{i}} - y^i(m^{T}x^{i}) + log(1+e^{-m^{T}x^{i}}) $$\n",
    "$\\quad$\n",
    "$$ = log{1 + e^{m^{T}x^{i}}} - y^i(m^{T}x^{i})  \\quad  \\scriptsize{ ............. \\ using \\ loga +  logb = log(a+b)}\n",
    "$$  \n",
    "  \n",
    "    \n",
    "Now lets find what is $\\frac{\\partial E^i(m)}{\\partial m_{j}}$,   \n",
    "\n",
    "before differentiating, lets find what is $y^i(m^{T}x^{i})$, \n",
    "\n",
    "$y^i(m^{T}x^{i}) = y^i \\sum_{j}m_{j}x_{j}^i$ this basically means for all the parameters, my ith training datapoint.\n",
    "\n",
    "Now differentiating,  \n",
    "$$\\frac{\\partial E^i (m)}{\\partial m_{j}} = \\frac{1}{1+e^{m^{T}x^{i}}} * e^{m^{T}x^{i}} * x_{j}^i  - y_{i}x_{j}^i$$  \n",
    "$$ = -(y^i - \\frac{1}{1+e^{m^{T}x^{i}}} * e^{m^{T}x^{i}}) * x_{j}^i   \\quad \\scriptsize { \\ ......... took \\ minus \\ sign \\ and \\ x_{j}^i \\ common.}  $$\n",
    "Now, dividing numerator and denominator by $e^{m^{T}x^{i}})$, we get\n",
    "\n",
    "$$  = -(y^i - \\frac{1}{1+e^{-m^{T}x^{i}}}) * x_{j}^i $$   \n",
    "\n",
    "Now, if we look carefully, $\\frac{1}{1+e^{-m^{T}x^{i}}}$ = $h(x_{i})$,    \n",
    "  \n",
    "\n",
    "So, $\\frac{\\partial E^i(m)}{\\partial m_{j}} = -(y^i - \\frac{1}{1+e^{-m^{T}x^{i}}}) * x_{j}^i $ this is the error looks like for ith training data point. \n",
    "$\\quad$\n",
    "\n",
    "For Complete data, \n",
    "$$ \\frac{\\partial E(m)}{\\partial m_{j}} = \\frac{1}{m} \\sum_{i=1} -(y^i - \\frac{1}{1+e^{-m^{T}x^{i}}}) * x_{j}^i $$  \n",
    "As we have learned Gradient descent before, we need to find,  \n",
    "$$m_{j} = m_{j} - \\alpha \\frac{\\partial E(m)}{\\partial m_{j}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fd8e1",
   "metadata": {},
   "source": [
    "### LR from Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4c3dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 31), (31,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets,preprocessing\n",
    "\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "breast_cancer.DESCR.split(\"\\n\")\n",
    "\n",
    "X = breast_cancer.data\n",
    "Y = breast_cancer.target\n",
    "\n",
    "X_df = pd.DataFrame(X)\n",
    "X_df.describe()\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "X_train,X_val,Y_train,Y_val = model_selection.train_test_split(X,Y)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "\n",
    "X_train_scaled.shape\n",
    "\n",
    "\n",
    "#### Inserting column of ones in dataset\n",
    "\n",
    "X_train_scaled = np.append(X_train_scaled,np.ones(X_train_scaled.shape[0]).reshape(-1,1),axis=1)\n",
    "\n",
    "\n",
    "X_train_scaled.shape,X_train_scaled[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f38477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(agg):\n",
    "    return 1/(1+np.exp(-agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bafa2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X_train,Y_train,m):\n",
    "    \n",
    "    cost_ = 0\n",
    "    N = X_train.shape[0]\n",
    "    for i in range(N):\n",
    "        agg = (X_train[i]*m).sum()\n",
    "        h = sigmoid(agg)\n",
    "        cost = -Y_train[i]*np.log(h) - (1-Y_train[i])*np.log(1-h)\n",
    "        cost_ += cost\n",
    "    \n",
    "    return cost_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b04d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(X_train,Y_train,lr,m):\n",
    "    \n",
    "    N = X_train.shape[0]\n",
    "    slope_m = np.zeros(X_train.shape[1])\n",
    "    for i in range(N):\n",
    "        agg = (X_train[i]*m).sum()\n",
    "        h = sigmoid(agg)\n",
    "        slope_m+=(-1/N)*(Y_train[i]-h)*X_train[i]\n",
    "        \n",
    "    m = m - lr*slope_m\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be5dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train,Y_train,epochs=100,lr=0.01):\n",
    "    \n",
    "    m = np.zeros(X_train.shape[1])\n",
    "    cost_array = []\n",
    "    unit = epochs//100\n",
    "    for i in range(epochs):\n",
    "        m = step_gradient(X_train,Y_train,lr,m)\n",
    "        cost_ = cost(X_train,Y_train,m)\n",
    "        cost_array.append(cost_)\n",
    "        if i%unit==0:\n",
    "            print(\"Epoch:{}, Cost:{}\".format(i,cost_))\n",
    "    \n",
    "    return m,cost_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78991d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test,m):\n",
    "    \n",
    "    y_pred = []\n",
    "    N = X_test.shape[0]\n",
    "    for i in range(N):\n",
    "        agg = (X_test[i]*m).sum()\n",
    "        h = sigmoid(agg)\n",
    "        if h>=0.5:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            \n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2948676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y_test,Y_pred):\n",
    "    \n",
    "    correct = 0\n",
    "    N = Y_test.shape[0]\n",
    "    correct = (Y_test==Y_pred).sum()\n",
    "    \n",
    "    return (correct/N)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b65d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Cost:287.3949631395047\n",
      "Epoch:50, Cost:144.58017433787987\n",
      "Epoch:100, Cost:110.31433155433054\n",
      "Epoch:150, Cost:93.55460361835422\n",
      "Epoch:200, Cost:83.20104518787481\n",
      "Epoch:250, Cost:76.02837005213217\n",
      "Epoch:300, Cost:70.70378187007415\n",
      "Epoch:350, Cost:66.56056441287038\n",
      "Epoch:400, Cost:63.223539108330066\n",
      "Epoch:450, Cost:60.46394108635781\n",
      "Epoch:500, Cost:58.133762055777794\n",
      "Epoch:550, Cost:56.13270998739433\n",
      "Epoch:600, Cost:54.390237971778866\n",
      "Epoch:650, Cost:52.85516313382668\n",
      "Epoch:700, Cost:51.489371178056075\n",
      "Epoch:750, Cost:50.263840962337774\n",
      "Epoch:800, Cost:49.156046362304984\n",
      "Epoch:850, Cost:48.148207238480744\n",
      "Epoch:900, Cost:47.22608123172834\n",
      "Epoch:950, Cost:46.378109994110574\n",
      "Epoch:1000, Cost:45.59480361581722\n",
      "Epoch:1050, Cost:44.86828874900164\n",
      "Epoch:1100, Cost:44.19197150352009\n",
      "Epoch:1150, Cost:43.56028227464229\n",
      "Epoch:1200, Cost:42.96848002060837\n",
      "Epoch:1250, Cost:42.412500321134274\n",
      "Epoch:1300, Cost:41.88883611708942\n",
      "Epoch:1350, Cost:41.39444315026672\n",
      "Epoch:1400, Cost:40.92666428553824\n",
      "Epoch:1450, Cost:40.48316842084813\n",
      "Epoch:1500, Cost:40.06190077770625\n",
      "Epoch:1550, Cost:39.661042150791666\n",
      "Epoch:1600, Cost:39.27897527017827\n",
      "Epoch:1650, Cost:38.91425685485043\n",
      "Epoch:1700, Cost:38.56559425383188\n",
      "Epoch:1750, Cost:38.23182581086542\n",
      "Epoch:1800, Cost:37.91190427095404\n",
      "Epoch:1850, Cost:37.60488268705906\n",
      "Epoch:1900, Cost:37.309902393548015\n",
      "Epoch:1950, Cost:37.02618269740128\n",
      "Epoch:2000, Cost:36.75301200445171\n",
      "Epoch:2050, Cost:36.489740150293045\n",
      "Epoch:2100, Cost:36.235771747137356\n",
      "Epoch:2150, Cost:35.99056039121497\n",
      "Epoch:2200, Cost:35.75360360211451\n",
      "Epoch:2250, Cost:35.52443838714367\n",
      "Epoch:2300, Cost:35.302637341424386\n",
      "Epoch:2350, Cost:35.08780520884043\n",
      "Epoch:2400, Cost:34.879575840785165\n",
      "Epoch:2450, Cost:34.67760949941062\n",
      "Epoch:2500, Cost:34.48159046015654\n",
      "Epoch:2550, Cost:34.291224875055796\n",
      "Epoch:2600, Cost:34.10623886392209\n",
      "Epoch:2650, Cost:33.926376805223136\n",
      "Epoch:2700, Cost:33.751399802398105\n",
      "Epoch:2750, Cost:33.581084304711396\n",
      "Epoch:2800, Cost:33.415220864562336\n",
      "Epoch:2850, Cost:33.25361301556722\n",
      "Epoch:2900, Cost:33.09607625777736\n",
      "Epoch:2950, Cost:32.942437138143546\n",
      "Epoch:3000, Cost:32.79253241583445\n",
      "Epoch:3050, Cost:32.646208303305464\n",
      "Epoch:3100, Cost:32.50331977512304\n",
      "Epoch:3150, Cost:32.36372993750885\n",
      "Epoch:3200, Cost:32.2273094523967\n",
      "Epoch:3250, Cost:32.09393601051958\n",
      "Epoch:3300, Cost:31.96349384866712\n",
      "Epoch:3350, Cost:31.83587330680505\n",
      "Epoch:3400, Cost:31.710970421223514\n",
      "Epoch:3450, Cost:31.588686550302512\n",
      "Epoch:3500, Cost:31.468928029848442\n",
      "Epoch:3550, Cost:31.351605855281246\n",
      "Epoch:3600, Cost:31.236635388234692\n",
      "Epoch:3650, Cost:31.123936085385594\n",
      "Epoch:3700, Cost:31.0134312475499\n",
      "Epoch:3750, Cost:30.905047787280154\n",
      "Epoch:3800, Cost:30.79871601337493\n",
      "Epoch:3850, Cost:30.69436943086531\n",
      "Epoch:3900, Cost:30.591944555183453\n",
      "Epoch:3950, Cost:30.491380739340237\n",
      "Epoch:4000, Cost:30.392620013050838\n",
      "Epoch:4050, Cost:30.29560693284481\n",
      "Epoch:4100, Cost:30.20028844228644\n",
      "Epoch:4150, Cost:30.10661374150952\n",
      "Epoch:4200, Cost:30.014534165343466\n",
      "Epoch:4250, Cost:29.92400306937038\n",
      "Epoch:4300, Cost:29.834975723311164\n",
      "Epoch:4350, Cost:29.747409211190742\n",
      "Epoch:4400, Cost:29.661262337779654\n",
      "Epoch:4450, Cost:29.57649554085185\n",
      "Epoch:4500, Cost:29.493070808836002\n",
      "Epoch:4550, Cost:29.41095160347444\n",
      "Epoch:4600, Cost:29.330102787133978\n",
      "Epoch:4650, Cost:29.250490554441676\n",
      "Epoch:4700, Cost:29.172082367946146\n",
      "Epoch:4750, Cost:29.094846897526665\n",
      "Epoch:4800, Cost:29.01875396329636\n",
      "Epoch:4850, Cost:28.94377448176303\n",
      "Epoch:4900, Cost:28.869880415032064\n",
      "Epoch:4950, Cost:28.797044722849233\n",
      "[-0.55954922 -0.63169024 -0.54517592 -0.5714886  -0.1600114   0.00693481\n",
      " -0.48528483 -0.57721608 -0.09041072  0.32168495 -0.61754766  0.11846445\n",
      " -0.44709852 -0.52796791 -0.14084455  0.42003557  0.11082985 -0.07511645\n",
      "  0.17319797  0.42216581 -0.7618386  -0.84009927 -0.69598767 -0.73067568\n",
      " -0.63411018 -0.17160908 -0.53417816 -0.69039838 -0.56450744 -0.16281039\n",
      "  0.50638724]\n"
     ]
    }
   ],
   "source": [
    "m,cost_array = fit(X_train_scaled,Y_train,5000,0.01)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b784578a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost_array)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b77ebce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = predict(X_train_scaled,m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55518c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.3006993006993"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(Y_train,y_pred_train)\n",
    "\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_val_scaled = np.append(X_val_scaled,np.ones(X_val_scaled.shape[0]).reshape(-1,1),axis=1)\n",
    "\n",
    "y_pred_val = predict(X_val_scaled,m)\n",
    "\n",
    "accuracy(Y_val,y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1636626b",
   "metadata": {},
   "source": [
    "## LR for NLP\n",
    "\n",
    "Steps:\n",
    "- Suppose you have Positive and Negative tweet, first prepare a vocabulary of all the sentences in corpus. \n",
    "- Then add the frequency of positive and Negative frequency of words in the corpus. It will look like for following sentence\n",
    "```python\n",
    "\"I am happy because i am learning NLP\"\n",
    " \"I am sad because i am not learning NLP\"\n",
    "```\n",
    "\n",
    "| Vocabulary | PosFreq(1)  | NegFreq(0)  |\n",
    "|------------|-------------|-------------|\n",
    "| am         | 2           | 2           |\n",
    "| because    | 2           | 2           |\n",
    "| happy      | 1           | 0           |\n",
    "| I          | 2           | 2           |\n",
    "| learning   | 1           | 1           |\n",
    "| NLP        | 1           | 1           |\n",
    "| not        | 0           | 1           |\n",
    "| sad        | 0           | 1           |\n",
    "\n",
    "- Create Dictionary in the following way.  $$X_m = [1, \\quad \\sum_{w}freqs(w, 1), \\quad \\sum_{w}freqs(w, 0) ]$$ of where 1 is bias, second is count of freq of words in positive tweet, second is for negative and w is word.  \n",
    "\n",
    "- So for the above corpus, $X_m$ will look like $$X_m = [1, 8, 11]$$\n",
    "- Preprocess handles, urls, stopwords and punctuations from the sentence.  \n",
    "- Perform Stemming and Lemmatization as well.   \n",
    "So for n sentences, the whole data looks like \n",
    "$[1, X_{1}^1, X_{2}^2]$\n",
    "\n",
    "- Training LR  \n",
    "  \n",
    "<img src= \"https://editor.analyticsvidhya.com/uploads/29525YGmjEyR0Sw2poxMkdBsNeQ_74cb9a1075fb4d1eb835b14a8d5b2456_Screen-Shot-2020-09-01-at-8.39.39-AM.png\" width=\"650\"/>\n",
    "\n",
    "- Find the accuracy \n",
    "<img src= \"https://editor.analyticsvidhya.com/uploads/94665xq8RYoHvROKvEWKB73TiUg_ac2e78d0c6654f58ab40822d08b68465_Screen-Shot-2020-09-02-at-10.47.33-AM.png\" width=\"650\"/>\n",
    "\n",
    "check more at [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/11/create-a-pipeline-to-perform-sentiment-analysis-using-nlp/) and check NLP course 1 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab126a",
   "metadata": {},
   "source": [
    "# Questions\n",
    "`1. Question: What is logistic regression?`  \n",
    "   Answer: Logistic regression is a supervised machine learning algorithm used for binary classification tasks, where the dependent variable takes only two possible outcomes. It predicts the probability of an event occurring based on input features using the logistic function. \n",
    "   \n",
    "   ---\n",
    "\n",
    "`2. Question: How does logistic regression differ from linear regression?`  \n",
    "   Answer: While both regression methods deal with predicting outcomes, linear regression is used for continuous dependent variables, whereas logistic regression is used for discrete binary outcomes. Linear regression produces continuous values, while logistic regression outputs probabilities in the range (0, 1).\n",
    "   \n",
    "   ---\n",
    "\n",
    "`3. Question: What is the logistic function (sigmoid function)?`  \n",
    "   Answer: The logistic function, also known as the sigmoid function, is given by f(x) = 1 / (1 + e^(-x)). It maps any real number to a value between 0 and 1, making it suitable for converting linear combinations of features and coefficients to probabilities in logistic regression.\n",
    "   \n",
    "   ---\n",
    "\n",
    "`4. Question: What are the assumptions of logistic regression?`  \n",
    "   Answer: The main assumptions of logistic regression are linearity between the log odds of the outcome and the predictors, independence of observations, absence of multicollinearity, and a large enough sample size for stable estimates.\n",
    "   \n",
    "   ---\n",
    "\n",
    "`5. Question: How do you interpret the coefficients in logistic regression?`  \n",
    "   Answer: In logistic regression, the coefficients represent the log odds of the outcome associated with each predictor variable. A positive coefficient indicates a positive relationship with the log odds of the event occurring, while a negative coefficient suggests a negative relationship.\n",
    "   \n",
    "   ---\n",
    "\n",
    "`6. Question: What is the logit function in logistic regression?`  \n",
    "   Answer: The logit function is the inverse of the logistic function and is used to convert probabilities back to the log odds scale. It is given by g(p) = ln(p / (1 - p)), where 'p' is the probability of the event occurring.\n",
    "   \n",
    "   ---\n",
    "\n",
    "`7. Question: How is the cost function (log-loss) used in logistic regression model training?`  \n",
    "   Answer: The log-loss (cross-entropy) function is used as the cost function in logistic regression to quantify the difference between predicted probabilities and the actual binary outcomes. The goal during training is to minimize this loss, adjusting the model's parameters (coefficients) accordingly.\n",
    "   \n",
    "   ---\n",
    "\n",
    "`8. Question: How do you evaluate the performance of a logistic regression model?`  \n",
    "   Answer: Common evaluation metrics for logistic regression include accuracy, precision, recall (sensitivity), specificity, and the F1-score. The receiver operating characteristic (ROC) curve and area under the curve (AUC) are also used to assess model performance.\n",
    "   \n",
    "   ---\n",
    "\n",
    "`9. Question: How do you handle multicollinearity in logistic regression?`  \n",
    "   Answer: Multicollinearity occurs when predictor variables are highly correlated. To address this, one can use techniques such as removing correlated features, using dimensionality reduction methods, or applying regularization (e.g., L1 or L2 regularization).\n",
    "\n",
    "`10. Question: How do you handle class imbalance in logistic regression?`  \n",
    "    Answer: Class imbalance occurs when one class has significantly more samples than the other. Techniques to address this issue include resampling methods (e.g., oversampling the minority class or undersampling the majority class) or using cost-sensitive learning algorithms that penalize misclassification of the minority class more heavily.\n",
    "    \n",
    "    ---\n",
    "\n",
    "`11. Question: Can logistic regression be used for multi-class classification? If so, how?`   \n",
    "    Answer: Yes, logistic regression can be extended to handle multi-class classification using techniques like one-vs-rest (OvR) or softmax regression (multinomial logistic regression). OvR trains multiple binary classifiers, one for each class against the rest, while softmax regression generalizes logistic regression for multiple classes.  \n",
    "    \n",
    "    ---\n",
    "    \n",
    "`12. Question: What are some common regularization techniques used in logistic regression?`  \n",
    "    Answer: Regularization helps prevent overfitting. Common techniques include L1 regularization (Lasso), which adds the absolute value of the coefficients to the cost function, and L2 regularization (Ridge), which adds the square of the coefficients. Regularization controls the magnitude of coefficients and helps avoid excessive influence from individual features.  \n",
    "    \n",
    "    ---\n",
    "\n",
    "`13. Question: Explain the process of feature selection in logistic regression.`  \n",
    "    Answer: Feature selection involves choosing the most relevant features to improve model performance and reduce complexity. Techniques include univariate feature selection (e.g., using statistical tests like chi-square), recursive feature elimination, and regularization-based methods that automatically select important features during model training.  \n",
    "    \n",
    "    ---\n",
    "\n",
    "`14. Question: What is stepwise logistic regression, and what are its advantages and disadvantages?`  \n",
    "    Answer: Stepwise logistic regression is an automated feature selection method that sequentially adds or removes features based on their statistical significance. Advantages include its simplicity and ability to handle large feature sets. However, it may lead to overfitting and lacks robustness compared to other feature selection approaches.  \n",
    "    \n",
    "    ---\n",
    "\n",
    "`15. Question: How do you deal with outliers in logistic regression?`  \n",
    "    Answer: Outliers can have a significant impact on logistic regression models. Possible approaches include removing outliers, transforming the variables, or using robust logistic regression techniques that are less influenced by extreme values.\n",
    "\n",
    "---\n",
    "\n",
    "`16. Question: Explain the concept of cross-entropy in the context of logistic regression.`\n",
    "    Answer: Cross-entropy, also known as log-loss, measures the dissimilarity between predicted probabilities and the actual binary outcomes. It quantifies the information loss between the predicted and true probability distributions, and the aim during model training is to minimize this loss to improve the model's accuracy.\n",
    "    \n",
    "    ---\n",
    "\n",
    "`17. Question: What is the difference between accuracy and precision in logistic regression evaluation?`\n",
    "    Answer: Accuracy measures the overall correctness of predictions, considering both true positives and true negatives. Precision, on the other hand, measures the proportion of correctly predicted positive cases (true positives) out of all predicted positive cases, and it helps assess the model's performance in correctly identifying positive instances.\n",
    "\n",
    "  ---\n",
    "  \n",
    "`18. Question: How is the ROC curve analysis useful in assessing logistic regression models?`\n",
    "    Answer: The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various probability thresholds. It helps visualize the trade-off between sensitivity and specificity and allows comparison of different models based on their AUC values, which represents the overall performance of the classifier.\n",
    "\n",
    "---\n",
    "\n",
    "`19. Question: What are the advantages and disadvantages of logistic regression?`\n",
    "    Answer: Advantages of logistic regression include its simplicity, fast training, and interpretability of coefficients. It is effective for linearly separable data and can handle high-dimensional datasets. However, logistic regression may not perform well on complex datasets with non-linear relationships, and its performance may be limited when assumptions are not met.\n",
    "    \n",
    "  ---\n",
    "\n",
    "`20. Question: Can you explain the concept of regularization in logistic regression?`\n",
    "    Answer: Regularization in logistic regression adds penalty terms to the cost function to control the magnitude of coefficients. L1 regularization (Lasso) adds the sum of the absolute values of coefficients, promoting sparsity by setting some coefficients to exactly zero. L2 regularization (Ridge) adds the sum of the squared coefficients, shrinking them towards zero without eliminating any. Regularization prevents overfitting and helps improve the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f839a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
