{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f76307",
   "metadata": {},
   "source": [
    "# SVM (Support Vector Machine)  \n",
    "\n",
    "SVM works by mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable. A separator between the categories is found, then the data is transformed in such a way that the separator could be drawn as a hyperplane. Following this, characteristics of new data can be used to predict the group to which a new record should belong.\n",
    "  \n",
    "#### Goal of SVM  \n",
    "The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.  \n",
    "  \n",
    "SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane.  \n",
    "\n",
    "<img src=\"https://www.kaggleusercontent.com/kf/96203833/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..aJbY55ADFmiI5jgyN60vuA.sYsYSHG8N7pCsz7JmiUf1oFtcPUanar3jrIxJuiUTaqWev_96CeLU6-o0erbKdpTGigc8qY3mI78MI0tFgW8PJR3Bo-wioIw-p_e4FnzVnIut5UOaoSYrcxZDRaTHMO3Wrn2WPbAnBCermNAOaufmIzPNT1MWFmKtmUNxXEyVF0fT6KAvl1ydzeoUlx4UbO6RTt5acgbF9fkED7_K2ZSANrXoubc-6pQAHe4lkc-_5j4spIeK9Z3dseMDDuxNX8tN_GsdbjsuX_LchzYfHVkVrOmBlXxvghx5az2v9NRHcn0t4ApK1xxdmZQcJ3KpDH99aL9zW39_D1iwurAVXlnEFBhyzNg7fNFpoAzdMtGu5EiXRud6bTmMbstliH-dn6sCRT_VkOmqUD63aA4VqrkLa_xH15_IvwPmrzU-6rO_n__uCdK803fDmn7IecDDSaDi1JuoS7yGqR9IZtxPLqTyG_OKKV39DANyBwz_rBn18HdWFLDoE10ArTPmNjYTWBdeUUuHIGtSHYUvH9AKaZWp8sMtk9iQG9l-xcb_MVj-Bz1a_uBgkoPkDs71xl8Fa7z_hEKHUbPRWJ6hdUcfYWUrnYZ2_WYmwG-_8oQPik6OxsarMxig5-LLiEBI3UlfsEQUd06ueffqFq_p_rsdAHDQ_et6GaJLO4CQyzbgPQMoywp0-AcwCuU_O6PVaVM0jjytzx-N1lNRlxHut6qrO0OAw.eCizIh-48cp2MBhhNrH5LA/__results___files/__results___3_0.png\" width=\"850\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0eac73",
   "metadata": {},
   "source": [
    "# Types of SVM\n",
    "#### SVM have two types:\n",
    "\n",
    "1. Linear SVM: Linear SVM is used for linearly separable data, which means if a dataset can be classified into two classes by using a single straight line, then such data is termed as linearly separable data, and classifier is used called as Linear SVM classifier.\n",
    "<img src=\"https://www.kaggleusercontent.com/kf/96203833/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..aJbY55ADFmiI5jgyN60vuA.sYsYSHG8N7pCsz7JmiUf1oFtcPUanar3jrIxJuiUTaqWev_96CeLU6-o0erbKdpTGigc8qY3mI78MI0tFgW8PJR3Bo-wioIw-p_e4FnzVnIut5UOaoSYrcxZDRaTHMO3Wrn2WPbAnBCermNAOaufmIzPNT1MWFmKtmUNxXEyVF0fT6KAvl1ydzeoUlx4UbO6RTt5acgbF9fkED7_K2ZSANrXoubc-6pQAHe4lkc-_5j4spIeK9Z3dseMDDuxNX8tN_GsdbjsuX_LchzYfHVkVrOmBlXxvghx5az2v9NRHcn0t4ApK1xxdmZQcJ3KpDH99aL9zW39_D1iwurAVXlnEFBhyzNg7fNFpoAzdMtGu5EiXRud6bTmMbstliH-dn6sCRT_VkOmqUD63aA4VqrkLa_xH15_IvwPmrzU-6rO_n__uCdK803fDmn7IecDDSaDi1JuoS7yGqR9IZtxPLqTyG_OKKV39DANyBwz_rBn18HdWFLDoE10ArTPmNjYTWBdeUUuHIGtSHYUvH9AKaZWp8sMtk9iQG9l-xcb_MVj-Bz1a_uBgkoPkDs71xl8Fa7z_hEKHUbPRWJ6hdUcfYWUrnYZ2_WYmwG-_8oQPik6OxsarMxig5-LLiEBI3UlfsEQUd06ueffqFq_p_rsdAHDQ_et6GaJLO4CQyzbgPQMoywp0-AcwCuU_O6PVaVM0jjytzx-N1lNRlxHut6qrO0OAw.eCizIh-48cp2MBhhNrH5LA/__results___files/__results___5_0.png\" width=\"450\"/>\n",
    "\n",
    "\n",
    "2. Non-linear SVM: Non-Linear SVM is used for non-linearly separated data, which means if a dataset cannot be classified by using a straight line, then such data is termed as non-linear data and classifier used is called as Non-linear SVM classifier.\n",
    "<img src=\"https://www.kaggleusercontent.com/kf/96203833/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..aJbY55ADFmiI5jgyN60vuA.sYsYSHG8N7pCsz7JmiUf1oFtcPUanar3jrIxJuiUTaqWev_96CeLU6-o0erbKdpTGigc8qY3mI78MI0tFgW8PJR3Bo-wioIw-p_e4FnzVnIut5UOaoSYrcxZDRaTHMO3Wrn2WPbAnBCermNAOaufmIzPNT1MWFmKtmUNxXEyVF0fT6KAvl1ydzeoUlx4UbO6RTt5acgbF9fkED7_K2ZSANrXoubc-6pQAHe4lkc-_5j4spIeK9Z3dseMDDuxNX8tN_GsdbjsuX_LchzYfHVkVrOmBlXxvghx5az2v9NRHcn0t4ApK1xxdmZQcJ3KpDH99aL9zW39_D1iwurAVXlnEFBhyzNg7fNFpoAzdMtGu5EiXRud6bTmMbstliH-dn6sCRT_VkOmqUD63aA4VqrkLa_xH15_IvwPmrzU-6rO_n__uCdK803fDmn7IecDDSaDi1JuoS7yGqR9IZtxPLqTyG_OKKV39DANyBwz_rBn18HdWFLDoE10ArTPmNjYTWBdeUUuHIGtSHYUvH9AKaZWp8sMtk9iQG9l-xcb_MVj-Bz1a_uBgkoPkDs71xl8Fa7z_hEKHUbPRWJ6hdUcfYWUrnYZ2_WYmwG-_8oQPik6OxsarMxig5-LLiEBI3UlfsEQUd06ueffqFq_p_rsdAHDQ_et6GaJLO4CQyzbgPQMoywp0-AcwCuU_O6PVaVM0jjytzx-N1lNRlxHut6qrO0OAw.eCizIh-48cp2MBhhNrH5LA/__results___files/__results___7_0.png\" width=\"450\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f09cc1",
   "metadata": {},
   "source": [
    "# Optimal Hyperplane\n",
    "Suppose we have a dataset that has two tags (green and blue), and the dataset has two features x1 and x2. We want a classifier that can classify the pair(x1, x2) of coordinates in either green or blue. The SVM algorithm helps to find the best line or decision boundary. SVM algorithm finds the closest point of the lines from both the classes. These points are called support vectors. The distance between the vectors and the hyperplane is called as margin. And the goal of SVM is to maximize this margin. The hyperplane with maximum margin is called the optimal hyperplane.  \n",
    "<img src=\"https://www.kaggleusercontent.com/kf/96203833/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..aJbY55ADFmiI5jgyN60vuA.sYsYSHG8N7pCsz7JmiUf1oFtcPUanar3jrIxJuiUTaqWev_96CeLU6-o0erbKdpTGigc8qY3mI78MI0tFgW8PJR3Bo-wioIw-p_e4FnzVnIut5UOaoSYrcxZDRaTHMO3Wrn2WPbAnBCermNAOaufmIzPNT1MWFmKtmUNxXEyVF0fT6KAvl1ydzeoUlx4UbO6RTt5acgbF9fkED7_K2ZSANrXoubc-6pQAHe4lkc-_5j4spIeK9Z3dseMDDuxNX8tN_GsdbjsuX_LchzYfHVkVrOmBlXxvghx5az2v9NRHcn0t4ApK1xxdmZQcJ3KpDH99aL9zW39_D1iwurAVXlnEFBhyzNg7fNFpoAzdMtGu5EiXRud6bTmMbstliH-dn6sCRT_VkOmqUD63aA4VqrkLa_xH15_IvwPmrzU-6rO_n__uCdK803fDmn7IecDDSaDi1JuoS7yGqR9IZtxPLqTyG_OKKV39DANyBwz_rBn18HdWFLDoE10ArTPmNjYTWBdeUUuHIGtSHYUvH9AKaZWp8sMtk9iQG9l-xcb_MVj-Bz1a_uBgkoPkDs71xl8Fa7z_hEKHUbPRWJ6hdUcfYWUrnYZ2_WYmwG-_8oQPik6OxsarMxig5-LLiEBI3UlfsEQUd06ueffqFq_p_rsdAHDQ_et6GaJLO4CQyzbgPQMoywp0-AcwCuU_O6PVaVM0jjytzx-N1lNRlxHut6qrO0OAw.eCizIh-48cp2MBhhNrH5LA/__results___files/__results___9_0.png\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a88c2",
   "metadata": {},
   "source": [
    "# SVM Kernel Functions\n",
    "A kernel is a function used in SVM for helping to solve problems. With the help of kernel we can go to higher dimensions and perform smooth calculations. We can go up to an infinite number of dimensions using kernels. Kernel plays a vital role in classification and is used to analyze some patterns in the given dataset. They are very helpful in solving a non-linear problem by using a linear classifier.  \n",
    "  \n",
    "Sometimes, we cannot have a hyperplane for certain problems. This problem arises when we go up to higher dimensions and try to form a hyperplane. We have various svm kernel functions to convert the non-linear data to linear. In this notebook, we listed 8 such popular svm kernel functions.  \n",
    "<img src=\"https://www.kaggleusercontent.com/kf/96203833/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..aJbY55ADFmiI5jgyN60vuA.sYsYSHG8N7pCsz7JmiUf1oFtcPUanar3jrIxJuiUTaqWev_96CeLU6-o0erbKdpTGigc8qY3mI78MI0tFgW8PJR3Bo-wioIw-p_e4FnzVnIut5UOaoSYrcxZDRaTHMO3Wrn2WPbAnBCermNAOaufmIzPNT1MWFmKtmUNxXEyVF0fT6KAvl1ydzeoUlx4UbO6RTt5acgbF9fkED7_K2ZSANrXoubc-6pQAHe4lkc-_5j4spIeK9Z3dseMDDuxNX8tN_GsdbjsuX_LchzYfHVkVrOmBlXxvghx5az2v9NRHcn0t4ApK1xxdmZQcJ3KpDH99aL9zW39_D1iwurAVXlnEFBhyzNg7fNFpoAzdMtGu5EiXRud6bTmMbstliH-dn6sCRT_VkOmqUD63aA4VqrkLa_xH15_IvwPmrzU-6rO_n__uCdK803fDmn7IecDDSaDi1JuoS7yGqR9IZtxPLqTyG_OKKV39DANyBwz_rBn18HdWFLDoE10ArTPmNjYTWBdeUUuHIGtSHYUvH9AKaZWp8sMtk9iQG9l-xcb_MVj-Bz1a_uBgkoPkDs71xl8Fa7z_hEKHUbPRWJ6hdUcfYWUrnYZ2_WYmwG-_8oQPik6OxsarMxig5-LLiEBI3UlfsEQUd06ueffqFq_p_rsdAHDQ_et6GaJLO4CQyzbgPQMoywp0-AcwCuU_O6PVaVM0jjytzx-N1lNRlxHut6qrO0OAw.eCizIh-48cp2MBhhNrH5LA/__results___files/__results___32_0.jpg\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73ea7c",
   "metadata": {},
   "source": [
    "### Linear Kernel\n",
    "It is the most basic type of kernel, usually one dimensional in nature. It proves to be the best function when there are lots of features.  \n",
    "  \n",
    "Linear kernel functions are faster than other functions. \n",
    "  \n",
    "Linear Kernel Formula\n",
    "F(x, xj) = sum( x.xj)  \n",
    "   \n",
    "Here, x, xj represents the data we’re trying to classify.\n",
    "  \n",
    "Now we will make our svc classifier using a linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460ec32",
   "metadata": {},
   "source": [
    "#### Gaussian RBF kernel\n",
    "It is one of the most preferred and used kernel functions in svm. It is usually chosen for non-linear data. It helps to make proper separation when there is no prior knowledge of data.   \n",
    "  \n",
    "Gaussian Radial Basis Formula\n",
    "F(x, xj) = exp(-gamma * ||x - xj||^2)  \n",
    "  \n",
    "The value of gamma varies from 0 to 1. We have to manually provide the value of gamma in the code. The most preferred value for gamma is 0.1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c13af5",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel\n",
    "It is a more generalized representation of the linear kernel. It is not as preferred as other kernel functions as it is less efficient and accurate.\n",
    "\n",
    "Polynomial Kernel Formula\n",
    "F(x, xj) = (x.xj+1)^d\n",
    "\n",
    "Here ‘.’ shows the dot product of both the values, and d denotes the degree.\n",
    "\n",
    "F(x, xj) representing the decision boundary to separate the given classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457895a",
   "metadata": {},
   "source": [
    "#### Sigmoid Kernel\n",
    "It is mostly preferred for neural networks. This kernel function is similar to a two-layer perceptron model of the neural network, which works as an activation function for neurons.\n",
    "\n",
    "Sigmoid Kenel Formula\n",
    "F(x, xj) = tanh(αxay + c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
