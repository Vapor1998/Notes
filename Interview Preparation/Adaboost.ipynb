{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4aef04",
   "metadata": {},
   "source": [
    "# [AdaBoost](https://towardsdatascience.com/a-mathematical-explanation-of-adaboost-4b0c20ce4382) \n",
    "Adaboost, short for Adaptive Boosting, is an ensemble learning algorithm that combines weak learners (classifiers with weak predictive power) to create a strong classifier. It is an iterative algorithm that assigns weights to each training sample, focusing on the misclassified samples in subsequent iterations.\n",
    "\n",
    "Step 1: Initialize the weights for each training sample as $w_i = \\frac{1}{N}$, where $N$ is the total number of samples.\n",
    "\n",
    "Step 2: For $t = 1$ to $T$ (the number of iterations):\n",
    "  \n",
    "  a) Train a weak learner $G_t(x)$ on the training data using the current sample weights.\n",
    "  \n",
    "  b) Calculate the weighted error of the weak learner as $\\epsilon_t = \\sum_{i=1}^{N} w_i^{(t)} \\cdot I(y_i \\neq G_t(x_i))$, where $w_i^{(t)}$ is the weight of sample $i$ at iteration $t$ and $I(\\cdot)$ is the indicator function.\n",
    "  \n",
    "  c) Calculate the weight of the weak learner as $\\alpha_t = \\frac{1}{2} \\ln \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$.\n",
    "  \n",
    "  d) Update the sample weights as $w_i^{(t+1)} = \\frac{w_i^{(t)} \\cdot \\exp(-\\alpha_t \\cdot y_i \\cdot G_t(x_i))}{Z_t}$, where $Z_t$ is the normalization factor (the sum of all updated weights).\n",
    "  \n",
    "Step 3: Repeat steps 2 until $T$ iterations are completed.\n",
    "\n",
    "Step 4: The final boosted model is given by $F(x) = \\text{sign} \\left(\\sum_{t=1}^{T} \\alpha_t \\cdot G_t(x)\\right)$.\n",
    "\n",
    "The derivation of Adaboost involves minimizing the exponential loss function by iteratively updating the weights of misclassified samples. The weight update equation ensures that the subsequent weak learners focus more on the misclassified samples from the previous iterations. The weight $\\alpha_t$ of each weak learner depends on its weighted error, emphasizing more accurate weak learners.\n",
    "\n",
    "The final prediction is obtained by aggregating the predictions of all weak learners, where each weak learner's contribution is weighted by its importance (determined by $\\alpha_t$). The sign function ensures that the predictions are binary.\n",
    "\n",
    "Adaboost effectively combines the weak learners to create a strong ensemble model that improves the overall prediction performance. It is widely used in classification tasks and has shown good generalization capabilities.\n",
    "\n",
    "Please note that this is a brief summary of the derivation process. The actual derivation involves more mathematical details and proofs, which can be found in the original paper by Freund and Schapire (1997) titled \"A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ebff2",
   "metadata": {},
   "source": [
    "         +---------------------+\n",
    "         |                     |\n",
    "         |    Initialize       |\n",
    "         |   sample weights    |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Training          |\n",
    "         |   Iterations        |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Weak Learner      |\n",
    "         |   Training          |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Calculate         |\n",
    "         |   Weighted Error    |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Calculate         |\n",
    "         |   Weak Learner      |\n",
    "         |   Weight            |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Update            |\n",
    "         |   Sample Weights    |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Repeat            |\n",
    "         |   Iterations        |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Final             |\n",
    "         |   Prediction        |\n",
    "         |                     |\n",
    "         +---------+-----------+\n",
    "                   |\n",
    "                   v\n",
    "         +---------------------+\n",
    "         |                     |\n",
    "         |   Ensemble          |\n",
    "         |   Model             |\n",
    "         |                     |\n",
    "         +---------------------+\n",
    "         \n",
    "         \n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Here's how it works in simple terms:\n",
    "\n",
    "1. Imagine you have a group of people who are not very good at solving a problem individually. These people are like the weak learners in Adaboost.\n",
    "\n",
    "2. You give each person a different set of data and ask them to make predictions about the problem. Their predictions might not be very accurate on their own.\n",
    "\n",
    "3. Now, you analyze the predictions made by each person and pay more attention to the predictions that were wrong. You want to focus on the areas where they struggled.\n",
    "\n",
    "4. In the next round, you give more importance to the predictions that were incorrect. This means you try to make those predictions better by adjusting the parameters of the weak learners.\n",
    "\n",
    "5. You repeat this process for several rounds, each time adjusting the parameters of the weak learners based on their previous performance.\n",
    "\n",
    "6. Finally, you combine the predictions from all the weak learners, giving more weight to those who performed better overall.\n",
    "\n",
    "By doing this, Adaboost creates a strong model that learns from the mistakes of the weak learners and improves its accuracy over time. It's like having a group of people work together, where each person focuses on the areas they are not good at, and the group as a whole becomes better at solving the problem.\n",
    "         \n",
    "         \n",
    "\n",
    "1. Initialize Sample Weights:\n",
    "   - Each training sample is assigned an initial weight, typically set to 1/N, where N is the total number of samples.\n",
    "\n",
    "2. Training Iteration:\n",
    "   - Adaboost performs a series of training iterations.\n",
    "\n",
    "3. Weak Learner Training:\n",
    "   - In each iteration, a weak learner (e.g., decision stump) is trained on the training data.\n",
    "   - The weak learner aims to minimize the weighted error, taking into account the sample weights.\n",
    "\n",
    "4. Calculate Weighted Error:\n",
    "   - The weighted error of the weak learner is calculated as the sum of weights of misclassified samples.\n",
    "   - It measures the performance of the weak learner on the weighted training data.\n",
    "\n",
    "5. Calculate Weak Learner Weight:\n",
    "   - The weight of the weak learner is determined based on its weighted error.\n",
    "   - The weight emphasizes more accurate weak learners by assigning higher weights to those with lower errors.\n",
    "   - The weight is calculated using a formula: alpha = 0.5 * ln((1 - weighted_error) / weighted_error).\n",
    "\n",
    "6. Update Sample Weights:\n",
    "   - The sample weights are updated to emphasize the misclassified samples from the current weak learner.\n",
    "   - The weights of correctly classified samples are reduced, while the weights of misclassified samples are increased.\n",
    "   - The updated weights ensure that the misclassified samples have higher weights for the next iteration.\n",
    "\n",
    "7. Repeat Iterations:\n",
    "   - Steps 3 to 6 are repeated for a specified number of iterations or until a stopping criterion is met.\n",
    "\n",
    "8. Final Prediction:\n",
    "   - After all iterations are completed, the weak learners are combined to form the final boosted model.\n",
    "   - The prediction for a new sample is determined by aggregating the predictions of all weak learners, weighted by their respective alpha values.\n",
    "   - Typically, a weighted majority vote or weighted sum is used to make the final prediction.\n",
    "\n",
    "9. Ensemble Model:\n",
    "   - The ensemble model consists of the combination of weak learners, each with its respective weight.\n",
    "   - The ensemble model is capable of making more accurate predictions than individual weak learners.\n",
    "\n",
    "The diagram illustrates the step-by-step process of Adaboost, highlighting the training iterations, the calculation of weighted error and weak learner weight, and the updating of sample weights. Ultimately, Adaboost creates an ensemble model by combining weak learners to make accurate predictions on unseen data.\n",
    "\n",
    "-------------\n",
    "\n",
    "Given: Data x  \n",
    "**Step 1:**  \n",
    "- Initialize weights $w_{i}$ = for every i  \n",
    "- Start with the null classifier $f_{0}(x)$ = $g_{0}(x)$ = 0  \n",
    "  \n",
    "\n",
    "\n",
    "|Row# | F1|  F2   | F3   | Y  | Wgt|\n",
    "|-----|---|-------|------|----|----|\n",
    "|1    | 83|  0.30 | 73   | +  |0.1 |\n",
    "|2    | 91|  0.06 |  7   | +  |0.1 |\n",
    "|3    | 98|  0.41 | 42   |+   |0.1 |\n",
    "|4    | 95| 0.16  |29    |+   |0.1 |\n",
    "|5    | 89|   0.71|  99  |  + |0.1 |\n",
    "|6    | 73|   0.81| 37   |  - |0.1 |\n",
    "|7    | 58|   0.66|  82  | -  |0.1 |\n",
    "|8    | 32|  0.65 | 36   | -  |0.1 |\n",
    "|9    | 13|  0.11 |91    | -  |0.1 |\n",
    "|10   | 82| 0.28  |91    |-   |0.1 |\n",
    "\n",
    "\n",
    "**Step 2:**\n",
    "- For t = 1 to T:  \n",
    "    Generate training dataset by sampling with $w_i$  \n",
    "    - Undersampling those which are correctly predicted datapoints.(whose weights are lower)    \n",
    "    - oversampling those which are Incorrect datapoints.(whose weights are higher)      \n",
    "    - If the weights are higher, it will be oversampling, if it is lower, it will be undersampling. \n",
    "- Fit some weak learner $g_t$ ($g_t$ is algorithm of choice. In this case, it is decision tree.)  \n",
    " \n",
    " \n",
    "**Step 3:**\n",
    "- Lets assume $g_t$ makes some prediction     \n",
    "\n",
    "|Row# | F1|  F2   | F3   | Y  | Wgt|$g_t$ Prediction| Error|\n",
    "|-----|---|-------|------|----|----|----------------|------|\n",
    "|1    | 83|  0.30 | 73   | +  |0.1 |+               |0     |\n",
    "|2    | 91|  0.06 |  7   | +  |0.1 |+               |0     |\n",
    "|3    | 98|  0.41 | 42   |+   |0.1 |-               |1     |\n",
    "|4    | 95| 0.16  |29    |+   |0.1 |-               |1     | \n",
    "|5    | 89|   0.71|  99  |  + |0.1 |-               |1     |\n",
    "|6    | 73|   0.81| 37   |  - |0.1 |-               |0     |\n",
    "|7    | 58|   0.66|  82  | -  |0.1 |-               |0     |\n",
    "|8    | 32|  0.65 | 36   | -  |0.1 |-               |0     |\n",
    "|9    | 13|  0.11 |91    | -  |0.1 |-               |0     |\n",
    "|10   | 82| 0.28  |91    |-   |0.1 |-               |0     |  \n",
    "\n",
    "- In the above table, we can see, in error column 1 means incorrect prediction and 0 for correct prediction.  \n",
    "- AdaBoost loss function = \n",
    "$L(f) = \\frac{1}{N}\\sum{e^{-y_i.f(x_i)}}$    \n",
    "\n",
    "  \n",
    "**Step 4:**\n",
    "- set $\\lambda_t$ = $\\frac{1}{2}\\log\\frac{1 - e_t}{e_t}$, $\\lambda_t$ is weight of each model. $\\lambda_t$ is also called \"Amount of Say\"  \n",
    "- $e_t$  is the Total Error is equal to the sum of the weights of the incorrectly classified samples\n",
    "\n",
    "So in this example, error, ($e_T$) becomes 0.1+0.1+0.1 = 0.3  \n",
    "And $\\lambda_t$ becomes 0.42.  \n",
    "**Step 5:**  \n",
    "- update the weights.\n",
    "    - $w_i$ = $w_i * e^{\\lambda_t}$ if wrongly classified by $g_i$\n",
    "    - $w_i$ = $w_i * e^{- \\lambda_t}$ if correctly classified by $g_i$  \n",
    "    \n",
    "    \n",
    "|Row# | F1|  F2   | F3   | Y  | Wgt|New Weight| Error|\n",
    "|-----|---|-------|------|----|----|----------|------|\n",
    "|1    | 83|  0.30 | 73   | +  |0.1 |0.065     |0     |\n",
    "|2    | 91|  0.06 |  7   | +  |0.1 |0.065     |0     |\n",
    "|3    | 98|  0.41 | 42   |+   |0.1 |0.153     |1     |\n",
    "|4    | 95| 0.16  |29    |+   |0.1 |0.153     |1     | \n",
    "|5    | 89|   0.71|  99  |  + |0.1 |0.153     |1     |\n",
    "|6    | 73|   0.81| 37   |  - |0.1 |0.065     |0     |\n",
    "|7    | 58|   0.66|  82  | -  |0.1 |0.065     |0     |\n",
    "|8    | 32|  0.65 | 36   | -  |0.1 |0.065     |0     |\n",
    "|9    | 13|  0.11 |91    | -  |0.1 |0.065     |0     |\n",
    "|10   | 82| 0.28  |91    |-   |0.1 |0.065     |0     |  \n",
    "\n",
    "So weight for correct classifier = 0.065 and for incorrect classifier = 0.153  \n",
    "\n",
    "**step 6:**  \n",
    "- Normalize the $w_i$ to sum to one  \n",
    "- We need to multiply each New weight to sum of new weights  \n",
    "    - Normalized Weights = $\\frac{New\\ Weight_i}{Total\\ of\\ New\\ Weight}$\n",
    "    - Total of Normalized weights adds to 1.  \n",
    "    \n",
    "|Row# | F1|  F2   | F3   | Y  | Wgt|New Weight   | Normalized Sample weights|\n",
    "|-----|---|-------|------|----|----|-------------|--------------------------|\n",
    "|1    | 83|  0.30 | 73   | +  |0.1 |0.065        |0.071                     |\n",
    "|2    | 91|  0.06 |  7   | +  |0.1 |0.065        |0.071                     |\n",
    "|3    | 98|  0.41 | 42   |+   |0.1 |0.153        |0.167                     |\n",
    "|4    | 95| 0.16  |29    |+   |0.1 |0.153        |0.167                     | \n",
    "|5    | 89|   0.71|  99  |  + |0.1 |0.153        |0.167                     |\n",
    "|6    | 73|   0.81| 37   |  - |0.1 |0.065        |0.071                     |\n",
    "|7    | 58|   0.66|  82  | -  |0.1 |0.065        |0.071                     |\n",
    "|8    | 32|  0.65 | 36   | -  |0.1 |0.065        |0.071                     |\n",
    "|9    | 13|  0.11 |91    | -  |0.1 |0.065        |0.071                     |\n",
    "|10   | 82| 0.28  |91    |-   |0.1 |0.065        |0.071                     |  \n",
    "|     |   |       |      |    |    |Total = 0.917|  Total = 1               |  \n",
    "\n",
    "\n",
    "**Step 7:**  \n",
    "- The New Model is $f_t = f_{t-1} + \\lambda_t g_t$  \n",
    "- Output of the final model is  $f_T(x) = sgn(\\sum^{T}_{t=1}\\lambda_t g_t)$  .   \n",
    "\n",
    " \n",
    "--------\n",
    "Summary:  \n",
    "\n",
    "\n",
    "- Initialize weights $w_{i}$ = for every i  \n",
    "- Start with the null classifier $f_{0}(x)$ = $g_{0}(x)$ = 0  \n",
    "- For t = 1 to T:  \n",
    "    - Generate training dataset by sampling with $w_i$ \n",
    "    - set $\\lambda_t$ = $\\frac{1}{2}\\log\\frac{1 - e_t}{e_t}$ \n",
    "    - update the weights.\n",
    "        - $w_i$ = $w_i * e^{\\lambda_t}$ if wrongly classified by $g_i$\n",
    "        - $w_i$ = $w_i * e^{- \\lambda_t}$ if correctly classified by $g_i$\n",
    "    - Normalize the $w_i$ to sum to one  \n",
    "    - The New Model is $f_t = f_{t-1} + \\lambda_t g_t$  \n",
    "\n",
    "- Output of the final model is  $f_T(x) = sgn(\\sum^{T}_{t=1}\\lambda_t g_t)$  .   \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4442954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an AdaBoost classifier\n",
    "adaboost_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
