{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=np.genfromtxt('0000000000002419_training_ccpp_x_y_train.csv',delimiter=',')\n",
    "#df_training.head()\n",
    "x_train=df_training[:,:-1]\n",
    "y_train=df_training[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([482.26, 446.94, 452.56, ..., 437.65, 459.97, 444.42])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.47827466e+00, -1.24764165e+00,  1.30221067e+00,\n",
       "         7.58624590e-01],\n",
       "       [ 2.89012041e-01,  3.06797549e-01,  6.61749044e-01,\n",
       "        -4.46921842e-01],\n",
       "       [-3.99975582e-01, -4.21012529e-01, -2.87207194e-01,\n",
       "         3.75010552e-01],\n",
       "       ...,\n",
       "       [ 1.36062192e+00,  1.18048335e+00, -6.54382840e-01,\n",
       "        -5.83682640e-01],\n",
       "       [-4.36097263e-01,  9.29089763e-04,  7.84140927e-01,\n",
       "        -6.59584883e-01],\n",
       "       [ 1.40209496e+00,  6.07960340e-01, -4.31394890e-01,\n",
       "        -1.73110573e+00]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "SS=StandardScaler()\n",
    "x_train[:,:]=SS.fit_transform(x_train[:,:])\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x,y, m, c):\n",
    "    total_cost = 0\n",
    "    M = len(x)\n",
    "    for i in range(M):\n",
    "        x1 = x[i, :]\n",
    "        y1 = y[i]\n",
    "        total_cost += (1/M)*((y1 - (m*x1).sum() - c)**2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_calc(x,y, learning_rate, m , c):\n",
    "    m_slope = list(0 for ni in range(x.shape[1]))\n",
    "    c_slope = 0\n",
    "    rows = len(x)\n",
    "    a=0\n",
    "    for i in range(rows):\n",
    "        x1 = x[i, :]\n",
    "        y1 = y[i]\n",
    "        a=m*x1\n",
    "        #print(a.shape)\n",
    "        for j in range(x1.shape[0]):\n",
    "            m_slope[j]+=(-2/rows)* (y1 - (a.sum(axis=0)) - c)*x1[j]\n",
    "        c_slope += (-2/rows)* (y1 - (a.sum(axis=0)) - c)\n",
    "    m_slope=np.array(m_slope)\n",
    "    new_m = m - (learning_rate*m_slope)\n",
    "    new_c = c - (learning_rate*c_slope)\n",
    "    return new_m, new_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y, learning_rate, num_iterations):\n",
    "    m=[0 for i in range(x.shape[1])] \n",
    "    c = 0\n",
    "    for i in range(num_iterations):\n",
    "        m, c = mc_calc(x,y, learning_rate, m , c)\n",
    "        print(i, \" Cost: \", cost(x,y, m, c))\n",
    "    return m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_iterations = 300\n",
    "m, c = gradient_descent(x_train,y_train, learning_rate, num_iterations)\n",
    "print(m, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=np.genfromtxt('0000000000002419_test_ccpp_x_test.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.05179958, -0.9803295 ,  0.73940279,  1.20354042],\n",
       "       [-1.03554677, -1.28089884, -0.09850361,  0.5680886 ],\n",
       "       [ 0.97438468,  1.64050792, -1.29819445,  0.38358038],\n",
       "       ...,\n",
       "       [ 0.62359476,  0.94554072, -0.70944148,  1.27238677],\n",
       "       [ 0.51117945, -0.91910241,  0.1216388 , -0.55754924],\n",
       "       [ 0.27686804,  0.47003684,  0.97490397,  0.80216619]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[:,:]=SS.fit_transform(df_test[:,:])\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.array((m*df_test).sum(axis=1))+c\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('y_pred_ccpp.csv',y_pred,fmt='%10.10f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes are we pass m as array with last element as c in it.\n",
    "# We will not have m_slope or c_slope as well, we will have another array whoch will start with all 0's and we will update \n",
    "# these values within this array.  \n",
    "\n",
    "def step_gradient(X, Y, learning_rate, m):\n",
    "    \n",
    "    # m_slope = 0\n",
    "    # c_slope = 0\n",
    "    \n",
    "    features = X.shape[1]\n",
    "    mslope  = np.array([0.0 for i in range(features)])   # make sure we take numpy array here or else it will give typeerror.\n",
    "    M = len(X)\n",
    "    for i in range(M): # going through all data\n",
    "        x = X[i, :]\n",
    "        y = Y[i]\n",
    "        \n",
    "        # calculating m*x for a row here\n",
    "        # this will return an array of mx.\n",
    "        mx = m*x\n",
    "        for j in range(features):\n",
    "            mslope[j] +=   (-2/M)* (y - mx.sum(axis = 0) ) * x[j]  \n",
    "    new_m = m - learning_rate*mslope\n",
    "    m = new_m\n",
    "    return m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(X, Y, learning_rate, num_iterations):\n",
    "    m = [0 for i in range(X.shape[1])]\n",
    "    for i in range(num_iterations):\n",
    "        m= step_gradient(X,Y, learning_rate, m)\n",
    "        print(i, 'Cost: ', cost(X, Y, m))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, Y, m):\n",
    "    total_cost = 0\n",
    "    M = len(X)\n",
    "    \n",
    "    for i in range(M):\n",
    "        x = X[i, :]\n",
    "        y = Y[i]\n",
    "        total_cost += (1/M) * ( y - (m*x).sum() )**2\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.c_[x_train, np.ones(x_train.shape[0])]\n",
    "df_test = np.c_[df_test, np.ones(df_test.shape[0])]\n",
    "\n",
    "def run():\n",
    "    #data = np.loadtxt(\"https://files.codingninjas.in/data-6984.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 200\n",
    "    \n",
    "    # here we adding columns with all 1's for Generic Case.\n",
    "    X = x_train\n",
    "    Y = y_train\n",
    "    #for i in X:\n",
    "    #    i = np.append(i, 1.0)\n",
    "\n",
    "    m = gd(X, Y, learning_rate, num_iterations)\n",
    "    print(m, m[-1])# As last element of m is c.\n",
    "    return m\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  132273.17028197853\n",
      "1 Cost:  84643.21738144128\n",
      "2 Cost:  54177.474227792685\n",
      "3 Cost:  34683.656935452964\n",
      "4 Cost:  22208.479529620818\n",
      "5 Cost:  14224.382169986968\n",
      "6 Cost:  9114.383468776608\n",
      "7 Cost:  5843.781284088756\n",
      "8 Cost:  3750.4051743364307\n",
      "9 Cost:  2410.4727339052597\n",
      "10 Cost:  1552.762193756394\n",
      "11 Cost:  1003.6891423474597\n",
      "12 Cost:  652.1571417929908\n",
      "13 Cost:  427.06242464503333\n",
      "14 Cost:  282.8969056987255\n",
      "15 Cost:  190.5340535532469\n",
      "16 Cost:  131.33178845157047\n",
      "17 Cost:  93.35828546206275\n",
      "18 Cost:  68.97644681535036\n",
      "19 Cost:  53.297930953967374\n",
      "20 Cost:  43.193704743608556\n",
      "21 Cost:  36.66077597552082\n",
      "22 Cost:  32.41688558517924\n",
      "23 Cost:  29.641097514004986\n",
      "24 Cost:  27.807765771435207\n",
      "25 Cost:  26.58026504888727\n",
      "26 Cost:  25.742971704330984\n",
      "27 Cost:  25.157726328859923\n",
      "28 Cost:  24.73596559178298\n",
      "29 Cost:  24.42088326947476\n",
      "30 Cost:  24.176010622787736\n",
      "31 Cost:  23.977905690256787\n",
      "32 Cost:  23.81147271061384\n",
      "33 Cost:  23.6669651644581\n",
      "34 Cost:  23.53806659977331\n",
      "35 Cost:  23.420661452103346\n",
      "36 Cost:  23.312047631015602\n",
      "37 Cost:  23.21043197232065\n",
      "38 Cost:  23.114606832432933\n",
      "39 Cost:  23.02374270009555\n",
      "40 Cost:  22.937255128393236\n",
      "41 Cost:  22.85471928725651\n",
      "42 Cost:  22.775815037704607\n",
      "43 Cost:  22.700291575975836\n",
      "44 Cost:  22.627944631473948\n",
      "45 Cost:  22.55860172275722\n",
      "46 Cost:  22.492112589901975\n",
      "47 Cost:  22.428342955482307\n",
      "48 Cost:  22.36717042881025\n",
      "49 Cost:  22.308481792579244\n",
      "50 Cost:  22.252171183169498\n",
      "51 Cost:  22.198138850389235\n",
      "52 Cost:  22.146290294387008\n",
      "53 Cost:  22.096535649353935\n",
      "54 Cost:  22.04878922981523\n",
      "55 Cost:  22.00296918501041\n",
      "56 Cost:  21.958997225978127\n",
      "57 Cost:  21.91679840228908\n",
      "58 Cost:  21.87630091333524\n",
      "59 Cost:  21.837435944228766\n",
      "60 Cost:  21.800137519716298\n",
      "61 Cost:  21.764342371686467\n",
      "62 Cost:  21.72998981727437\n",
      "63 Cost:  21.69702164549969\n",
      "64 Cost:  21.665382010998314\n",
      "65 Cost:  21.635017333811987\n",
      "66 Cost:  21.605876204483067\n",
      "67 Cost:  21.57790929388724\n",
      "68 Cost:  21.55106926736342\n",
      "69 Cost:  21.52531070279576\n",
      "70 Cost:  21.50059001236028\n",
      "71 Cost:  21.476865367699265\n",
      "72 Cost:  21.454096628318098\n",
      "73 Cost:  21.432245273025437\n",
      "74 Cost:  21.41127433425805\n",
      "75 Cost:  21.39114833514531\n",
      "76 Cost:  21.37183322918237\n",
      "77 Cost:  21.353296342391733\n",
      "78 Cost:  21.335506317857824\n",
      "79 Cost:  21.318433062533124\n",
      "80 Cost:  21.302047696213297\n",
      "81 Cost:  21.28632250259065\n",
      "82 Cost:  21.27123088229573\n",
      "83 Cost:  21.25674730784548\n",
      "84 Cost:  21.242847280416473\n",
      "85 Cost:  21.22950728836725\n",
      "86 Cost:  21.216704767440664\n",
      "87 Cost:  21.204418062572305\n",
      "88 Cost:  21.192626391244037\n",
      "89 Cost:  21.18130980831611\n",
      "90 Cost:  21.170449172278783\n",
      "91 Cost:  21.160026112866184\n",
      "92 Cost:  21.150022999977\n",
      "93 Cost:  21.140422913847434\n",
      "94 Cost:  21.131209616428116\n",
      "95 Cost:  21.1223675239147\n",
      "96 Cost:  21.113881680385216\n",
      "97 Cost:  21.105737732501176\n",
      "98 Cost:  21.097921905227587\n",
      "99 Cost:  21.090420978532254\n",
      "100 Cost:  21.08322226502254\n",
      "101 Cost:  21.07631358848572\n",
      "102 Cost:  21.06968326329244\n",
      "103 Cost:  21.06332007463025\n",
      "104 Cost:  21.057213259534194\n",
      "105 Cost:  21.051352488681687\n",
      "106 Cost:  21.045727848920382\n",
      "107 Cost:  21.04032982650078\n",
      "108 Cost:  21.03514929098414\n",
      "109 Cost:  21.030177479797988\n",
      "110 Cost:  21.025405983415762\n",
      "111 Cost:  21.02082673113154\n",
      "112 Cost:  21.01643197740931\n",
      "113 Cost:  21.012214288782104\n",
      "114 Cost:  21.00816653127768\n",
      "115 Cost:  21.004281858352826\n",
      "116 Cost:  21.000553699312253\n",
      "117 Cost:  20.996975748194465\n",
      "118 Cost:  20.99354195310638\n",
      "119 Cost:  20.99024650598614\n",
      "120 Cost:  20.98708383277924\n",
      "121 Cost:  20.984048584010555\n",
      "122 Cost:  20.981135625736062\n",
      "123 Cost:  20.97834003085861\n",
      "124 Cost:  20.975657070794636\n",
      "125 Cost:  20.97308220747528\n",
      "126 Cost:  20.970611085671457\n",
      "127 Cost:  20.9682395256264\n",
      "128 Cost:  20.96596351598644\n",
      "129 Cost:  20.963779207015513\n",
      "130 Cost:  20.961682904083958\n",
      "131 Cost:  20.95967106141917\n",
      "132 Cost:  20.957740276108154\n",
      "133 Cost:  20.955887282341273\n",
      "134 Cost:  20.954108945889313\n",
      "135 Cost:  20.952402258800962\n",
      "136 Cost:  20.950764334315846\n",
      "137 Cost:  20.949192401981318\n",
      "138 Cost:  20.947683802966854\n",
      "139 Cost:  20.946235985566073\n",
      "140 Cost:  20.944846500881333\n",
      "141 Cost:  20.94351299868166\n",
      "142 Cost:  20.942233223426438\n",
      "143 Cost:  20.941005010451548\n",
      "144 Cost:  20.939826282307305\n",
      "145 Cost:  20.93869504524452\n",
      "146 Cost:  20.937609385842677\n",
      "147 Cost:  20.936567467772797\n",
      "148 Cost:  20.935567528692236\n",
      "149 Cost:  20.93460787726294\n",
      "150 Cost:  20.93368689029104\n",
      "151 Cost:  20.93280300998161\n",
      "152 Cost:  20.93195474130333\n",
      "153 Cost:  20.931140649459746\n",
      "154 Cost:  20.93035935746267\n",
      "155 Cost:  20.929609543802616\n",
      "156 Cost:  20.928889940214376\n",
      "157 Cost:  20.928199329530614\n",
      "158 Cost:  20.92753654362443\n",
      "159 Cost:  20.926900461432407\n",
      "160 Cost:  20.926290007059148\n",
      "161 Cost:  20.925704147957056\n",
      "162 Cost:  20.925141893180225\n",
      "163 Cost:  20.92460229170774\n",
      "164 Cost:  20.92408443083567\n",
      "165 Cost:  20.923587434632893\n",
      "166 Cost:  20.923110462459558\n",
      "167 Cost:  20.922652707545577\n",
      "168 Cost:  20.92221339562539\n",
      "169 Cost:  20.92179178362904\n",
      "170 Cost:  20.921387158424483\n",
      "171 Cost:  20.92099883561226\n",
      "172 Cost:  20.920626158367217\n",
      "173 Cost:  20.920268496327616\n",
      "174 Cost:  20.91992524452955\n",
      "175 Cost:  20.919595822382686\n",
      "176 Cost:  20.919279672688884\n",
      "177 Cost:  20.918976260699715\n",
      "178 Cost:  20.918685073211698\n",
      "179 Cost:  20.91840561769831\n",
      "180 Cost:  20.91813742147707\n",
      "181 Cost:  20.91788003091003\n",
      "182 Cost:  20.917633010636106\n",
      "183 Cost:  20.917395942835014\n",
      "184 Cost:  20.91716842652081\n",
      "185 Cost:  20.916950076862992\n",
      "186 Cost:  20.91674052453586\n",
      "187 Cost:  20.91653941509423\n",
      "188 Cost:  20.91634640837322\n",
      "189 Cost:  20.916161177913352\n",
      "190 Cost:  20.915983410408472\n",
      "191 Cost:  20.915812805174962\n",
      "192 Cost:  20.915649073644367\n",
      "193 Cost:  20.915491938874567\n",
      "194 Cost:  20.91534113508108\n",
      "195 Cost:  20.915196407188514\n",
      "196 Cost:  20.915057510397933\n",
      "197 Cost:  20.91492420977386\n",
      "198 Cost:  20.91479627984604\n",
      "199 Cost:  20.914673504228347\n",
      "[-1.47722756e+01 -2.99083025e+00  3.74019566e-01 -2.29976796e+00\n",
      "  4.54431293e+02] 454.4312931995539\n"
     ]
    }
   ],
   "source": [
    "m = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = m[-1]\n",
    "y_pred=np.array((m*df_test).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('y_pred_for_14.csv',y_pred,fmt='%10.10f',delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
